[
  {
    "chunk_id": 0,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] What do we actually, what have you understood? [Speaker 2] Basically I understood that in a meeting, how there are agents that transcribe everything and what is being said. Something similar to that, but like for the entire, everything that is being shown in the meeting, like what is being presented, who the people are. [Speaker 1] So, let me show you what, one of the recordings which I downloaded. So, let me share my screen. So, I'm showing you, I downloaded this recording from, you were in AI, right? I was in AI course. Of course you did, right. So, we did all this stuff on the iCAP. You did any recording on iCAP? [Speaker 1] So, I'll just, I downloaded one of them, right? One minute. So, let's say this one, right? So, this is a recording. Oops, why is it not showing? [Speaker 2] You don't need your laptop. [Speaker 1] One sec, one sec. Let's play. Okay. So, you can see this, right? So, this is a recording of one of your classmates. What happened here? So, this is, he's recording this and he's going through the whole thing, right? So, you can, so at some point he's opening this. At this point, the LLM has no clue what's going on, right? Later on, he also shows a demo of some kind, right? He does all this. And this is not from the course. This is from the iCAP, some demo that he gave, right? Like you gave a demo, right? It's one of those, right? This is not for the course. So, he gives at some point a demo. So, you can see he's giving a demo. Sometimes opening the course, sometimes giving a demo. It's all recorded on iCAP. I downloaded that for you. Now the question is this part is not, the LLM has no clue about this in iCAP, number one. Number two, the meeting recording which is being captured does not have any idea what's happening on the screen in general, right? In iCAP, it has the PDF. It",
    "created_at": "2026-01-25T08:23:57.388418+00:00"
  },
  {
    "chunk_id": 1,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "for you. Now the question is this part is not, the LLM has no clue about this in iCAP, number one. Number two, the meeting recording which is being captured does not have any idea what's happening on the screen in general, right? In iCAP, it has the PDF. It has whatever course the student has uploaded or this applicant has uploaded that it has, right? But it doesn't have what he's showing on the screen, okay? So, the first thing we can do is take this video. I can give you more. It just takes a bit of time to download them. I can give you more of them, but they're not too big. They are 50 MB, et cetera, et cetera, videos. We'll find a way to efficiently share the data. Slack is not the best way. What I can do is I can create a classroom for all of us and we'll just share on the classroom, right? I'll upload these videos there and all that material for this particular classroom. And we'll share everything over there. And then we'll, what you need to do on this is we need to now break it up into frames, right? So, first thing you need to do is take this video. I'll give you more. You've done CVM, right? You know how to process a video and break the frames. You can run a basic change detection. [Speaker 2] Change detection. [Speaker 1] Change detection. [Speaker 2] Oh, change detection. [Speaker 1] You just figure out where the basic frames are changing, right? So, there's one part where he's just showing this code, one part where he's showing this, one part. So, the basic different pieces are there. And then, so now you know this piece, this piece, this piece, the three different pieces are there. And now for each of these pieces, first step should be let us find out what he's doing. Oh, okay. Right? So, he's showing some code, right? So, this, that you can just take some sample frames from this, from that part of",
    "created_at": "2026-01-25T08:23:57.388425+00:00"
  },
  {
    "chunk_id": 2,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "piece, the three different pieces are there. And now for each of these pieces, first step should be let us find out what he's doing. Oh, okay. Right? So, he's showing some code, right? So, this, that you can just take some sample frames from this, from that part of that thing and just pass it to Vision LLM, right? Could be Gemini with Vision. Do you have Gemini key in this? I have Gemini. It will, you use a free quota to the extent possible. At any point of time you face a problem, actually there should be no problem because. It's 300 dollars. Huh? [Speaker 2] It's 300 dollars. [Speaker 1] Free, right? So, if for some reason your quota limits are, at least your rate limit is going to exceed, et cetera, I'll give you a key. For the moment you can just use that because you won't use it much. It's not like you're using it continuously, right? You use it just to extract from this. What is he doing? I mean, this is, are you showing a screen? Are you doing this? Are you doing that, et cetera, right? Next step we can go into the, say, okay, for a few videos we can actually use the LLM to generate the ground truth. That means what is on the screen exactly, fully. There is a scene with this, this, this, this. It will do that. That will cost you more tokens because you're giving more frames. That's the next level of change detection. First level is broadly what's happening. And within the demo, every time something significant changes, more detail, you'll again, you'll get more detail. This has changed. So, that's the ground truth. Once we get that, right, then it's a matter of making it efficient because you can't be giving every frame with LLM, right? So, now once you have ground truth from many videos, right, now we can see what kind of vision model can be used to, we can use a blip or something like Puneet was saying there, right, to",
    "created_at": "2026-01-25T08:23:57.388431+00:00"
  },
  {
    "chunk_id": 3,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "it efficient because you can't be giving every frame with LLM, right? So, now once you have ground truth from many videos, right, now we can see what kind of vision model can be used to, we can use a blip or something like Puneet was saying there, right, to extract that text from that, that, that pieces. And now we have a description of what's happening in the video plus texts. And then we have the third problem, which is you have the recording. And recording a transcription is not a problem. I mean, I use free tools for transcribing. They're going to give us their own whatever rocket read or something they have, right? But right now, we can just transcribe the actual audio, whatever is happening. From this video, we can transcribe. You can take the audio out of the video, right? And then we can give it to a transcription tool, like the Google scribe or something. You have to run it many times, maybe three a day. You can do one and a half hours a day. You can do it for free, right? So, you just transcribe. You have the video transcription of the recording. You have what is extracted from the frames. Now, we just have to stitch them together to make a better context. Right? And then the chatbot, which is going to ask questions on this context, will also be able to answer questions on what has happened in the video and also be able to, in the context of ICAP, also be able to ask questions about what he's doing on the screen. Now, what I've said is a very broad thing. There will be many nuances of issues in that, right? So, there will be issues of when to do the next level of. You can't give every frame to the LLM, nor can you. We can do that initially to generate ground truth, right? But the mixture of pain detection and text extraction and broad description of what is happening, how to make that more efficient, that will",
    "created_at": "2026-01-25T08:23:57.388433+00:00"
  },
  {
    "chunk_id": 4,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "do the next level of. You can't give every frame to the LLM, nor can you. We can do that initially to generate ground truth, right? But the mixture of pain detection and text extraction and broad description of what is happening, how to make that more efficient, that will be the core. First part, if we can just get done, let's say, in Jan only, with LLM only, without any vision other than change detection. Right? Next will be all about making it more explicit. Broad picture is clear? [Speaker 2] So, I think the first step is from this video that we did, we need to take the detection and take the broad... [Speaker 1] First you take the audio. [Speaker 2] Take the audio. [Speaker 1] Right? [Speaker 2] Not audio, video. [Speaker 1] Why audio? Transcription is there. And then you can get an idea of what he's doing at what point of time. Right? So, there is a change detection and there is the transcript of the audio along with the timestamp. So, you know that this part is different from this part is different from this part. For each of these different parts, right, you have the audio or the textual transcript of that part also. And then you can just feed it to the LLC. What is he doing at this point of time? Is he showing a demo? Not doing a demo? So, now you know he's showing a demo. [Speaker 2] Right. [Speaker 1] Now, for the audience showing code. Now, for showing code, showing demo, next level of analysis is to take. Right. So, what do you think you can do by next meeting? [Speaker 2] By next meeting, I think I can get a sense. [Speaker 1] No, no. A little bit more detail. [Speaker 2] Okay. So, [Speaker 1] How many videos will I get? I'll give you one right now. I'll give you two more, two, three more. I'll download them and give you a few. [Speaker 2] So, all of these videos are in different screens.",
    "created_at": "2026-01-25T08:23:57.388434+00:00"
  },
  {
    "chunk_id": 5,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "A little bit more detail. [Speaker 2] Okay. So, [Speaker 1] How many videos will I get? I'll give you one right now. I'll give you two more, two, three more. I'll download them and give you a few. [Speaker 2] So, all of these videos are in different screens. So, I'll do a change detection. [Speaker 1] So, you know which are the basic parts. You'll transcribe and say, okay, describe what is happening on each of these parts. And then, and then we fill that with the LLM and add some more context from the screen to that transcript. So, right now you're saying I'm doing this, I'm doing this, I'm doing this. Right. What is on the screen should be added to that transcript. [Speaker 2] Okay, okay. So, I asked the LLM to change this into a text. Text. F Okay. [Speaker 1] What is he actually doing on the screen? Okay, okay. So, then that means we insert into the transcript, right? Those pieces. So, now the transcript is a bigger thing. It's not just what the student is saying or the applicant is saying. It's also what he's doing on the screen. You can insert in any which way. So, by next meeting you can't do all this, I'm sure. What can you do? What realistically minimum you can do all this by next meeting? [Speaker 2] I think I can do it. One week is a long time. So, it's the first week. [Speaker 1] First week. [Speaker 2] No classes also. [Speaker 1] So, let's say let's have a stretch target. See how much you can do by next week. I'll give you a few videos. You're doing nothing on the screen right now. So, this video is useless. But this video we'll just record it and take a download anyway because that will be the source of future videos, right? Meeting videos, right? So, maybe other meetings like I'll be having a meeting in the evening with some other students. Maybe I'll record that also. But leave it for this week.",
    "created_at": "2026-01-25T08:23:57.388435+00:00"
  },
  {
    "chunk_id": 6,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "we'll just record it and take a download anyway because that will be the source of future videos, right? Meeting videos, right? So, maybe other meetings like I'll be having a meeting in the evening with some other students. Maybe I'll record that also. But leave it for this week. This week you just take some of the iCAP videos which I'm going to give you. Okay. And the future next meeting you will also be doing something on the screen. That will also get recorded. No, no, but that part will also be recorded. Right. So, the transcript of that will again be shoved into your system. We'll recursively use it every time. Whatever you have up to then and see whether it's useful to us. Yes. And the other guy will build a chat bot of the whole thing hopefully. Let's see. Right now you're just trying to enhance that recording or the transcript with more information. You want to enhance that transcript with more information basically from the screen. Okay. Right. What's going on? Clear? Okay. So, I will now give you videos. I will also give you a recording of this meeting. Right. Which you should also translate. Okay. Right. And anyway, keep it for record purposes. What will be done with the recording of that meeting? The other guy will figure out what to do. We'll make something. So, as of now, let's keep this time. [Speaker 1] If there's a reason for this time to be replaced by somebody else, we'll move it here and there. [Speaker 2] That's fine. [Speaker 1] But maybe we'll keep it 11.30. Okay. If you can make it. Or you have a problem getting here. [Speaker 2] No, no. If it is the first day, I was getting my application. Okay. [Speaker 1] Otherwise, you don't have a class tonight. No. In the morning. Right. So, 11.30. Because my class ended at 11 after answering questions, etc. By 11.30, I'll be here. Cool? Any other questions you have? [Speaker 2] No. So, I'll share my updates on Slack.",
    "created_at": "2026-01-25T08:23:57.388437+00:00"
  },
  {
    "chunk_id": 7,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "my application. Okay. [Speaker 1] Otherwise, you don't have a class tonight. No. In the morning. Right. So, 11.30. Because my class ended at 11 after answering questions, etc. By 11.30, I'll be here. Cool? Any other questions you have? [Speaker 2] No. So, I'll share my updates on Slack. Yes. Every day. [Speaker 1] Whenever you have questions, issues, you share on Slack, etc. And the whole idea of this second guy's project is that he'll build a Slack bot, right, which will take the transcript of this meeting. And your question, at least I'll answer it, but hopefully Slack bot also will answer it. Let's see where that goes. He's having some trouble registering for some reason. I don't know. [Speaker 2] He needs some help. [Speaker 1] Why don't you reach out to him? Do you have his email? [Speaker 2] He's in Slack. [Speaker 1] I'll put him on Slack. Cool? Good. All right. Thanks. We're done. [Speaker 2] Okay. That's it for the meeting? [Speaker 1] The meeting is typically not going to be more than 20 minutes. Unless there's a lot to show. [Speaker 2] Okay. [Speaker 1] Anything else you have to discuss? [Speaker 2] No. So, when will I have to train a model? [Speaker 1] You will have to train a model once we decide that. Because we cannot be giving this whole thing. You can't give every frame with NLM, no? We may not train a model. We will now. We'll have a discussion. Okay. Okay. Okay. We'll have to do it using a normal computer vision model. Which computer is the machine vision model to use? You also decide. Nobody trains from scratch or what? You always use something. [Speaker 2] You don't use the hero. [Speaker 1] That's object detection, not text. Blip. Blip. Blip. Blip was the text extraction model. Blip will probably do something. But search. How do you best extraction of text from these things? We will have to fine-tune some more which model to fine-tune we don't But that's after we generate a lot",
    "created_at": "2026-01-25T08:23:57.388438+00:00"
  },
  {
    "chunk_id": 8,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "object detection, not text. Blip. Blip. Blip. Blip was the text extraction model. Blip will probably do something. But search. How do you best extraction of text from these things? We will have to fine-tune some more which model to fine-tune we don't But that's after we generate a lot of ground truth using the LLMs to generate the ground truth for that right Anyway, you'll have the recording and transcript in case there's a doubt you can always refer to the transcript or ask me questions Okay, good You",
    "created_at": "2026-01-25T08:23:57.388439+00:00"
  },
  {
    "chunk_id": 9,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 0,
    "text": "[Speaker 1] So, you have to wait, because you are supposed to be at 11.38. No, I had received it was 12. No, I changed it. Anyway, just wait, it is not 11.30. We will meet today, but first let me finish this. So, this stores the information for every thing. What is that on the frame? What is the topic on the frame? You have to join the meeting and share. That is like Google meeting. [Speaker 2] Google meeting. [Speaker 1] There is a Google meeting up today. It is on calendar. Open your calendar. Now you can share the screen. Done. Shut the mic. So, there is evidence for each summary and every thing is sourced. Like the name, the serving, this is the decision item. These are different decisions taken by people. There are different actions taken by people. Then there is a chapter. Chapters are like a general summary and time frame. This was the title summary. People, decisions, action items. Let's say you have a speaker called Rahul. This will try to find the name of the speaker. This is the meeting stage in which you have speaker notes. You have all the speakers. All they are. [Speaker 1] You are telling me the how. You haven't told me what. What is it going to do? What have you done? What is all this about? You are going to the internals of it. [Speaker 2] First it takes out the aspect using Deepgram. And then it takes out. So, you take the audio, take the video. We take key frames from the video using another library called Cpredict. ICpredict. But this doesn't work. There is a fallback also. Let's see how the images are. And then we use Gemini to get the details from the key frames. Screenshots of the video and summary from the audio. And then the audio you are passing to the Gemini through the transcription. And then we make small time frames for movements. They can be slightly changing key frames changing or the person stops talking. There is a",
    "created_at": "2026-01-25T08:23:57.920189+00:00"
  },
  {
    "chunk_id": 10,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 1,
    "text": "the key frames. Screenshots of the video and summary from the audio. And then the audio you are passing to the Gemini through the transcription. And then we make small time frames for movements. They can be slightly changing key frames changing or the person stops talking. There is a break between the speech. We create a small time frame. And then we compare the post accessibility of all these time frames. Whichever time frame was similar, we will open it in the templates. Then we use these chapters and pass them to the editor to create summary. Then we also store different things like the name of the speaker. What else is happening. So, let's see what works so far. This was the output. These are the chapters that got generated. I have stored them into five active objects. This is the summary. The title we got. This is for this video. [Speaker 1] We have the key points. Some of the decisions that were made. And then these are the movement IDs. These are the movements to make up the chapter. We got this movement from 90 seconds to the hundred and third second. And we have the text also. These are the other chapters. This is the frame analysis for all the different frames that have been detected. We get a view like what are they. Is it slides. Is it mixed. Is it camera. What the topic is. The visual summary. What is on the screen. The text. The entities. And then if there is a name of the speaker present on the screen. These are the different key frames. These are the movements. The key frames. The audio. This library actually gives the different speakers. You can see the AI evaluator was there. It was given the name of the speaker 0. And then the student was speaking. It was given a label of 1. For example, let's say the student purchases something, or his name is visible on the screen, this speaker will get replaced by that. [Speaker 1] As regards the goals of",
    "created_at": "2026-01-25T08:23:57.920202+00:00"
  },
  {
    "chunk_id": 11,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 2,
    "text": "given the name of the speaker 0. And then the student was speaking. It was given a label of 1. For example, let's say the student purchases something, or his name is visible on the screen, this speaker will get replaced by that. [Speaker 1] As regards the goals of the project, what progress has been made so far? What are the goals and where have we reached? [Speaker 2] The goals was to create data which includes the demonstration which are done on the screen. There is something happening in terms of the meeting or in the presentation which is based on the conversation and any other context which is pre-provided, which is the PDF file that is already there. So that data is already there. The transcript is there. Everything is there. But what is happening on the screen at a particular point in time is not there. You have the transcript. You have the context. And now you have done something which has created all this structure. Idea was to firstly in batch mode augment that meeting transcript with some information about what the presenter is presenting on the screen in terms of a demonstration. Have you been able to say this is a demo going on versus something else? This is part of the presentation or slide one, slide two, slide three. [Speaker 2] I think it is. You could say for example this is a night demonstration and showcase the time code. [Speaker 1] Where is that demo happening? What key frame is it happening? You have generated a whole bunch of stuff. But is it can we actually get out in this presentation at this key frame from this frame to this frame somebody is giving a demo? [Speaker 2] In these tablets. This is what that evidence object was about. For this one you get this summary. The presentation introduces an AI chatbot in this file. [Speaker 1] Where do you get the references? From what moments? We talked about it will be shown on the screen. But you are not answering my",
    "created_at": "2026-01-25T08:23:57.920205+00:00"
  },
  {
    "chunk_id": 12,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 3,
    "text": "is what that evidence object was about. For this one you get this summary. The presentation introduces an AI chatbot in this file. [Speaker 1] Where do you get the references? From what moments? We talked about it will be shown on the screen. But you are not answering my question. You have done a lot of stuff. But I still do not see that in this entire transcript when is the presenter actually either showing a demo or showing some code? Do we have that information? It should be somewhere in the chapter but it is not given clearly. [Speaker 1] So what we wanted to do was, okay you have done something. It is fine. Interesting. Certainly interesting. But does it solve the problem? It may solve some other problem. It may say I have dumped all this, now I do not know what the hell this is. Some information has come out. But is it telling me that this transcript is there and then from this period to this period there is a demo going on? And number two, within that demo, what is going on is the description of what that person presented in that demo. Exactly what that narration. If you were to describe this demo to me, that this is what the person presented. This is what actually presented. Or this is slide one. A lot of stuff can come from the context. What you produced is actually more data but it's not informative in the sense that we wanted it to be. [Speaker 1] Yes, you could probably use this data. Information that I think is definitely there but I don't think that is happening. So what we wanted were two things. One is in batch mode, that this entire meeting should be enhanced. The transcript of that meeting needs to be enhanced with whatever is on the screen. Could be a presentation slide on the screen and a narration what that slide is. All that in some sense already there because the PDF of that somewhere is there. The file is also",
    "created_at": "2026-01-25T08:23:57.920207+00:00"
  },
  {
    "chunk_id": 13,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 4,
    "text": "enhanced. The transcript of that meeting needs to be enhanced with whatever is on the screen. Could be a presentation slide on the screen and a narration what that slide is. All that in some sense already there because the PDF of that somewhere is there. The file is also there on the screen. [Speaker 1] The system is asking questions only based on what he's speaking, but nothing to do with the screen. But suppose the guy is speaking, but the screen is showing something totally wrong? Or the system has no idea what's on the screen? So a lot of stuff has been generated, but from these two goals perspective, to what extent have we got there? That's what I think we need to think. Think like from this data generated algorithm, we can go. So what I think we need to do now is, we need to take the result of what has happened so far and say this is the input, and this is the desired output. [Speaker 1] And now I think we should stay on Slack. After this meeting, can you summarize on Slack what are the two inputs and outputs that we need? One is batch mode, one is data type. At least then Pankaj can take a look. He's also on Slack. Second thing is, this particular meeting that we are having today, get a transcript of that meeting. Summarize that transcript and post it on Slack. At least we know that we are keeping them up to date in what is happening. [Speaker 2] They just need to get a batch summary. [Speaker 1] Batch summary will be done fast. I think this deliverable will be done very fast. And the next thing will be to, by the way, they will have to supply their tool and then we have to do it with their tool. Meanwhile, before they supply that tool, let's see if we can then, after the batch summary is done, maybe a week after next, we can work on can we make it real-time, which I can",
    "created_at": "2026-01-25T08:23:57.920210+00:00"
  },
  {
    "chunk_id": 14,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 5,
    "text": "have to supply their tool and then we have to do it with their tool. Meanwhile, before they supply that tool, let's see if we can then, after the batch summary is done, maybe a week after next, we can work on can we make it real-time, which I can integrate. Batch summary should be done based on meetings, but now let's generate that batch summary for not just that meeting, but let's generate for the other meetings which are shared. And so now we have results for three, four meetings. Let that be the target for next week. A batch summary process, this is what the person is telling me about on the screen at this particular time, where the demos are, et cetera. So then now that can be the basis for what Akshit can put into his chat box. [Speaker 2] I have a request. So they write out the meetings that are not shared with anyone speaker. If it's possible, could I get some? [Speaker 1] You will. So this will be there. Then our previous meeting is also there. Usman, two speakers, and then the other guy's meeting will also be there. Soon there'll be two speakers, and earlier there was no presentation. Now they'll be starting making presentations. I'd like you to make a GitHub repo for this. Add me also. Don't make it public. Make it private. Add me, and then we'll add Pankaj. And then we'll see. Same events. Action items, write down on Slack. Goals, write down on Slack. Just do a summary of today's meeting and put it on Slack. Today's meeting, when it's recorded and ready, I'll upload it, and then you can get it. [Speaker 1] Ultimately, my goal is, I want this integrated real-time. Their goal, they have to specify after seeing this exactly what they want. The core engine should be generic, but there'll be a real-time version, and there'll be a batch version. Batch version will be based on what you're doing now, but batch version will be more toward what they want. And the",
    "created_at": "2026-01-25T08:23:57.920212+00:00"
  },
  {
    "chunk_id": 15,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 6,
    "text": "have to specify after seeing this exactly what they want. The core engine should be generic, but there'll be a real-time version, and there'll be a batch version. Batch version will be based on what you're doing now, but batch version will be more toward what they want. And the real-time version is what is required by them. How long does it take, the whole pipeline? [Speaker 2] This video is 10 minutes and around two minutes. I think if it's real-time, you can do, like, one frame takes very long. It's very quick, probably one frame. It's like 80 frames or something, it's a video for two minutes. But chain detection, in terms of the keyframe, whether it's a keyframe or not a keyframe, based on what is there. I mean, that architecture will have to be well done. To see is this something new happening, and is it a demo? If it's a demo, I start augmenting. If it's not a demo, I don't want to augment. [Speaker 1] Overall, we're able to think through this. But first, let's get past this phase. From this to a system is a journey. This is easy. Getting into a system which is usable in real-time, or a system which is usable for their purpose, is a journey. Transcription is happening, it's batched. Every 10 seconds, it's being batched. So, it's not a simple architecture. It's a complex architecture. Anyway, those steps have been taken. All right, we'll end this one. I think we'll provide the batch and release on the tomorrow after.",
    "created_at": "2026-01-25T08:23:57.920214+00:00"
  },
  {
    "chunk_id": 35,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 0,
    "text": "[Speaker 2] You mentioned that the patient's behavior is controlled using a LanGraph based state machine. Please elaborate more on your point. [Speaker 1] What LanGraph state machine is doing is like in our backend, what is happening is each session is created. Let me show you in the PPT. Like in our system, each session is created and what LanGraph is doing, it is transitioning the state of that session ID. Like I am storing some variables like diagnosis, symptoms, and questions asked in your backend, Flask backend. And what LanGraph is doing is it is transitioning, it is updating the memory of each session. Like we can consider our Flask as a bucket and which have some certain boxes and LanGraph as a man that is updating those boxes. So now I will be explaining the system architecture. Each frontend session maps to a unique backend session object here. So what happens is like our Flask backend session holds all the patient state, symptom states, and questions asked and nothing is shared globally in it. So then what happened is for every message, we invoke a LanGraph based patient agent using graph.invokeState. This ensures deterministic state-driven behavior instead of prompt-based randomness. And because state is a session scope, multiple users can interact simultaneously without any memory leakage or cross-user contamination. And so the technologies that I use is HTML for CS frontend, CSS and JavaScript. I kept it simple for lightweight framework. For backend, I use Python and Flask. For agent framework, I use LanGraph and LanChain. And for data, I have used in-memory session store because right now we are not having much of the data. In future scopes, I have added this that I would be adding the backend too. But right now I have done it in in-memory session store only like to reduce the heaviness. [Speaker 2] You mentioned that LanGraph ensures deterministic state-driven behavior. Could you elaborate on how backquote graph invoke underscore state backquote achieves this deterministic behavior, contrasting it with prompt-based randomness? [Speaker 1] The deterministic behavior is achieved by the",
    "created_at": "2026-01-25T08:24:40.079424+00:00"
  },
  {
    "chunk_id": 36,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 1,
    "text": "it in in-memory session store only like to reduce the heaviness. [Speaker 2] You mentioned that LanGraph ensures deterministic state-driven behavior. Could you elaborate on how backquote graph invoke underscore state backquote achieves this deterministic behavior, contrasting it with prompt-based randomness? [Speaker 1] The deterministic behavior is achieved by the rules like let me show you in this diseases and treatments.py files. Actually, what's happening is I have certainly putted and constrained. Right now, I have not added much of the diseases. I have only constrained it to three diseases. The deterministic behavior is because I have added these symptoms and optional symptoms. I have limited them and I have putted them into diseases and treatments.py. So, this is achieving the deterministic behavior. Also, LanGraph is only ensuring the symptoms from these files. [Speaker 2] You mentioned adding rich symptoms and opiates optional symptoms to achieve deterministic behavior. Could you elaborate on how these specific additions ensure a more predictable outcome in the simulation? [Speaker 1] So, what the required symptoms are doing is required symptoms are the symptoms that are specifically needed to diagnose a particular disease. But optional symptoms are the symptoms that increase the confidence bar that I will show you afterwards in the project. Let me explain the key design choices and engineering decisions. So, what I am doing is my session is a scoped state, not a global memory. Each user session maintains its own isolated patient state. So, no state is global. Every request mutates only its session state. The LanGraph for control flow is not a prompt. The LLM cannot skip steps because transitions are enforced by the graph. Also, it is a rubric-based diagnosis evaluation. Diagnosis correctness is checked against revealed evidence and not model confidence. And separation of intent classification and symptom revelation. Understanding a question does not automatically reveal a symptom. Also, design choice 5 is like you can see why the model leans towards a diagnosis. Live system and demo demonstrator is here. This is the backend evidence. Here I created a session through a session ID and thread",
    "created_at": "2026-01-25T08:24:40.079432+00:00"
  },
  {
    "chunk_id": 37,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 2,
    "text": "symptom revelation. Understanding a question does not automatically reveal a symptom. Also, design choice 5 is like you can see why the model leans towards a diagnosis. Live system and demo demonstrator is here. This is the backend evidence. Here I created a session through a session ID and thread ID. Then I ask particularly symptoms that are revealed gradually. Also, here is the diagnosis which is allowed only after a sufficient evidence. LanGraph transition is questioning it to ready for diagnosis. Here is the testing of my model. [Speaker 1] These are the diseases and these are the required symptoms that I was talking about. The fever, cough, headache, nausea, vomiting. And these are the optional symptoms that increase the patient memory which increase basically the confidence bar. Also, the minimum question needed to diagnose which should be required symptoms basically. These are the special interaction test cases like greeting and your duration ambiguity handling like how many days have it been that I will show. Let me show you that too like right now. Like here I will ask hi. So patient will be replying by this. Then I will ask for his symptoms which it says that that's not been an issue. Now nausea is a symptom which is between food poisoning. [Speaker 2] I'm following your presentation well. Please continue your explanation of the required and optional symptoms. [Speaker 1] So food poisoning is a symptom between food poisoning and migraine and nausea. So what happens is I will elaborate further more to a specific disease to increase diagnostic confidence. Like I will be asking do you have stomach ache which it shows it doesn't have us. So we will be constraining the symptoms. [Speaker 2] You mentioned elaborating further on a specific disease to increase diagnostic confidence. Please continue. [Speaker 1] So we are seeing that nausea and light sensitivity is going specifically. So what we'll be doing next is asking do you have this. So these three symptoms. [Speaker 2] Could you elaborate on what you mean by going specifically in the context of",
    "created_at": "2026-01-25T08:24:40.079434+00:00"
  },
  {
    "chunk_id": 38,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 3,
    "text": "diagnostic confidence. Please continue. [Speaker 1] So we are seeing that nausea and light sensitivity is going specifically. So what we'll be doing next is asking do you have this. So these three symptoms. [Speaker 2] Could you elaborate on what you mean by going specifically in the context of nausea and light sensitivity? [Speaker 1] Mm hmm. So what's happening is like in flask backend. Right now we were having symptoms like an empty array. And then what's happening is the state the land graph like I am asking the symptoms and the symptoms however the symptoms as are getting revealed they are going in the your flask backend. And what's happening there is it is transitioning that state for the particular session ID of this patient which is 998620F0. And what's happening if I create a new session like a new patient. So that would be having a different session ID. And the land graph would be connecting to different session ID then. But now right now we are doing this. So what I will be doing is diagnosis migraine. So diagnosis migraine is good. Right now I will be showcasing the good test cases. Then I will be also showing the error test cases. Like if the diagnosis I have done is bad or something. Treatment. So different treatments like for migraine there are three treatments dark room painkillers and rest. And for food poisoning ORS hydrogen diseases are these. [Speaker 2] I'm following your presentation. Please continue. [Speaker 1] Now once the treatment is completed the session is stopped. So the session ID of this particular patient is done. So what will happen next is like we will be showing the diagnosis migraine. We will be going in patient 2. We will ask again from you. Also the each patient would be having random disease. I have done random in my script.js file. Through that the each patient would be having a different disease. Like a random disease not a different. [Speaker 2] You mentioned assigning each patient a random disease using back quote script. What's",
    "created_at": "2026-01-25T08:24:40.079436+00:00"
  },
  {
    "chunk_id": 39,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 4,
    "text": "Also the each patient would be having random disease. I have done random in my script.js file. Through that the each patient would be having a different disease. Like a random disease not a different. [Speaker 2] You mentioned assigning each patient a random disease using back quote script. What's the symptoms of flu? [Speaker 1] It was fever cough. So now I can diagnose it with flu. Now I will be showcasing what if my diagnosis is wrong. Like if I diagnose. [Speaker 2] I'm following your question. [Speaker 1] With migraine. [Speaker 2] Please continue. And I'll do my best to help you understand the symptoms. [Speaker 1] Like if I do a diagnosis as something else. So what will happen like it will show that verdict is incorrect. So I will now show. This is the error handling that I have done in my project. Like what if the particular doctor gets to diagnosis us patient wrongly. Diagnosis flu. Now it shows that. [Speaker 2] You mentioned that the system shows what verdict is incorrect. Can you elaborate on how the system determines that a diagnosis is incorrect and what specific steps are taken in the error handling process. It meant of this is. So session is you elaborate on what aspects of the treatment you are referring to. [Speaker 1] Now I will be showing what if the treatment is wrong. Like if I. [Speaker 2] Evaluation has been successfully saved. You may now disconnect. We are done here. Like what if I do now. We are done here. Please return to your task.",
    "created_at": "2026-01-25T08:24:40.079437+00:00"
  },
  {
    "chunk_id": 16,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 0,
    "text": "[Speaker 2]: I am speaking in English also in between, I am speaking in Hindi also in between. So, it was being messy. [Speaker 1]: That's okay, but you don't have to read it. [Speaker 2]: Okay. [Speaker 1]: LLM has to read it. [Speaker 2]: LLM has to read it. [Speaker 1]: Will you do it yourself? [Speaker 2]: So, sir, like I am speaking in Hindi in between. I am speaking in English from here and speaking in Hindi in between. So, the Hindi one that has come in between. So, that should be in English only or should it be in Hindi separately? [Speaker 1]: They say that you do multimodal transcription. Yes. And then the chat board that you will make, it will read it and give the full response in English. [Speaker 2]: So, it will understand the Hindi one. [Speaker 1]: Let's see. [Speaker 2]: Okay. [Speaker 1]: Do that only. Now see, you are speaking in Hindi, is it recording? Yes. Let's see what happens. [Speaker 2]: Okay. Right. Also, sir, there was one more thing. Like I am saying something 100%, so 100% it was not coming right. [Speaker 1]: See, there is transcription. Okay. There is language detection. Yes, language detection. There is translation. [Speaker 2]: Yes. [Speaker 1]: We will put all of them on one. Right. [Speaker 2]: Okay. [Speaker 1]: This is the meeting. More meetings will come. We will do all of that. So, now what have you understood is the project. Not this. You are asking what is the problem. Okay. What is the definition of the problem? Yes. [Speaker 2]: So, sir, what I have understood in total is like I have to create a LLM or a chatbot that is like representing you or any instructor or as a personal base that we can keep in a meeting or that we can keep in a meeting. It can take inputs from the meeting, from the transcripts that we provide to him or from your past data and how you speak and also I am",
    "created_at": "2026-01-25T08:23:58.392488+00:00"
  },
  {
    "chunk_id": 17,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 1,
    "text": "any instructor or as a personal base that we can keep in a meeting or that we can keep in a meeting. It can take inputs from the meeting, from the transcripts that we provide to him or from your past data and how you speak and also I am collecting your email replies and all just to report that data. And then if you are not available, like if you are available at that moment and some person asks, so you can reply. But if you are not available at that moment, so whatever that the chatbot has understood from all the past data and all. So, he could reply best by accessing the internet to the student who is like asking doubt or asking his grades. How did you get the internet? I didn't get the grades. What do you mean by internet? I mean, sir, like I said, whatever happened, he can access the internet and tell something better. [Speaker 1]: Yes, that is okay. So, one is it should respond. Suppose you say I am confused, should I be doing this in the project or not doing this in the project? My assumption is correct or wrong. Then on the basis of the meeting recordings and my past responses, you should give the best possible answer. Number one. Second, if we have a doubt, can I am doing, I am going to use this pre-trained model, let us say or something. I am going to use this internet resource. If you were to ask me, can you suggest something better? I may think what I know. But then LMA also knows what is on the internet. So, it may also find on the internet. [Speaker 2]: So, it can do that. [Speaker 1]: So, the first that is good. So, what you are going to build is going to be number one, a chat, an agent based with have maybe multiple agents. So, one is going to try to find out what the guy is asking. One is going to try to look at the meeting",
    "created_at": "2026-01-25T08:23:58.392503+00:00"
  },
  {
    "chunk_id": 18,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 2,
    "text": "that is good. So, what you are going to build is going to be number one, a chat, an agent based with have maybe multiple agents. So, one is going to try to find out what the guy is asking. One is going to try to look at the meeting recordings and see what is going to be evaluated the response. Multiple mini agents will be there. [Speaker 2]: And then we will be collaborating. Collaborating to give the answer. [Speaker 1]: That is one thing you want to do. Second, that is, but and that is going to have as a resource. The transcriptions of various meetings. The transcriptions may have multilingual content. So, you have to deal with the multilingual content. That is the one aspect. Let us first keep two years English content. [Speaker 2]: Just English. [Speaker 1]: Part one. Right. [Speaker 2]: Okay. [Speaker 1]: Part one, let us see English content. The Hindi part doesn't give its okay. Right. Or only Hindi content. Whatever. We will just see. Okay. Maybe there is one file which is Hindi. One file is English. We can break it up. We will do something. Right. We will do something to make it work. Right. So, there is somebody. You ask a question from this meeting. What happened in this meeting? What were the goals? What were the. That is what the read.ai audit does. [Speaker 2]: Something like that. It will answer. Right. [Speaker 1]: Rather than give me a summary. Read.ai what does it do? It gives me a summary. I have to read it. Right. Summarize this meeting and all that. Right. But I have to read the damn summary. [Speaker 2]: So, can we do something that it speaks? [Speaker 1]: That is a different thing. Right. Problem with these meeting summarizations is it takes up my time to read the damn meeting. I have to read the meeting summary. It takes time. But I have a doubt. And I asked for a doubt. Right. So, it will tell me. Okay. So, first goal. For",
    "created_at": "2026-01-25T08:23:58.392506+00:00"
  },
  {
    "chunk_id": 19,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 3,
    "text": "thing. Right. Problem with these meeting summarizations is it takes up my time to read the damn meeting. I have to read the meeting summary. It takes time. But I have a doubt. And I asked for a doubt. Right. So, it will tell me. Okay. So, first goal. For month of January. Okay. Right. It has to be finished. Is. Let us just call it a simple chatbot. Web interface. [Speaker 2]: Right. Okay. [Speaker 1]: Not slack back another thing. Web interface. Meetings. Jo bhi ho rahi hai. Uspe if anybody asks a question. Right. Based on this meeting. It should give a proper concise answer. [Speaker 2]: Okay. Right. [Speaker 1]: Let us get that. Right. Okay. Second. Immediate. Once that is done. So, this is basically. There is a UI piece. There will be the core chat interface. We should keep it clean. So, that it can be called from anywhere. Right. And there is the repository of meetings, transcripts, etc. Properly organized. So, that everyone can read. Right. Second. Very small piece. Perhaps in half of February. Connected to slack. [Speaker 2]: Okay. That is the thing. Connected to slack. [Speaker 1]: So, the slackbot. So, that on slack. When we are putting. I will put you on slack also. Okay. Your chatbot. First will be web based. Second will be. Chat. Second will be slack. So, it will be on slack. You are connected to the same channel that we are talking. [Speaker 2]: Okay. [Speaker 1]: So, why. It will decide. So, there the other thing comes. Once you connect it. It is not. If you have to. You could have. That bot could respond to a DM. Like the chat message. Chat. Chat. DM. DM. [Speaker 2]: Each DM different. [Speaker 1]: No, no. Suppose I DM the bot. Okay. Then I am asking a question. That is one. Second is I am having a conversation. The chatbot decides to come in. [Speaker 2]: Okay. By himself. Okay. It is listening to conversation. [Speaker 1]: So, there is a different type of",
    "created_at": "2026-01-25T08:23:58.392508+00:00"
  },
  {
    "chunk_id": 20,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 4,
    "text": "No, no. Suppose I DM the bot. Okay. Then I am asking a question. That is one. Second is I am having a conversation. The chatbot decides to come in. [Speaker 2]: Okay. By himself. Okay. It is listening to conversation. [Speaker 1]: So, there is a different type of agent there. Deciding. Is there something where I need to talk? [Speaker 2]: I could help. Or I can elevate the conversation. Right. [Speaker 1]: Okay. Alright. Okay. At this point. Okay. Right. And March. Half of March is holiday. But anyway. You have to work in holiday. March is. If I have. I am doing some work. You are doing. Agrim is doing. Somebody else is doing. And it has a code base. [Speaker 2]: Okay. [Speaker 1]: Right. If this bot has access to this code base. Can it see? Is it doing? Is it doing the right work or not right? Based on. [Speaker 2]: Evaluating the code base. [Speaker 1]: Based on the meeting. Is it doing. What I am supposed to do. Or doing something else. Oh. [Speaker 2]: Okay. [Speaker 1]: Okay. [Speaker 2]: It's like. It's evaluating the code. Is that. [Speaker 1]: For example. I don't ask questions. About your code. Except in the five. Ten minutes. I can look at the code. Right. Right. It can look at the code. Continuously. Okay. Let us say. We just give it. Git access. It will periodically. Do a git pull. Right. And it will periodically. Look. And if it wants to ask a question. About the code. It will ask questions. [Speaker 2]: It will ask questions. [Speaker 1]: What are you doing here? What is that? What is that? So you feel like. There is a reviewer of the code. Somebody who is trying to. Understand the code. And whatever. It gets the answers. It keeps putting that back. [Speaker 2]: Okay. He is basically evaluating. If it is correct or not. [Speaker 1]: It is correct. And it is clarifying. [Speaker 2]: And clarifying. Right. Okay. [Speaker 1]: So you feel.",
    "created_at": "2026-01-25T08:23:58.392511+00:00"
  },
  {
    "chunk_id": 21,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 5,
    "text": "is trying to. Understand the code. And whatever. It gets the answers. It keeps putting that back. [Speaker 2]: Okay. He is basically evaluating. If it is correct or not. [Speaker 1]: It is correct. And it is clarifying. [Speaker 2]: And clarifying. Right. Okay. [Speaker 1]: So you feel. That there is somebody. Over there. Okay. So. So. So. So. So. So. So. So. So. No need. Especially if. [Speaker 2]: So. I. [Speaker 1]: I will give you the transcript of the first meeting, transcribe the meeting, run the chat on that, run it on the command line, make the interview, this agent, that agent, who will actually answer questions on that chat. Read more in UI, UI is wasting time actually. Because once this part works, the UI will come on the web, it will come on the site, it will come separately. It should answer the questions properly. [Speaker 2]: So sir, right now just in English. [Speaker 1]: Whatever is in it, first type it in English. Right. Let's see what happens. Let's see. Whatever is there. You can go to troublescribe, you can give three videos, three recordings of 30 minutes per day. Do it. I will keep giving you meetings, as meetings happen. Two are done today. Three are done. I will give you three today. This is it. There are two more. Start with those three first. [Speaker 2]: Right. [Speaker 1]: And it will be done by next week. Now what is your schedule? Meeting schedule. [Speaker 2]: So what? [Speaker 1]: What is your class schedule? When are you free? [Speaker 2]: Sir, right. On Monday, there is a DBMS. Till when will you be free on Monday? On Monday, from 11 to 12.30, there is a DBMS. And then there is a KELA. 1, 2, 3. Then free. [Speaker 1]: So Monday or Thursday? Hello. Hello. Hello. Yes. Yes. No, we don't want anything. [Speaker 2]: Thank you. Sir, from 11 to 12.30, there is a DBMS. Then DBMS 2, 2, 3. Then free. [Speaker 1]: And till when",
    "created_at": "2026-01-25T08:23:58.392513+00:00"
  },
  {
    "chunk_id": 22,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 6,
    "text": "there is a KELA. 1, 2, 3. Then free. [Speaker 1]: So Monday or Thursday? Hello. Hello. Hello. Yes. Yes. No, we don't want anything. [Speaker 2]: Thank you. Sir, from 11 to 12.30, there is a DBMS. Then DBMS 2, 2, 3. Then free. [Speaker 1]: And till when is there a DBMS on Monday? There is no class on Friday. Till when is there a DBMS on Monday? [Speaker 2]: DBMS on Monday. Sir, from 11 to 12.30. So can we meet after 12.30? [Speaker 1]: Yes. When is your next class after that? [Speaker 2]: Sir, it is on 3.00. Till 3.00, right? From 3.00 to 4.30, there is a DBMS. You can come at 12.30. Yes. [Speaker 1]: So from 12.30 to 12.35, where is the class? Class is on C1.0. So it will take 5 minutes to come. You will come by 12.35, right? So we will meet at 12.30 on Monday. [Speaker 2]: Meet. Okay. [Speaker 1]: I will send the invite also. Okay. [Speaker 2]: Every Monday. Sir, when will you send the meeting? [Speaker 1]: I am making a classroom. [Speaker 2]: Okay. [Speaker 1]: I will make a classroom for this project. Actually, I have made a classroom for the other project. I will put you in that. It is called Multimodal Meeting Agent. [Speaker 2]: Okay. [Speaker 1]: Right. I will put you in that classroom. Okay. I will give you access to that in Slack. I will make a separate Slack for your project. Okay. I will make a new channel for your Slack, your project in Slack. But you will have access to both. But I will put you in one classroom. Okay. Because I am uploading a lot there. Why should I upload separately? I will put you in one place. Okay. And not only meetings, there are other videos, recordings, etc. All these things are there. You will have to take out the audio from the meeting recording. [Speaker 2]: Yes. [Speaker 1]: Then it will be a video. [Speaker 2]: You will have to take",
    "created_at": "2026-01-25T08:23:58.392516+00:00"
  },
  {
    "chunk_id": 23,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 7,
    "text": "you in one place. Okay. And not only meetings, there are other videos, recordings, etc. All these things are there. You will have to take out the audio from the meeting recording. [Speaker 2]: Yes. [Speaker 1]: Then it will be a video. [Speaker 2]: You will have to take out the audio of the video. Yes, I will do that. [Speaker 1]: You will have to take that out. Yes, I will do that. And you will have to index it. Right. Let's get one agent which will work on these three meetings. These three meetings. Right. You will have to say which meeting are we talking about and who am I? If I am Agreem, we will talk about the Agreem meeting. If it is you, you will talk about this meeting. If somebody is a third person, you will talk about that meeting. You will have to ask questions about that meeting. Okay. And that meeting will increase. You have three projects now. [Speaker 2]: Okay. So you will have to ask about each meeting separately. [Speaker 1]: Yes. Okay. When you ask, which project are you in? Not which meeting. [Speaker 2]: Okay. [Speaker 1]: Who are you? Say if you are Ashmit, then\u2026 [Speaker 2]: If you are Ashmit, then we will have access to that meeting. [Speaker 1]: Sir, not just the last one. There will always be a meeting next week. There will be a meeting every week. There will be many meetings for the same person, same student. It will be based on login. Okay. Who is logging in? Who is which? [Speaker 2]: Okay. Right. [Speaker 1]: Which is your\u2026 [Speaker 2]: Login will be based on that. Okay. [Speaker 1]: Okay? Yes. See? Yes. Done? Yes, sir. Next week, 12.30. Next week. All right. Thank you so much. All right. Come on.",
    "created_at": "2026-01-25T08:23:58.392518+00:00"
  },
  {
    "chunk_id": 24,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 8,
    "text": "Next week. All right. Thank you so much. All right. Come on.",
    "created_at": "2026-01-25T08:23:58.392520+00:00"
  },
  {
    "chunk_id": 25,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] Sit down, show me what's going on. Share it with us. So sir, I have started with generating the No, I have started with the manual chunking of the transcripts. I have to get all the transcripts from the video and then what I have done is I have read those transcripts and what I see the pattern was like I made the chunks accordingly like when you What was the goal? Goal was what? So goal was this that I will divide the transcripts in chunks and then what I will do, I will like to embed them So I will take those transcripts and embed them in a file like embedding I will do the embedding of the transcripts to chunks and then I will store them in a FIOS data structure. What FIOS data structure? You will do the So sir, it is like finding the similarities of the query and the transcript chunks or the cosine intervals Ok, fine. But that was for what purpose? Sir, to find the specific, like I have asked some question so I have embedded that question and then in the FIOS data structure, that question and the transcript chunk of that specific part will be matched and then I will give that transcript chunk to the LLM and he will explain accordingly This was what? This you have done? Ok, let's see how it goes Now there is another way of doing it You give the entire transcript to the LLM give the question and he will give some answer Sir, actually what was happening though I had given entire transcript to the LLM, it was translucent again It was like, I asked some question it was giving me some other answer It was happening so that's why I divided it into chunks So you have a baseline? This did not work? And this did work? And for how many meetings? Sir, only one How many meetings have I shared with you? Sir, you have shared me 6 meetings So whatever meetings I have shared you should have",
    "created_at": "2026-01-25T08:24:04.968724+00:00"
  },
  {
    "chunk_id": 26,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "I divided it into chunks So you have a baseline? This did not work? And this did work? And for how many meetings? Sir, only one How many meetings have I shared with you? Sir, you have shared me 6 meetings So whatever meetings I have shared you should have the baseline which is, I should give the entire transcript to the LLM and the asking question and give these problems you should document that Ok, then I will give him chunks Then you give chunks and you document because otherwise it's not scientific You have to say, I did this I tried it on 6 meetings, this happened then I tried this, this happened We need to be systematic in the process Ok, now show me what you have done this is for in the future, do it systematically so everything you try you record it, store it so that we can later on even that is something you are meeting transcript you should be able to say, you did this, this happened all that is results right now you are giving only transcript later you will also give results you will also give code you will also give everything 80 around Oxford whatever you are going to do for others that is the beginning your life will become easier so sir, first I have manually chunked them like when the topic was changing I read all the scripts so I chunked them accordingly then I have stored them in JSON file like every chunk in future I am thinking to automate this manually or automatically sir, but I was thinking when I automated this it was not like suddenly doing well because what was happening the topic changed and it was overlapping some topics but question is how much does that affect the final result even with those errors may be final result if I will give whole transcript so it can first tell me what have you done so what I have done is I have took a meeting so what I have done is I have",
    "created_at": "2026-01-25T08:24:04.968738+00:00"
  },
  {
    "chunk_id": 27,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "much does that affect the final result even with those errors may be final result if I will give whole transcript so it can first tell me what have you done so what I have done is I have took a meeting so what I have done is I have took a meeting so what I have done is I have given user id to the specific chunk and meeting name and meeting time and I have given text in the specific chunk so I have done this for all the chunks then what I have done is generate ratings is like sir, I have called that bg model which was embedding the files this is chinese model yes sir use a better model they do but this bg model is not very good I have used it anyway, its ok this is part of demo you should find a model which has good paper on it and all that chinese model is used for demo so everybody uses it actually bg is not the best everybody uses it I know but its not the best model just because they have a demo people use it so sir what I have done is I have specifically created chunk embeddings so what is happening is like it was happening that meeting was crossing with each other so what I have done is I have created those meeting name and those were crossing each other so it was not happening afterwards then I have stored this is like storing embeddings what sir? sir what was happening like Agrim and Ashmit are two different persons so what was happening is like it was firstly hallucinating at Agrim's meeting and if I ask one question, it was giving his answer and if I ask so I have created those meeting id so these were the same embeddings then in that it was the chunk embeddings that happened this was just like cleaning for the cleaning purpose for the overlapping purpose like sir what I have done is in a specific chunk it is taking",
    "created_at": "2026-01-25T08:24:04.968741+00:00"
  },
  {
    "chunk_id": 28,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "so I have created those meeting id so these were the same embeddings then in that it was the chunk embeddings that happened this was just like cleaning for the cleaning purpose for the overlapping purpose like sir what I have done is in a specific chunk it is taking the last 30 words and it is like overlapping that in the next chunk so that if something is ending with specific so it can follow so you have overlapping things overlapping things so chunk embeddings on that so sir now I have two things like this was the first one thing, now what happened here is like it was creating the data structure, it was taking chunks it was taking chunks of the probably this chunk embeddings and it was taking the vectors now what happened here is I have removed all vectors of all specific chunks but the file data structure is only showing numbers so in future I will use that, right now I am not using that, what I will do in future is I will create different vectors for each user so that it will not overlapping afterwards you have to have some more metadata exactly, afterwards I will be using it but right now I am just storing those so in that now in generate answer now in this what I have done is first show it working [Speaker 2] so [Speaker 1] so so sir what tree chunks is doing is right now I have not used that vector index what it is doing is it is taking your user query and that it is creating in embeddings all user specific like I have taken one user I have taken user id of a user it is embedding that query and then from the stored transcripts that chunk embeddings I am not using vector index it is matching it is going in file data structure and then it is matching that user query embedding and stored chunk embedding then what's happening is ranking so I am ranking the most matched answers so you",
    "created_at": "2026-01-25T08:24:04.968744+00:00"
  },
  {
    "chunk_id": 29,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "the stored transcripts that chunk embeddings I am not using vector index it is matching it is going in file data structure and then it is matching that user query embedding and stored chunk embedding then what's happening is ranking so I am ranking the most matched answers so you are doing a direct right now I am doing direct in the function right now in this video you are retrieving all of them and just storing them right now all the user specific what's happening is like if I have taken the user id of Ashmit so he will only retrieve Ashmit's embeddings and then you are just doing a comparison with all of them and finding the top top 1 or top 3 then you are giving that to the relevant so then supervisor agent supervisor agent is just the pipeline like I am taking that it has a function to find the nearest next you are finding the top 3 like that it does the ranking by itself it does the ranking by itself it does the matching by itself so then supervisor agent is just the pipeline that I just showed it is just taking the input and giving the output so let's take a random transcript I will take like which transcript is it sir this is my live meeting we can take some data like which was just ask some question about your meeting sir we can ask at what date was the next meeting I will connect this to the what is happening here why it is taking so long sir I think wifi wifi is here because of the meeting so sir I have entered the user ID Ashwin now it will be taking the embedding of those what I am doing in the session ID right now I will make memories I have not implemented a lambda but what I will do later is like I have asked some question like what was the date or what something so I will then ask the follow up question like explain me this",
    "created_at": "2026-01-25T08:24:04.968747+00:00"
  },
  {
    "chunk_id": 30,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "in the session ID right now I will make memories I have not implemented a lambda but what I will do later is like I have asked some question like what was the date or what something so I will then ask the follow up question like explain me this question in specific detailed answer so it will take the previous question to the state to the state and then I will create that in next week this is random right now right so we can ask so it gave the answer it took the specific chance and it finds the ID and it gave the answer sir ask that the next meeting will be scheduled for next week what was the main action item for next meeting sir I will one again right now so what was the main action item discussed discussed and finalized we need to build a simple web based meeting agent that answer question for the meeting ok but what was the action item the immediate action item for the next week for the next week [Speaker 2] this is general [Speaker 1] which is correct for the immediate next week sir one more doubt that immediate deliverable immediate deliverable for the following for the following meeting for the following meeting but it is like the menu simple web based meeting to be integrated first option we agree that web based was the first one we agree on web based before next meeting yes sir you said web based and then i said web based is a problem and then i said command line command line why is it web based? because web based was planned who suggested web based meeting? web based was there in the paper in the paper first we put a slash and then in the end command line search you said a slash integration command line command line is written it is there but it is not retrieved in the chunk it is not retrieved in the chunk the immediate thing the point is this embedding retrieves the long chunk",
    "created_at": "2026-01-25T08:24:04.968749+00:00"
  },
  {
    "chunk_id": 31,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "slash and then in the end command line search you said a slash integration command line command line is written it is there but it is not retrieved in the chunk it is not retrieved in the chunk the immediate thing the point is this embedding retrieves the long chunk you have to devote this because this is a very important thing if the long chunk comes the answer will also be different what were you asking? [Speaker 2] sir i was asking can i normalize the transcripts [Speaker 1] like this transcript not this one [Speaker 2] this [Speaker 1] this is his transcript like this it was like hallucinating once can i normalize transcripts is there any model that can normalize transcripts this was some issue from the transcript this was a good one we told that we will put in the bit it can read full and then it can check the code accordingly this is yours someone came in between remember somebody came in there is somebody outside someone was standing outside sir but in video when i was listening to this transcript we were talking about something but that was not coming there was a noise sir i was asking is there any model that can normalize transcripts no you can go in Gemini and say get rid of stuff which appears to be noisy no no no so first i give transcripts to LLM then i store the transcripts so what is the action item now next time first of all i am thinking to automate that second one more thing so sir web based don't do web based it will take time keep it command line but keep it so that everything is properly in your own database and you can you log in to your chats it should tell you that meetings are coming for you both guys will be users each meeting will have all participants you log in all your meetings will be there and then you should be able to ask questions one by one by one and keep",
    "created_at": "2026-01-25T08:24:04.968751+00:00"
  },
  {
    "chunk_id": 32,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "in to your chats it should tell you that meetings are coming for you both guys will be users each meeting will have all participants you log in all your meetings will be there and then you should be able to ask questions one by one by one and keep the context i ask one question same thing chat based but keep the context explain me the previous question then sometimes you have to go back from this context it may have to go and get one more chat right that's what you have to think clean it sometimes it has to get more chat sometimes it will just ask based on the current context it may need to go back and get one more meeting it is deciding what to do whether to go or not that decision is being made by two agents one is going to give the response other guy is going to decide to give more context ok either the previous answers or chunks previous answers and chunks do i need more information or not make it make it a proper system that i have so many things stored in the database i can retrieve based on the user id then keep asking questions and chat basically that thing should be the user the terminal layer should be very simple it should be something which you can take that backend piece and connect it to a slack bot also we will put slack bot in our meeting in our channel so backend we will host it and every time we will push have you made the repo? push it make me a collaborator and we will put a company also and they can also be a group they can also sit here let us get this working in a way for all our meetings yours, other guys whatever is there for these three groups where i am recording the meeting we are able to have on slack a conversation with the previous what is happening let us get a system working first by that time",
    "created_at": "2026-01-25T08:24:04.968754+00:00"
  },
  {
    "chunk_id": 33,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "this working in a way for all our meetings yours, other guys whatever is there for these three groups where i am recording the meeting we are able to have on slack a conversation with the previous what is happening let us get a system working first by that time the company will wake up and they will provide their profit notes whatever they are recording we can use that ok sir the agreement the agreement now he is now able to pass the videos where is the demo what is the code that will also come in the transcript what did you show on the screen last time it will come clear any other question because if you have a question you better ask otherwise you will get lost close it sir so sir right now i was thinking that i have to create a system i have to implement a lambda that it can also answer the follow up questions and also i will be continuing this right now i am it should continue [Speaker 2] and [Speaker 1] it should also decide whether or not to get a new job so basically is he asking a different question which requires me to go back and get a new job or is it on the same context or do i need to go and get for that there will be a separate agent and sir that will be happening for a specific user yes for a specific user so sir in the command line i will first input the user id and then it will be continuing the transcript and other thing is the meeting that you get back you will have to get back for that user for that particular conversation right now it is for user only so we will do that later do it continuously once it is continuous then we will add help and slack that will be the first system and how is it going to be stored yes it is being stored in the file i am extending it along with every",
    "created_at": "2026-01-25T08:24:04.968757+00:00"
  },
  {
    "chunk_id": 34,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 9,
    "text": "only so we will do that later do it continuously once it is continuous then we will add help and slack that will be the first system and how is it going to be stored yes it is being stored in the file i am extending it along with every in the database there is the user id also there date everything is there meeting time yes meeting time and date will be there it is not just a vector you are also searching based on some other parameters so sir it will be storing that chat in a file for all questions and answers after the chat is off and this is chunk automation chunking automation understood then we will see how to deploy how to extend sure",
    "created_at": "2026-01-25T08:24:04.968760+00:00"
  },
  {
    "chunk_id": 40,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 0,
    "text": "[Speaker 2] I am an evaluator. I'll be evaluating your presentation based on the project and task requirements. Please begin when you're ready. Note, this session is being recorded. [Speaker 1] Hello everyone, this is a project agent in AI. So it is an AI chatbot application that simulates the patient behavior, which interacts with a user who is acting as a doctor. The message sent by the user goes to the backend through FastAPI, which acts like a bridge between the frontend and the backend. Then the message goes to the land graph, where it is classified. This intent classification helps the land chain to create a response accordingly. We also maintain patient memory so that the bot does not forget the context of the conversation. The tasks performed by the land graph, land chain, and patient memory together are known as the agent, which is created for a particular session ID. Tasks are completed with the help of an LLM accessed via API key. The reply generated by the agent goes back to the frontend through FastAPI. Technology used. The frontend is deployed on Vercel. The backend is deployed on Render. The Identity AI is built with LangChain and LangGraph. The LLM is served via Grok for fast response generation. The backend is coded in Python and the frontend is built using HTML, CSS and JavaScript. Key design choices. The message which goes to the LangGraph can be classified as an intake message, a treatment message or a closing message. Once a message is classified as closing, a conversation-ended flag becomes true, and any further input receives a session-ended response. A new session ID can be created using the reset button. [Speaker 2] You mentioned that the message goes to the LangGraph where it is classified. Could you elaborate on how this LangGraph is implemented and what features of the message it uses for intent classification? [Speaker 1] I would like to show you the code for intent classification. We use an LLM here, which is Grok, accessed through the API key. We set the temperature to zero",
    "created_at": "2026-01-25T08:24:52.813311+00:00"
  },
  {
    "chunk_id": 41,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 1,
    "text": "elaborate on how this LangGraph is implemented and what features of the message it uses for intent classification? [Speaker 1] I would like to show you the code for intent classification. We use an LLM here, which is Grok, accessed through the API key. We set the temperature to zero so that we get deterministic output. In the prompt, we define rules based on key terms. For example, intake when the user says hi or hello, treatment when treatment is recommended, and closing when the user politely ends the conversation. The message is classified and a JSON value is returned by the LLM indicating whether the message is intake, treatment, or closing. [Speaker 2] You mentioned setting the temperature to zero. Could you elaborate on why you chose this specific value and what effect it has? [Speaker 1] Temperature zero ensures deterministic output. Since intent classification only determines user intent, randomness is undesirable. This helps LangChain generate clearer responses based on intent. [Speaker 2] I'm following your presentation. Please continue. [Speaker 1] Here is another demonstration of chatbot functionality. The session ID created on the frontend goes to the backend with the user message. If that user ID already has an agent, the same agent is reused; otherwise, a new agent is created. [Speaker 2] You mentioned that the same agent is reused. Can you elaborate on how you ensure the correct agent state is maintained? [Speaker 1] We maintain a dictionary that stores session IDs as keys and agent objects as values. When a request comes, the dictionary is checked. If the session ID exists, the same agent is used, ensuring isolation between sessions. [Speaker 2] You mentioned that the object stored is the agent created for each session ID. Could you elaborate on how this agent object is structured? [Speaker 1] The agent object is created using a create_agent function, which invokes the LangGraph. The agent is activated and replies are generated through it. [Speaker 2] You mentioned that the agent is basically a patient which contains a LangGraph. Could you elaborate? [Speaker 1] The LangGraph maintains the",
    "created_at": "2026-01-25T08:24:52.813325+00:00"
  },
  {
    "chunk_id": 42,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 2,
    "text": "structured? [Speaker 1] The agent object is created using a create_agent function, which invokes the LangGraph. The agent is activated and replies are generated through it. [Speaker 2] You mentioned that the agent is basically a patient which contains a LangGraph. Could you elaborate? [Speaker 1] The LangGraph maintains the control flow. It classifies intent so that LangChain can generate responses based on intent. The graph structure ensures the flow between intake, treatment, and closing states. [Speaker 2] I'm following your presentation. Please continue. So here is a demonstration of our application. [Speaker 1] I can start the conversation by asking about symptoms. The bot responds accordingly. I can ask follow-up questions, and responses are generated based on context. The session ID is created after the first message and reused for subsequent messages. I can give treatment intent, accept treatment, and then end the conversation. Once the conversation ends, further messages return a consultation-ended response. [Speaker 2] Evaluation has been successfully saved. You may now disconnect.",
    "created_at": "2026-01-25T08:24:52.813329+00:00"
  },
  {
    "chunk_id": 43,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] So, this is my vision glass. This is the range parameter that how many centimeters or how many units it can do and this is for example, you cannot see 60 degrees. Like we can only see what is ahead of us. That in radius like that it is there. And this is how much the object is visible. So, this is a vision glass basically. [Speaker 2] So, this is what we observed. Try to keep it simple, keep everything simple, make sure the system is not forgotten. [Speaker 1] We are adding rooms with width and height. A person can only see up to 5 meters. If an object is at 6 meters, it should not be visible. This is implemented in demo.py. We created a world object and added walls. If a wall is in the field of view, objects behind it are not visible. Entities have types and locations. There are multiple agents, and currently we are printing their visible objects. [Speaker 1] In Jericho, each node represents a room. We need to define which objects exist in each room and specify coordinates for vision to work. [Speaker 2] When Jericho returns a message, it will list visible objects. Try to keep the first cut simple. First configure Jericho with your system without extra complexity. Show Jericho working first, then extend it later.",
    "created_at": "2026-01-25T08:25:01.604723+00:00"
  },
  {
    "chunk_id": 44,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] I've shared a couple of links with you just now. First, there's this paper, Generative Agents, right? You've seen that, right? So what they did is they put lots of relevant agents in a word and let them walk around and do things and make things happen, right? Read it carefully, how they've done that. You read it? The second one is this one, which you know about text-based games, right? Adventure games and Zork and stuff like that, right? So this put LLM inside those games and said, can you play them? So read this paper, right? And this paper has a link to their code, which is not, I've just tried to get it unlocked from the IIIT network, but it's right now some blockages there, so I'm telling you on my phone, right? I said this is not a game site, this is actually a research paper site, right? So just because it's called .ai or something, it's, but basically they have various games and they have the code for that, et cetera. This uses an environment called Jericho, right? Which allows basically, if you look at the quick start, you basically can, you have to do all this stuff and then you get the games, right? And then you basically create the environment, right? So you have to get this Z5 environment from whatever this Jericho game suite, right? You have to put this somewhere and you do, right? And you just see if you can use this. Basically, this lets you play the game in Python. So this TextQuest actually uses this environment to do play games, right? So what we want to do is first, all these games are single user games, right? So I'll start with that, that single user games are a problem. But first goal is let's get an LLM agent to play a game, right? The game environment will have a description of what's happening, et cetera, et cetera, right? And what we need to do is once the LLM agent goes and plays the game and maybe",
    "created_at": "2026-01-25T08:25:18.532675+00:00"
  },
  {
    "chunk_id": 45,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "are a problem. But first goal is let's get an LLM agent to play a game, right? The game environment will have a description of what's happening, et cetera, et cetera, right? And what we need to do is once the LLM agent goes and plays the game and maybe you should make it talks to itself, maybe it talks to somebody else, et cetera, et cetera. So let's say it plays the game, one. Second, let's see. There's this. I basically generated, I used the VO3 video generator of Google, right? So you basically, you have to create it. So we'll come to the keys issue later, right? So you have first person view of talking to a customer and trying to sell them a washing machine, right? Or other case, I had various different. So basically, we generated some videos like this, right? [Speaker 2] So this is a great deal. [Speaker 1] So it generates a video. It's pretty costly, right? So each of these things will probably end up eight seconds But you have, have you created an account in Germany? [Speaker 2] Are you exhausted? [Speaker 1] Are you exhausted 300 credits? When did you create? Last month. You still have some time. You have those 300 credits free, right? For example, I have over here my Germany account. If you associate a billing, look at the credits. So when you create an account and make it a paid account, right? You get 26,000 rupees. It lasts for three months, but that's okay. Enough for this course, right? So you have two months of your account left, right? And then you can use my account after that, right? So I hope you won't exhaust, but your time will exhaust. But then after that, you can use mine, right? So you can be pretty, 26,000 means pretty much like 2,000 videos, right? Isn't it? No. 200 videos, right? Those four videos are occurring, right? So 200 videos, I mean, each second may cost you 100 rupees to make a video or something, right? That's what, for",
    "created_at": "2026-01-25T08:25:18.532683+00:00"
  },
  {
    "chunk_id": 46,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "use mine, right? So you can be pretty, 26,000 means pretty much like 2,000 videos, right? Isn't it? No. 200 videos, right? Those four videos are occurring, right? So 200 videos, I mean, each second may cost you 100 rupees to make a video or something, right? That's what, for eight seconds, right? So it depends, right? You can see. But don't be too free about making videos, okay? Now there's a 1,000 rupee joining fee to pay. You can get, I'll write a note. You can get it back from there. And even if it goes above this, whatever it is, you can get back from there. But don't spend too much on the videos, okay? [Speaker 1] So the idea is this. First goal, use the Jericho environment, study those papers, how they made the LLMs, do various things, etc. Now the LLM is not just doing it blindly, right? So some of the ideas of the Simulca paper, you'll have to start thinking how to do, right? You'll keep the memory, what it's doing on, etc., etc., right? And so for the first goal, I think in January, we should get the situation where for one or two or three games, you can play a game. And then for any particular part of that game, we can generate a small video, right? So the game has all of the transcript. I did this, I did this, I saw this, I saw this. And then you generate a video of the player playing that game as if he's going there. So it's like you're creating that clip, right? And then that's step one, right? Hopefully this month we can do that, right? So just one clip for one eight-second period. Then there may be multiple clips. This concept of having continuity, having the same look and feel throughout the game, all that can be done, but we won't do that now. So that requires taking the first image and last image and making sure you give those two images and the video will interpolate between them. So it",
    "created_at": "2026-01-25T08:25:18.532685+00:00"
  },
  {
    "chunk_id": 47,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "of having continuity, having the same look and feel throughout the game, all that can be done, but we won't do that now. So that requires taking the first image and last image and making sure you give those two images and the video will interpolate between them. So it looks like the same environment you're playing through, right? That is the part two, right? But after first month, part two ke pehle, what we want to do is we want to, that you can start, once you start understanding this environment, you start understanding how to make a non-playing character in this environment, right? Or something in this environment, which is, can speak or do something like using an agent, right? Rather than the agent only taking actions and just creating a video with hardly any articulation, maybe music or whatever. There is somebody who's speaking back, right? Some animal says something, some, whatever, these are various types, right? So how do you bring a non-player character into this environment that you do some R&D on, right? You do that while you're doing this basic work, right? And then the step two will be that now you have non-player characters who are LLM agents, right? They are speaking, you are going, you are speaking to them as an agent, right? So you are articulating something to them, which is going into the, some is going into the game. Some is going into your transcript of the game. Means what I speak normally doesn't go to the game. Game doesn't care what you speak. Game only cares did you go right, left, did you pick up the sword, did you do this, did you do that, right? It doesn't care what you said. But you keep that speech here. What that guy said, once you have made changes to this, so that there's a non-player character who can speak, not just take action. Then their speech will come through the game and come back to us or come sideways somehow, some channel you have to make, right? And that becomes",
    "created_at": "2026-01-25T08:25:18.532686+00:00"
  },
  {
    "chunk_id": 48,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "that guy said, once you have made changes to this, so that there's a non-player character who can speak, not just take action. Then their speech will come through the game and come back to us or come sideways somehow, some channel you have to make, right? And that becomes part of the transcript. Next time you have video in part two, you will have not only just the environment, but you will also have other players talking to you in that game. And that is what gets animated. So it becomes like a mini story. So that's the basic thing. And if you can get that far, it would be very nice. Then we can see if there's time we can try to stitch the videos together to make them continual. And then, of course, if we get even that far, I don't think we will in one semester. But suppose now you want to put your avatar into the game. That means you will register your face and your personality and say, now you will speak with my personality. That's just prompts, right? Prompts and that's all bells and whistles. This can be done later. Right. OK. A core engine has to be built. No UI, no UI. [Speaker 1] Let's just if I just add the code level, let's build this. Then we can make we can think about making that later. Right. If you want to do it in some other time or whatever else you want, somebody else will do it. Is it clear? Tell me now what you expect to do in the project and what you expect to do. [Speaker 2] So one thing I want to say is that I read the generated engine paper and I tried to build something like that. So from Python, what I will do is, let's say we build a 2D world. We will be adding rooms of cuboid shape or multiple cuboid shapes to make one room. And in those rooms we can add multiple agents that will be interacting with each other and talking",
    "created_at": "2026-01-25T08:25:18.532688+00:00"
  },
  {
    "chunk_id": 49,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "that. So from Python, what I will do is, let's say we build a 2D world. We will be adding rooms of cuboid shape or multiple cuboid shapes to make one room. And in those rooms we can add multiple agents that will be interacting with each other and talking to each other. So for that in Python, I coded a message queue, like message bus, that if one agent wants to send a message, it will send to this message bus. It will be thing based, like every agent should take some action or process what message they have received, read the inbox. [Speaker 1] So you already have the core elements of a multi-agent thing, but you have to stitch it into this. Why? Because these games already have some descriptions of castles or... Oh, who wrote the descriptions? Already have, because all the games are built in. Let's just use that, because somebody has written it.",
    "created_at": "2026-01-25T08:25:18.532689+00:00"
  },
  {
    "chunk_id": 50,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] speaker, you just connect Okay, so the first thing that I tried, it was that I tried first in Jericho, like how can we, I made a simple script first that, you can just start it. So just a basic script that will be, that will ask us for actions and this was one of the games that was invented in Jericho. Hello? I think it will... So, basically, we can choose our options and... So, I was exploring, like, how it works in the games or things, I don't know. So, this was, like, the user is giving an input to the agent and it is taking action and doing all those things. So, basically, this was the first thing and then I tried... This is just plain Jericho. So this was like I wanted to explore how it works. So I saw that. So how the environment works. So that's the first thing that I see. And then you might try to inform. So first, I want to also say that I am. So it will provide me an ID, it's private. So then the next thing that I said, I want to inform seven. So there are lots of rules within it. So what happens is that we can create like a room. So we have to create a room and then we have to create items and every item. What is this? This is inform ID. So this is the language that is used for creating Jio games. we can create rooms, we can create objects and entities. So, every object will have its own properties and we can interact. So, basically, this what happens is that all environment are reset. First of all, environment is set. And this is environment. [Speaker 2] I know that. Now, tell me, in the framework you had created. [Speaker 1] Yes. [Speaker 2] There was a concept of an agent, right? Doing something. [Speaker 1] Yes. [Speaker 2] Have you done anything together with that or this would be an independent game? The thing that I showed",
    "created_at": "2026-01-25T08:25:33.994945+00:00"
  },
  {
    "chunk_id": 51,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "know that. Now, tell me, in the framework you had created. [Speaker 1] Yes. [Speaker 2] There was a concept of an agent, right? Doing something. [Speaker 1] Yes. [Speaker 2] Have you done anything together with that or this would be an independent game? The thing that I showed you last. [Speaker 1] Yes. No, this is independent. The thing that I was struck about is that, okay, I also tried making my own game and all. I tried to make an NPC and all. [Speaker 2] What was the goal? What was the goal you set for last? [Speaker 1] I had to create an agent in the Jericho that interacts and then I had to also check out the Jetman API, how we can do. [Speaker 2] The idea was that you have an agent. What did you do last? What did you show me last time? [Speaker 1] Last time I showed my script that creates walls and room and, it recognizes that in the field of view these objects are there and agent can send messages to each other. That's what you told me to explore with Jericho. [Speaker 2] The idea was that your agent could be a rapper around Jericho. That was what we discussed first. Your agent, there is going to be some position of that guy. For you, for Jericho, you are moving around in various rooms. When you move to a particular room, then your agent sort of come into your last time's agent is sitting in that room. It has, what is this, there are some items, no? [Speaker 1] Yes. This is the observation. All this, this is a particular thing and then the item, what is the event, what is there in that room? We do inventory. This is what the description shows. Every room has like text description. [Speaker 2] I agree, but what else is there? There must be some item in that room, right? There are some things that you have etc. etc. Now the idea was that once you're doing all this, you're",
    "created_at": "2026-01-25T08:25:33.994956+00:00"
  },
  {
    "chunk_id": 52,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "This is what the description shows. Every room has like text description. [Speaker 2] I agree, but what else is there? There must be some item in that room, right? There are some things that you have etc. etc. Now the idea was that once you're doing all this, you're moving around in this room, What was that we discussed last time? Tell me. You're supposed to summarize that also. We didn't do that. Right? So, the agent will be the basic. So, the agent will be able to interact. [Speaker 1] And with what? [Speaker 2] With, we will have our script on top of Jericho. Then the agent will be able to interact. So, agent can interact with Jericho is fine. It's just a matter of writing an LLM which will do things. Right. Which is, anyway, You just have to call them and you do something to take the output you give. What was the, we discussed something that you could write a wrapper on [Speaker 1] One thing we had discussed was that, okay, my code was about how the room looks. Like, in the room that at this position, this object is there. So, Jerry code just say that agent is in that room, and within the room, at what position, what things are there, that will be your Python code. That will not... That's not possible in Form 7 and Jerry. [Speaker 2] That's fine, okay, right. So, what have you... So, you put that aside. [Speaker 1] Yes, there can be a workaround, so I am trying those things also. But that you are not doing. But directly it's not possible. [Speaker 2] Alright, so now what are you, what is your goal, what are you trying to do? [Speaker 1] So, this is the one thing I did and the second thing, using inform rules I tried to make an NPC inside inform. So, basically, [Speaker 2] You made a game? [Speaker 1] Yes, using Inform7, this ID. And that game is being run in this script. So, what happens in this",
    "created_at": "2026-01-25T08:25:33.994958+00:00"
  },
  {
    "chunk_id": 53,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "is the one thing I did and the second thing, using inform rules I tried to make an NPC inside inform. So, basically, [Speaker 2] You made a game? [Speaker 1] Yes, using Inform7, this ID. And that game is being run in this script. So, what happens in this game is that there is a server here, that's our NPC basically. So, I am acting over all commands, the house is to the node. You can see a heavy chest and a saboteur there. I will do things, so here the saboteur arrives from the north. So what happens is that I try to base the NPC. It will move around north and south and take the chest. It will do things on its own. So I did not command the saboteur to move. [Speaker 2] Can you insert this saboteur in another existing game? By modifying the code of that? [Speaker 1] If I get the infant code. [Speaker 2] So, this is my game. [Speaker 1] No, but inside the Jericho, the master, all that could be there, right? [Speaker 2] Yes, all of that would have that, is it not, the Z5s we could have that? You can open it up. So, can you actually put a character inside that game? [Speaker 1] I can. [Speaker 2] You can, right? So, now what we should do is, so you have explored Jericho, you know how to put NPC. Now, what we need to do is, you need to, have a mechanism to put a NPC in one of their games. Because I don't want to write the entire description of words and all that stuff. Well, that creative stuff, I don't want to waste that. And somebody already done it. We'll work with that. But we should be able to put some NPC into that game. That NPC will, suppose you as a player go into that game, right, and you see an NPC there. You should be able to talk to that NPC. How to do that? Now there is a saboteur is there. Saboteur",
    "created_at": "2026-01-25T08:25:33.994960+00:00"
  },
  {
    "chunk_id": 54,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "should be able to put some NPC into that game. That NPC will, suppose you as a player go into that game, right, and you see an NPC there. You should be able to talk to that NPC. How to do that? Now there is a saboteur is there. Saboteur is now sitting, it's there, that guy only knows the saboteur is there. The fact that that saboteur is there, there should be something about that saboteur in some database which is outside Jericho, which is your database, there is this saboteur, it has some phase, whatever, etcetera, some metadata is there, and now if I want to speak to saboteur, I should be able to put an action here, speak to saboteur. That action should be picked up, that will not go to Jericho, that will go to your system, that will go and say I am speaking to saboteur, And saboteur will be, will say, okay, this is what I said. Saboteur will respond. There will be a conversation between you and saboteur. Jericho is not aware that we are talking. Jericho has no clue. Jericho only deals with movements of things. And what, who picks up, what is where, etc. And then they can move around and separate. Right. So, the idea is this, that right now you are talking to saboteur. The saboteur may actually give you an avatar which is actually playing this game with them, right? So then you are basically making this whole thing into a multi-user system. Many people can be playing this game. So they are all NPCs, as far as I am concerned, they are NPCs, right? And for that guy, you are the NPC. So we will create a system where The next two players are playing on both the systems, they are the agent and the other is the NPC. But before that, before you come to that situation right now, suppose there is an NPC, the fact that the NPC is being controlled by somebody else, that is part two. Think about it. But the idea is",
    "created_at": "2026-01-25T08:25:33.994962+00:00"
  },
  {
    "chunk_id": 55,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "the systems, they are the agent and the other is the NPC. But before that, before you come to that situation right now, suppose there is an NPC, the fact that the NPC is being controlled by somebody else, that is part two. Think about it. But the idea is that can you talk to them? So then when you can, talk, then it's like, I go there, there is this guy over there, like, there's a context of that guy and my conversation, then that guy moves around, I move around, that guy comes back, then that conversation is retrieved, then I can continue the conversation with that guy. So it's like, it's like that simulacra, simulacra, but instead of moving around in those rooms, you're moving around in this game. And these, these things are all moving around and you're doing this. But now, so the control of this saboteur, Right now, you are able to move that way around, right? [Speaker 1] Yes. [Speaker 2] How? [Speaker 1] By, actually, my agent or the cemetery. Cemetery, how does it move? [Speaker 2] Cemetery, it moves on its own. So, what happens is that there are, it informs that there are every turn rules. So, once I did an action, it's like a tick. So, after every tick, every turn rules are activated. So, cemetery will have, there's a random function and inform also. So, it will do, like, cause a point, And it will say I should wait or I should go somewhere. That's your code. [Speaker 1] You've written that code. For saboteur. That saboteur code could also be an agent playing the game. That's saboteur. But then, Jericho, do we have like two agents? [Speaker 2] I think only one agent. [Speaker 1] No, no. Only one. But that, what that saboteur needs to, he's in a particular room. You know, what is all in that room, you know. It can decide to move there, there, etc. It can decide, right? That's a decision that you can take, right? Just like you are taking right",
    "created_at": "2026-01-25T08:25:33.994964+00:00"
  },
  {
    "chunk_id": 56,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "Only one. But that, what that saboteur needs to, he's in a particular room. You know, what is all in that room, you know. It can decide to move there, there, etc. It can decide, right? That's a decision that you can take, right? Just like you are taking right now, right? One of the guys can be the main agent. Other guys are NPCs are playing the game, right? For the moment. That NPC, what it does, it's up to you to decide how it does, right? Instead of having something random, that guy can also be seeing and doing various things. In some sense, that can be an NPC agent. It can call anything to decide what to do, right? Now the, now that's the tricky part, that inside the inform7 code, there are after rules. So, in the inform only we have to specify rules. [Speaker 1] But can it not, that rule cannot call code. So, yes, that's, that's the thing, that the rule works within itself. [Speaker 2] But that rule can, it's just some Python code or what is it? [Speaker 1] No, those are inform7 code. InformSum. So it's a different language, like this. I guess you can see that the lab is a room. So InformSum, there are objects. There is a room or like this only. It can create room objects and every room has, we can add a west, north, east, south, four directions. So into four directions, we can add another room. So it's a very constricted language. So where is that 7W? How is that added? Let me see. It may be in another file. This one was... I think I have overwritten that code. Okay, yes, this is the same code. The garden is a room and this is a description of that. So the house is north of the garden. So like this is the code of that. So if we write the house is north of the garden, there will be room here. The player is in the garden. So that means us. The heavy",
    "created_at": "2026-01-25T08:25:33.994966+00:00"
  },
  {
    "chunk_id": 57,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "this is a description of that. So the house is north of the garden. So like this is the code of that. So if we write the house is north of the garden, there will be room here. The player is in the garden. So that means us. The heavy chest is a container in the house. It is closed and locked. So, closed and locked are its properties. They are Boolean properties. So, this language is there. The saboteur, where is it defined, doesn't it? The saboteur is a man in the house. So, it will start in the house. Understand, wait is waiting. So, carry on waiting. So, we added extra action of wait to the saboteur because it can choose or not to choose to do an action. So, there is no option in inform that we cannot do an action. So I created an action that's paid. It's called paid, so it does nothing. So every time when the saboteur is in the house. So a turn is done and we give it a command. That's one turn. So every time when the saboteur is in the house, it carries a silver cane, heavy chest is locked, try the saboteur and lock it in the hand. So like that, after every turn, these rules will be applied to saboteur. And it will decide whether to do it or not. So this is the random one. So that may be a random direction. then destination will move away from location of servitor. If destination is a room, try the servitor going there. [Speaker 1] Now this goes into a file, is it? [Speaker 2] Yes, this will be, we can release it and like that. Release where? No, it will release as a Z8 or Z55. The entire code, game? Yes, this code. So, for now it's Z code. But the Z code is for the entire game? Yes, it's for the entire game. And this is a tool? Yes, this is IDE. [Speaker 1] So, then some other mechanisms have to be used, right? So, then",
    "created_at": "2026-01-25T08:25:33.994968+00:00"
  },
  {
    "chunk_id": 58,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "entire code, game? Yes, this code. So, for now it's Z code. But the Z code is for the entire game? Yes, it's for the entire game. And this is a tool? Yes, this is IDE. [Speaker 1] So, then some other mechanisms have to be used, right? So, then in some sense the fact that this particular, so in some sense you are going to have to have the game is there which will only have the descriptions of various things, right? And there will be, which room the saboteur is in we do not know, right? If it starts in the house, it starts in the house in 7 years and then it every turn it goes randomly to one room and if it is in the house then it will and it sees a chest and it has a key it will unlock it. These are the rules of 7 years. Okay, so let's assume that there is a for the moment there is a there is an NPC in various places and they can move randomly. Now the question is if you enter a room, and that NVC called saboteur is there, right? It's moving around, it's a nice, it's not a reason, right? But the talking to that guy, and maintaining the context of your conversation with that guy, that is up to you. That is in your own. If you come across that guy again, then the context is retrieved and the conversation continues. You understand what I'm saying? So this, there could be ten to five different agents sitting in various places all moving around randomly. If they come together, then this is, the conversation starts from where you have talked to them, you have talked to them, now the conversation of both of them together, you are talking to each other, right? And they are also talking back to you. So conversation between you as an agent and these NPCs, you are taking care of in your system. I see. You are playing this game, there are some NPCs which are sitting",
    "created_at": "2026-01-25T08:25:33.994970+00:00"
  },
  {
    "chunk_id": 59,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 9,
    "text": "of them together, you are talking to each other, right? And they are also talking back to you. So conversation between you as an agent and these NPCs, you are taking care of in your system. I see. You are playing this game, there are some NPCs which are sitting over there and they are talking to you, right? You are the one who is moving around in the game, right? They are moving randomly, right? At the moment, you can also be moving randomly. How does it matter? You are moving randomly, they are moving randomly. So in some sense, both of them are equal. We can make it more stateful. I can add some properties to separator that will change with every interaction so we can make it more stateful also. So it will not be completely random. That change is going to be hard coded inside this code. [Speaker 1] Yes. So let's say the properties of separator is that like happy equal to true or false. So, if he interacts with us, he is happy. If he is happy, he is more likely to go to North. There is no way to get just this particular side of this code can... On run tag, we cannot do. On run tag, once the game is running, then we cannot install this element on the code. So, the decision making will be hard coded in this code. [Speaker 2] No, but it can read things which are in the room, right? [Speaker 1] Yes, yes, yes. It can say what things are there in the room and we also have a guard view feature that we can, for the user also, we can retract everything that is there in the whole room. So, we can make such rules like for a Saturday. [Speaker 2] So, it can read everything which is there, right? Right. And that, everything which is there can be written by anybody else? Can we return? Can you... Say that I want to place something in the room, that something has properties which you decide.",
    "created_at": "2026-01-25T08:25:33.994971+00:00"
  },
  {
    "chunk_id": 60,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 10,
    "text": "like for a Saturday. [Speaker 2] So, it can read everything which is there, right? Right. And that, everything which is there can be written by anybody else? Can we return? Can you... Say that I want to place something in the room, that something has properties which you decide. From the Python's group. Or from your, when you talk to zero you can say I can, I want to put a note in the room which says this. [Speaker 1] We can do that. Text which is your own text. [Speaker 2] But like the objects must exist at the starting. [Speaker 1] But can you change the properties of the object? [Speaker 2] Yes, we can change the properties. So what properties? [Speaker 1] Yeah, like the chest was locked. No, but can it have text in the properties? [Speaker 2] Yeah, it can say. So, like, we are saying, say you wait. [Speaker 1] Yeah, but can that you wait? Suppose I say, I write, there's a piece of paper there. Okay. And as an agent playing the game, can I say, write on this piece of paper something? I have not seen this. We can probably do that. Because descriptions exist for every object. But that description is changed here, or changed by the agent. [Speaker 2] No, it can be changed. Probably changed. In Enform score or by the agent? In Enform score. [Speaker 1] But by the agent acting, you can't change it? OK. So then maybe this stuff about what is there in which room, et cetera. So that's what I said in the beginning. You need to take this game and map out the entire thing and create a copy of that map, the graph. Right, and then the, what is happening, what is the context of each agent with each other agent is kept with you. You have a sort of parallel copy of what's going on over here, right? And you can keep running that and figuring out what's going on over there. You have a God view, right? You",
    "created_at": "2026-01-25T08:25:33.994973+00:00"
  },
  {
    "chunk_id": 61,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 11,
    "text": "what is the context of each agent with each other agent is kept with you. You have a sort of parallel copy of what's going on over here, right? And you can keep running that and figuring out what's going on over there. You have a God view, right? You know what is where. You always know what is where. You can keep that updated. And then, then the context of, what that agent is thinking, doing, etc., etc., right, is there in your context, right? So, you understand what I am trying to say, right? You want to be able to talk to these players in this game, just like in simulacra, they are talking to each other. And that context is maintained. The next time that you talk to that agent, he remembers what you said last time, etc. That conversation can be played, right? [Speaker 2] Yes, but we want that conversation inside the uniform. [Speaker 1] In your name. That's what you should do. And from that, we can make the movies, right? And then the next step is those agents which are keeping track, one of them is the guy who is running the ticks. The others can be people who are connected. They are avatars which are just playing. They don't have to be actually playing because they are avatars of people. Can you summarize what you are doing in Slack? You summarize what you decided to do. You want to create a layer on top where you can talk to these NPCs and the NPCs have to be inside the game which exists. These two things let's see we can do. You can talk to them, maintain their context. So you know what is the context of my chat with this guy, chat with that guy, where we met, which room we met, all that stuff we know. It's just a little layer on top where I can talk to the NPC. And then we use these games to create those sort of narratives. This is my story, your story. You can have",
    "created_at": "2026-01-25T08:25:33.994975+00:00"
  },
  {
    "chunk_id": 62,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 12,
    "text": "with that guy, where we met, which room we met, all that stuff we know. It's just a little layer on top where I can talk to the NPC. And then we use these games to create those sort of narratives. This is my story, your story. You can have a story from his point of view also, Saboteur's point of view. So all that can be done later. Once you have this basic framework. OK. Can you repeat? [Speaker 1] What we will do is, we will create a layer of the Enfon7 game. All the other players, apart from our player, of the user, they will be as NPCs, they will be taking their own decision in the cherry-picking. But, the conversation that they were having, we will show in the states of them in our Python code. So now, when I encounter you, right, as an NPC, you are the NPC, I will have a conversation with you. That conversation will happen in the Python. That context is stored in your database, right? Okay. That is stage one. Next, we can have, they can also talk to each other. All that can happen, right? Because I think what is happening is up to you. So, you are effectively, your own symbol cry is happening, right? is a copy of your simulgra. It's just that the descriptions of what is happening in terms of the environment and what is this area or some castle or this, all that is coming from the game. So it's a little bit more interesting. The simulgra was one fixed area, right? Here any game, it can be, right? So it is arbitrary. So all the users will be as an NPC first and then moving and some of the rules will be hard coded in the genico. All their movements. All movements and actions in the game are hard coded. But what they are talking and what they are interacting with each other. Okay, maybe we can have some actions that are on the Python end that are not happening on Jericho,",
    "created_at": "2026-01-25T08:25:33.994976+00:00"
  },
  {
    "chunk_id": 63,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 13,
    "text": "hard coded in the genico. All their movements. All movements and actions in the game are hard coded. But what they are talking and what they are interacting with each other. Okay, maybe we can have some actions that are on the Python end that are not happening on Jericho, but they are happening on the Python end. Like I slapped another thing. Exactly. It may not be happening on the Python end. Jericho may be used to just track the movements. Movements and any items which are lying around, etc. Because if the item is there, it comes back, the item is still there, somebody picked it up, all that stuff can be inside Jericho. So some action, user will not, the over end user will not know what is happening inside. He can observe, right? In the end, there will be no loser. There will only be an enemy. So ultimately from this, I want to create a, any particular, particular strand of thing happening, room plus time. Time plus locations. I'll create that narrative from this, whatever history is there or what is going on and from there it will appear on you. So we can create many stories of what is happening. That is part two. Once we can create things happening, interaction happening in the world, then we can look at snapshots of it and then back the story, that story, that story. So what I will try to do is that I will look upon that God using and what I can, let's say my This is room 1, this is room 2 and this is the agent here and this is the NPC here. What they are looking at, I will apply loop command on both of them and what I say is that when we put the print statement here, So, I will print all of the observations together from the genic room and the observation will be like agent observe this at this room at this time and NPC observe this. Both of this maybe will be feed into an LLM",
    "created_at": "2026-01-25T08:25:33.994978+00:00"
  },
  {
    "chunk_id": 64,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 14,
    "text": "that when we put the print statement here, So, I will print all of the observations together from the genic room and the observation will be like agent observe this at this room at this time and NPC observe this. Both of this maybe will be feed into an LLM and LLM will be also be fed along with these observations their personalities or whatever. Interaction. Let us see, okay. Good, anything else? No, I think that's as much and let me see. This was Gemini. Okay, now 12, it's going to be 11.30 on Monday. Clear? Okay with you, right? All right.",
    "created_at": "2026-01-25T08:25:33.994980+00:00"
  }
]