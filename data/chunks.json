[
{
  "chunk_id": 1,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 2]\nI am speaking in English also in between, I am speaking in Hindi also in between. So, it was being messy.\n\n[Speaker 1]\nThat's okay, but you don't have to read it.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nLLM has to read it.\n\n[Speaker 2]\nLLM has to read it.\n\n[Speaker 1]\nWill you do it yourself?\n\n[Speaker 2]\nSo, sir, like I am speaking in Hindi in between. I am speaking in English from here and speaking in Hindi in between. So, the Hindi one that has come in between.\n\nSo, that should be in English only or should it be in Hindi separately?\n\n[Speaker 1]\nThey say that you do multimodal transcription. Yes. And then the chat board that you will make, it will read it and give the full response in English.\n\n[Speaker 2]\nSo, it will understand the Hindi one.\n\n[Speaker 1]\nLet's see.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nDo that only. Now see, you are speaking in Hindi, is it recording? Yes.\n\nLet's see what happens.\n\n[Speaker 2]\nOkay. Right. Also, sir, there was one more thing.\n\nLike I am saying something 100%, so 100% it was not coming right.\n\n[Speaker 1]\nSee, there is transcription. Okay. There is language detection.\n\nYes, language detection. There is translation.\n\n[Speaker 2]\nYes.\n\n[Speaker 1]\nWe will put all of them on one. Right.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nThis is the meeting. More meetings will come. We will do all of that.\n\nSo, now what have you understood is the project. Not this. You are asking what is the problem.\n\nOkay. What is the definition of the problem? Yes.\n\n[Speaker 2]\nSo, sir, what I have understood in total is like I have to create a LLM or a chatbot that is like representing you or any instructor or as a personal base that we can keep in a meeting or that we can keep in a meeting. It can take inputs from the meeting, from the transcripts that we provide to him or from your past data and how you speak and also I am collecting your email replies and all just to report that data. And then if you are not available, like if you are available at that moment and some person asks, so you can reply.\n\nBut if you are not available at that moment, so whatever that the chatbot has understood from all the past data and all. So, he could reply best by accessing the internet to the student who is like asking doubt or asking his grades. How did you get the internet?\n\nI didn't get the grades. What do you mean by internet? I mean, sir, like I said, whatever happened, he can access the internet and tell something better.\n\n[Speaker 1]\nYes, that is okay. So, one is it should respond. Suppose you say I am confused, should I be doing this in the project or not doing this in the project?\n\nMy assumption is correct or wrong. Then on the basis of the meeting recordings and my past responses, you should give the best possible answer. Number one.\n\nYeah. Second, if we have a doubt, can I am doing, I am going to use this pre-trained model, let us say or something. I am going to use this internet resource.\n\nIf you were to ask me, can you suggest something better? I may think what I know. But then LMA also knows what is on the internet.\n\nSo, it may also find on the internet.\n\n[Speaker 2]\nSo, it can do that. Yeah."
},
{
  "chunk_id": 2,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nSo, the first that is good. So, what you are going to build is going to be number one, a chat, an agent based with have maybe multiple agents. So, one is going to try to find out what the guy is asking.\n\nOne is going to try to look at the meeting recordings and see what is going to be evaluated the response. Multiple mini agents will be there.\n\n[Speaker 2]\nAnd then we will be collaborating. Collaborating to give the answer.\n\n[Speaker 1]\nThat is one thing you want to do. Second, that is, but and that is going to have as a resource. The transcriptions of various meetings.\n\nThe transcriptions may have multilingual content. So, you have to deal with the multilingual content. That is the one aspect.\n\nLet us first keep two years English content.\n\n[Speaker 2]\nJust English.\n\n[Speaker 1]\nPart one. Right.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nPart one, let us see English content. The Hindi part doesn't give its okay. Right.\n\nOr only Hindi content. Whatever. We will just see.\n\nOkay. Maybe there is one file which is Hindi. One file is English.\n\nWe can break it up. We will do something. Right.\n\nWe will do something to make it work. Right. So, there is somebody.\n\nYou ask a question from this meeting. What happened in this meeting? What were the goals?\n\nWhat were the. That is what the read.ai audit does.\n\n[Speaker 2]\nSomething like that. It will answer. Right.\n\n[Speaker 1]\nRather than give me a summary. Read.ai what does it do? It gives me a summary.\n\nI have to read it. Right. Summarize this meeting and all that.\n\nRight. But I have to read the damn summary.\n\n[Speaker 2]\nSo, can we do something that it speaks?\n\n\n[Speaker 1]\nThat is a different thing. Right. Problem with these meeting summarizations is it takes up my time to read the damn meeting.\n\nI have to read the meeting summary. It takes time. But I have a doubt.\n\nAnd I asked for a doubt. Right. So, it will tell me.\n\nOkay. So, first goal. For month of January.\n\nOkay. Right. It has to be finished.\n\nIs. Let us just call it a simple chatbot. Web interface.\n\n[Speaker 2]\nRight. Okay.\n\n[Speaker 1]\nNot slack back another thing. Web interface. Meetings.\n\nJo bhi ho rahi hai. Uspe if anybody asks a question. Right.\n\nBased on this meeting. It should give a proper concise answer.\n\n[Speaker 2]\nOkay. Right.\n\n[Speaker 1]\nLet us get that. Right. Okay.\n\nSecond. Immediate. Once that is done.\n\nSo, this is basically. There is a UI piece. There will be the core chat interface.\n\nWe should keep it clean. So, that it can be called from anywhere. Right.\n\nAnd there is the repository of meetings, transcripts, etc. Properly organized. So, that everyone can read.\n\nRight. Second. Very small piece.\n\nPerhaps in half of February. Connected to slack.\n\n[Speaker 2]\nOkay. That is the thing. Connected to slack.\n\n[Speaker 1]\nYeah. So, the slackbot. So, that on slack.\n\nWhen we are putting. I will put you on slack also. Okay.\n\nYour chatbot. First will be web based. Second will be.\n\nChat. Second will be slack. So, it will be on slack.\n\nYou are connected to the same channel that we are talking.\n\n[Speaker 2]\nOkay.  "
},
{
  "chunk_id": 3,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nSo, why. It will decide. So, there the other thing comes.\n\nOnce you connect it. It is not. If you have to.\n\nYou could have. That bot could respond to a DM. Like the chat message.\n\nChat. Chat. DM.\n\nDM.\n\n[Speaker 2]\nEach DM different.\n\n[Speaker 1]\nNo, no. Suppose I DM the bot. Okay.\n\nThen I am asking a question. That is one. Second is I am having a conversation.\n\nThe chatbot decides to come in.\n\n[Speaker 2]\nOkay. By himself. Okay.\n\nIt is listening to conversation.\n\n[Speaker 1]\nSo, there is a different type of agent there. Deciding. Is there something where I need to talk?\n\n[Speaker 2]\nYeah. I could help. Or I can elevate the conversation.\n\nRight.\n\n[Speaker 1]\nOkay. Alright. Okay.\n\nAt this point. Okay. Right.\n\nAnd March. Half of March is holiday. But anyway.\n\nYou have to work in holiday. March is. If I have.\n\nI am doing some work. You are doing. Agrim is doing.\n\nSomebody else is doing. And it has a code base.\n\n[Speaker 2]\nOkay."
},
{
  "chunk_id": 4,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nRight. If this bot has access to this code base. Can it see?\n\nIs it doing? Is it doing the right work or not right? Based on.\n\n[Speaker 2]\nEvaluating the code base.\n\n[Speaker 1]\nBased on the meeting. Is it doing. What I am supposed to do.\n\nOr doing something else. Oh.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nOkay.\n\n[Speaker 2]\nIt's like. It's evaluating the code. Is that.\n\n[Speaker 1]\nFor example. I don't ask questions. About your code.\n\nExcept in the five. Ten minutes. I can look at the code.\n\nRight. Right. It can look at the code.\n\nContinuously. Okay. Let us say.\n\nWe just give it. Git access. It will periodically.\n\nDo a git pull. Right. And it will periodically.\n\nLook. And if it wants to ask a question. About the code.\n\nIt will ask questions.\n\n[Speaker 2]\nIt will ask questions.\n\n[Speaker 1]\nWhat are you doing here? What is that? What is that?\n\nSo you feel like. There is a reviewer of the code. Somebody who is trying to.\n\nUnderstand the code. And whatever. It gets the answers.\n\nIt keeps putting that back.\n\n[Speaker 2]\nOkay. He is basically evaluating. If it is correct or not.\n\n[Speaker 1]\nIt is correct. And it is clarifying.\n\n[Speaker 2]\nAnd clarifying. Right. Okay.\n\n[Speaker 1]\nSo you feel. That there is somebody. Over there.\n\nOkay. So. So.\n\nSo. So. So.\n\nSo. So. So.\n\nSo. So. So.\n\nSo. No need. Especially if.\n\n[Speaker 2]\nSo. I..\n\n[Speaker 1]\nI will give you the transcript of the first meeting, transcribe the meeting, run the chat on that, run it on the command line, make the interview, this agent, that agent, who will actually answer questions on that chat. Read more in UI, UI is wasting time actually. Because once this part works, the UI will come on the web, it will come on the site, it will come separately.\n\nIt should answer the questions properly."
},
{
  "chunk_id": 5,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 2]\nSo sir, right now just in English.\n\n[Speaker 1]\nWhatever is in it, first type it in English. Right. Let's see what happens.\n\nLet's see. Whatever is there. You can go to troublescribe, you can give three videos, three recordings of 30 minutes per day.\n\nDo it. I will keep giving you meetings, as meetings happen. Two are done today.\n\nThree are done. I will give you three today. This is it.\n\nThere are two more. Start with those three first.\n\n[Speaker 2]\nRight.\n\n[Speaker 1]\nAnd it will be done by next week. Now what is your schedule? Meeting schedule.\n\n[Speaker 2]\nSo what?\n\n[Speaker 1]\nWhat is your class schedule? When are you free?\n\n[Speaker 2]\nSir, right. On Monday, there is a DBMS. Till when will you be free on Monday? On Monday, from 11 to 12.30, there is a DBMS. And then there is a KELA. 1, 2, 3. Then free.\n\n[Speaker 1]\nSo Monday or Thursday? Hello. Hello.\n\nHello. Yes. Yes.\n\nNo, we don't want anything.\n\n[Speaker 2]\nThank you. Sir, from 11 to 12.30, there is a DBMS. Then DBMS 2, 2, 3. Then free.\n\n[Speaker 1]\nAnd till when is there a DBMS on Monday? There is no class on Friday. Till when is there a DBMS on Monday?\n\n[Speaker 2]\nDBMS on Monday. Sir, from 11 to 12.30. So can we meet after 12.30?\n\n[Speaker 1]\nYes. When is your next class after that?\n\n[Speaker 2]\nSir, it is on 3.00. Till 3.00, right? From 3.00 to 4.30, there is a DBMS. You can come at 12.30. Yes.\n\n[Speaker 1]\nSo from 12.30 to 12.35, where is the class? Class is on C1.0. So it will take 5 minutes to come. You will come by 12.35, right?\n\nSo we will meet at 12.30 on Monday.\n\n[Speaker 2]\nMeet. Okay.\n\n[Speaker 1]\nI will send the invite also. Okay.\n\n[Speaker 2]\nEvery Monday. Sir, when will you send the meeting?\n\n[Speaker 1]\nI am making a classroom.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nI will make a classroom for this project. Actually, I have made a classroom for the other project. I will put you in that.\n\nIt is called Multimodal Meeting Agent.\n\n[Speaker 2]\nOkay."
},
{
  "chunk_id": 6,
  "user_id": "ashmit",
  "meeting_name": "ashmit_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nRight. I will put you in that classroom. Okay.\n\nI will give you access to that in Slack. I will make a separate Slack for your project. Okay.\n\nI will make a new channel for your Slack, your project in Slack. But you will have access to both. But I will put you in one classroom.\n\nOkay. Because I am uploading a lot there. Why should I upload separately?\n\nI will put you in one place. Okay. And not only meetings, there are other videos, recordings, etc.\n\nAll these things are there. You will have to take out the audio from the meeting recording.\n\n[Speaker 2]\nYes.\n\n[Speaker 1]\nThen it will be a video.\n\n[Speaker 2]\nYou will have to take out the audio of the video. Yes, I will do that.\n\n[Speaker 1]\nYou will have to take that out. Yes, I will do that. And you will have to index it.\n\nRight. Let's get one agent which will work on these three meetings. These three meetings.\n\nRight. You will have to say which meeting are we talking about and who am I? If I am Agreem, we will talk about the Agreem meeting.\n\nIf it is you, you will talk about this meeting. If somebody is a third person, you will talk about that meeting. You will have to ask questions about that meeting.\n\nOkay. And that meeting will increase. You have three projects now.\n\n[Speaker 2]\nOkay. So you will have to ask about each meeting separately.\n\n[Speaker 1]\nYes. Okay. When you ask, which project are you in?\n\nNot which meeting.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nWho are you? Say if you are Ashmit, then…\n\n[Speaker 2]\nIf you are Ashmit, then we will have access to that meeting.\n\n[Speaker 1]\nSir, not just the last one. There will always be a meeting next week. There will be a meeting every week.\n\nThere will be many meetings for the same person, same student. It will be based on login. Okay.\n\nWho is logging in? Who is which?\n\n[Speaker 2]\nOkay. Right.\n\n[Speaker 1]\nWhich is your…\n\n[Speaker 2]\nLogin will be based on that. Okay.\n\n[Speaker 1]\nOkay? Yes. See?\n\nYes. Done? Yes, sir.\n\nNext week, 12.30. Next week. All right. Thank you so much.\n\nAll right. Come on."
},
{
  "chunk_id": 1,
  "user_id": "agrim",
  "meeting_name": "agrim_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nWhat do we actually, what have you understood?\n\n[Speaker 2]\nBasically I understood that in a meeting, how there are agents that transcribe everything and what is being said. Something similar to that, but like for the entire, everything that is being shown in the meeting, like what is being presented, who the people are.\n\n[Speaker 1]\nYeah. So, let me show you what, one of the recordings which I downloaded. So, let me share my screen.\n\nSo, I'm showing you, I downloaded this recording from, you were in AI, right? I was in AI course. Yeah, of course you did, right.\n\nSo, we did all this stuff on the iCAP. You did any recording on iCAP?\n\n[Speaker 2]\nYeah.\n\n[Speaker 1]\nSo, I'll just, I downloaded one of them, right? One minute. So, let's say this one, right?\n\nSo, this is a recording. Oops, why is it not showing?\n\n[Speaker 2]\nYou don't need your laptop.\n\n[Speaker 1]\nOne sec, one sec. Let's play. Okay.\n\nSo, you can see this, right? So, this is a recording of one of your classmates. What happened here?\n\nYeah. So, this is, he's recording this and, you know, he's going through the whole thing, right? So, you can, so at some point he's opening this.\n\nAt this point, the LLM has no clue what's going on, right? Later on, he also shows a demo of some kind, right? He does all this.\n\nAnd this is not from the course. This is from the iCAP, some demo that he gave, right? Like you gave a demo, right?\n\nIt's one of those, right? This is not for the course. So, he gives at some point a demo.\n\nSo, you can see he's giving a demo. Sometimes opening the course, sometimes giving a demo. It's all recorded on iCAP.\n\nI downloaded that for you. Now the question is this part is not, the LLM has no clue about this in iCAP, number one. Number two, the meeting recording which is being captured does not have any idea what's happening on the screen in general, right?\n\nIn iCAP, it has the PDF. It has whatever course the student has uploaded or this applicant has uploaded that it has, right? But it doesn't have what he's showing on the screen, okay?\n\nSo, the first thing we can do is take this video. I can give you more. It just takes a bit of time to download them.\n\nI can give you more of them, but they're not too big. They are 50 MB, et cetera, et cetera, videos. We'll find a way to efficiently share the data.\n\nSlack is not the best way. What I can do is I can create a classroom for all of us and we'll just share on the classroom, right? I'll upload these videos there and all that material for this particular classroom.\n\nAnd we'll share everything over there. And then we'll, what you need to do on this is we need to now break it up into frames, right? So, first thing you need to do is take this video.\n\nI'll give you more. You've done CVM, right? You know how to process a video and break the frames.\n\nYou can run a basic change detection.\n\n\n[Speaker 2]\nChange detection.\n\n[Speaker 1]\nChange detection.\n\n[Speaker 2]\nOh, change detection.\n\n[Speaker 1]\nYou just figure out where the basic frames are changing, right? So, there's one part where he's just showing this code, one part where he's showing this, one part. So, the basic different pieces are there.\n\nAnd then, so now you know this piece, this piece, this piece, the three different pieces are there. And now for each of these pieces, first step should be let us find out what he's doing. Oh, okay.\n\nRight? So, he's showing some code, right? So, this, that you can just take some sample frames from this, from that part of that thing and just pass it to Vision LLM, right?\n\nCould be Gemini with Vision. Do you have Gemini key in this? I have Gemini.\n\nYeah. It will, you use a free quota to the extent possible. At any point of time you face a problem, actually there should be no problem because.\n\nIt's 300 dollars. Huh?\n\n[Speaker 2]\nIt's 300 dollars.\n\n[Speaker 1]\nFree, right? Yeah. So, if for some reason your quota limits are, at least your rate limit is going to exceed, et cetera, I'll give you a key.\n\nFor the moment you can just use that because you won't use it much. It's not like you're using it continuously, right? You use it just to extract from this.\n\nWhat is he doing? I mean, this is, are you showing a screen? Are you doing this?\n\nAre you doing that, et cetera, right? Next step we can go into the, say, okay, for a few videos we can actually use the LLM to generate the ground truth. That means what is on the screen exactly, fully.\n\nThere is a scene with this, this, this, this. It will do that. That will cost you more tokens because you're giving more frames.\n\nThat's the next level of change detection. First level is broadly what's happening. And within the demo, every time something significant changes, more detail, you'll again, you'll get more detail.\n\nThis has changed. So, that's the ground truth. Once we get that, right, then it's a matter of making it efficient because you can't be giving every frame with LLM, right?\n\nSo, now once you have ground truth from many videos, right, now we can see what kind of vision model can be used to, we can use a blip or something like Puneet was saying there, right, to extract that text from that, that, that pieces.\n\nAnd now we have a description of what's happening in the video plus texts. And then we have the third problem, which is you have the recording.\n\nAnd recording a transcription is not a problem. I mean, I use free tools for transcribing. They're going to give us their own whatever rocket read or something they have, right?\n\nBut right now, we can just transcribe the actual audio, whatever is happening. From this video, we can transcribe. You can take the audio out of the video, right?\n\nAnd then we can give it to a transcription tool, like the Google scribe or something. You have to run it many times, maybe three a day. You can do one and a half hours a day.\n\nYou can do it for free, right? So, you just transcribe. You have the video transcription of the recording.\n\nYou have what is extracted from the frames. Now, we just have to stitch them together to make a better context. Right?\n\nAnd then the chatbot, which is going to ask questions on this context, will also be able to answer questions on what has happened in the video and also be able to, in the context of ICAP, also be able to ask questions about what he's doing on the screen.\n\nNow, what I've said is a very broad thing. There will be many nuances of issues in that, right?\n\nSo, there will be issues of when to do the next level of. You can't give every frame to the LLM, nor can you. We can do that initially to generate ground truth, right?\n\nBut the mixture of pain detection and text extraction and broad description of what is happening, how to make that more efficient, that will be the core.\n\nFirst part, if we can just get done, let's say, in Jan only, with LLM only, without any vision other than change detection. Right?\n\nNext will be all about making it more explicit. Broad picture is clear?\n\n[Speaker 2]\nYeah?"
},
{
  "chunk_id": 2,
  "user_id": "agrim",
  "meeting_name": "agrim_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nNow, can you repeat what I said?\n\n[Speaker 2]\nYeah, yeah. So, I think the first step is from this video that we did, we need to take the detection and take the broad...\n\n[Speaker 1]\nFirst you take the audio.\n\n[Speaker 2]\nTake the audio.\n\n[Speaker 1]\nRight?\n\n[Speaker 2]\nNot audio, video.\n\n[Speaker 1]\nWhy audio? Transcription is there. And then you can get an idea of what he's doing at what point of time.\n\n[Speaker 2]\nYeah.\n\n[Speaker 1]\nRight? So, there is a change detection and there is the transcript of the audio along with the timestamp. So, you know that this part is different from this part is different from this part.\n\nFor each of these different parts, right, you have the audio or the textual transcript of that part also. And then you can just feed it to the LLC. What is he doing at this point of time?\n\nIs he showing a demo? Not doing a demo? So, now you know he's showing a demo.\n\n[Speaker 2]\nRight.\n\n[Speaker 1]\nNow, for the audience showing code. Now, for showing code, showing demo, next level of analysis is to take. Right.\n\nSo, what do you think you can do by next meeting?\n\n[Speaker 2]\nBy next meeting, I think I can get a sense.\n\n[Speaker 1]\nNo, no. A little bit more detail.\n\n[Speaker 2]\nOkay. So, yeah.\n\n[Speaker 1]\nHow many videos will I get? I'll give you one right now. I'll give you two more, two, three more.\n\nI'll download them and give you a few.\n\n[Speaker 2]\nSo, all of these videos are in different screens. So, I'll do a change detection.\n\n[Speaker 1]\nSo, you know which are the basic parts. Yeah, yeah, yeah. You'll transcribe and say, okay, describe what is happening on each of these parts.\n\nAnd then, and then we fill that with the LLM and add some more context from the screen to that transcript. So, right now you're saying I'm doing this, I'm doing this, I'm doing this. Right.\n\nWhat is on the screen should be added to that transcript.\n\n[Speaker 2]\nOkay, okay. So, I asked the LLM to change this into a text. Text.\n\nOkay.\n\n[Speaker 1]\nWhat is he actually doing on the screen? Okay, okay. So, then that means we insert into the transcript, right?\n\nThose pieces. So, now the transcript is a bigger thing. It's not just what the student is saying or the applicant is saying.\n\nIt's also what he's doing on the screen. You can insert in any which way. So, by next meeting you can't do all this, I'm sure.\n\nWhat can you do? What realistically minimum you can do all this by next meeting?\n\n[Speaker 2]\nI think I can do it. One week is a long time. So, it's the first week.\n\n[Speaker 1]\nFirst week.\n\n[Speaker 2]\nYeah, no classes also.\n\n[Speaker 1]\nSo, let's say let's have a stretch target. See how much you can do by next week. I'll give you a few videos.\n\nYou're doing nothing on the screen right now. So, this video is useless. But this video we'll just record it and take a download anyway because that will be the source of future videos, right?\n\nMeeting videos, right? So, maybe other meetings like I'll be having a meeting in the evening with some other students. Maybe I'll record that also.\n\nBut leave it for this week. This week you just take some of the iCAP videos which I'm going to give you. Okay.\n\nAnd the future next meeting you will also be doing something on the screen. Yeah, yeah. That will also get recorded.\n\nNo, no, but that part will also be recorded. Right. So, the transcript of that will again be shoved into your system.\n\nWe'll recursively use it every time. Whatever you have up to then and see whether it's useful to us. Yes.\n\nAnd the other guy will build a chat bot of the whole thing hopefully. Let's see. Right now you're just trying to enhance that recording or the transcript with more information.\n\nYou want to enhance that transcript with more information basically from the screen. Okay. Right.\n\nWhat's going on? Clear? Okay.\n\nSo, I will now give you videos. I will also give you a recording of this meeting. Right.\n\nWhich you should also translate. Okay. Right.\n\nAnd anyway, keep it for record purposes. What will be done with the recording of that meeting? The other guy will figure out what to do.\n\nWe'll make something. So, as of now, let's keep this time.\n\n[Speaker 2]\nOkay."
},
{
  "chunk_id": 3,
  "user_id": "agrim",
  "meeting_name": "agrim_live_meeting",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nIf there's a reason for this time to be replaced by somebody else, we'll move it here and there.\n\n[Speaker 2]\nYeah, yeah. That's fine.\n\n[Speaker 1]\nBut maybe we'll keep it 11.30. Okay. If you can make it. Or you have a problem getting here.\n\n[Speaker 2]\nNo, no. If it is the first day, I was getting my application. Okay.\n\n[Speaker 1]\nOtherwise, you don't have a class tonight. No. In the morning.\n\nRight. So, 11.30. Because my class ended at 11 after answering questions, etc. By 11.30, I'll be here. Cool? Any other questions you have?\n\n[Speaker 2]\nNo. So, I'll share my updates on Slack. Yes.\n\nEvery day.\n\n[Speaker 1]\nYeah. Whenever you have questions, issues, you share on Slack, etc. And the whole idea of this second guy's project is that he'll build a Slack bot, right, which will take the transcript of this meeting.\n\nAnd your question, at least I'll answer it, but hopefully Slack bot also will answer it. Let's see where that goes. He's having some trouble registering for some reason.\n\nI don't know.\n\n[Speaker 2]\nHe needs some help. Yeah.\n\n[Speaker 1]\nWhy don't you reach out to him? Do you have his email?\n\n[Speaker 2]\nHe's in Slack.\n\n[Speaker 1]\nI'll put him on Slack. Cool? Yeah.\n\nGood. All right. Thanks.\n\nWe're done.\n\n[Speaker 2]\nOkay. That's it for the meeting?\n\n[Speaker 1]\nYeah. The meeting is typically not going to be more than 20 minutes. Unless there's a lot to show.\n\n[Speaker 2]\nOkay.\n\n[Speaker 1]\nAnything else you have to discuss?\n\n[Speaker 2]\nNo. So, when will I have to train a model?\n\n[Speaker 1]\nYou will have to train a model once we decide that. Because we cannot be giving this whole thing. You can't give every frame with NLM, no?\n\nWe may not train a model. We will now. We'll have a discussion.\n\nOkay. Okay. Okay.\n\nWe'll have to do it using a normal computer vision model. Which computer is the machine vision model to use? You also decide.\n\nNobody trains from scratch or what? You always use something.\n\n[Speaker 2]\nYou don't use the hero.\n\n[Speaker 1]\nThat's object detection, not text. Blip. Blip.\n\nBlip. Blip was the text extraction model. Blip will probably do something.\n\nBut search. How do you best extraction of text from these things? We will have to fine-tune some more which model to fine-tune we don't But that's after we generate a lot of ground truth using the LLMs to generate the ground truth for that right Anyway, you'll have the recording and transcript in case there's a doubt you can always refer to the transcript or ask me questions Okay, good You"
},
{
  "chunk_id": 1,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned that the patient's behavior is controlled using a LanGraph based state machine. Please elaborate more on your point.\n\n[Speaker 1]\nWhat LanGraph state machine is doing is like in our backend, what is happening is each session is created. Let me show you in the PPT. Like in our system, each session is created and what LanGraph is doing, it is transitioning the state of that session ID.\n\nLike I am storing some variables like diagnosis, symptoms, and questions asked in your backend, Flask backend. And what LanGraph is doing is it is transitioning, it is updating the memory of each session. Like we can consider our Flask as a bucket and which have some certain boxes and LanGraph as a man that is updating those boxes.\n\nSo now I will be explaining the system architecture. Each frontend session maps to a unique backend session object here. So what happens is like our Flask backend session holds all the patient state, symptom states, and questions asked and nothing is shared globally in it.\n\nSo then what happened is for every message, we invoke a LanGraph based patient agent using graph.invokeState. This ensures deterministic state-driven behavior instead of prompt-based randomness. And because state is a session scope, multiple users can interact simultaneously without any memory leakage or cross-user contamination. And yeah, so the technologies that I use is HTML for CS frontend, CSS and JavaScript.\n\nI kept it simple for lightweight framework. For backend, I use Python and Flask. For agent framework, I use LanGraph and LanChain.\n\nAnd for data, I have used in-memory session store because right now we are not having much of the data. In future scopes, I have added this that I would be adding the backend too. But right now I have done it in in-memory session store only like to reduce the heaviness."
},
{
  "chunk_id": 2,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned that LanGraph ensures deterministic state-driven behavior. Could you elaborate on how backquote graph invoke underscore state backquote achieves this deterministic behavior, contrasting it with prompt-based randomness?\n\n[Speaker 1]\nThe deterministic behavior is achieved by the rules like let me show you in this diseases and treatments.py files. Actually, what's happening is I have certainly putted and constrained. Right now, I have not added much of the diseases.\n\nI have only constrained it to three diseases. The deterministic behavior is because I have added these symptoms and optional symptoms. I have limited them and I have putted them into diseases and treatments.py. So, this is achieving the deterministic behavior. Also, LanGraph is only ensuring the symptoms from these files."
},
{
  "chunk_id": 3,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned adding rich symptoms and opiates optional symptoms to achieve deterministic behavior. Could you elaborate on how these specific additions ensure a more predictable outcome in the simulation?\n\n[Speaker 1]\nSo, what the required symptoms are doing is required symptoms are the symptoms that are specifically needed to diagnose a particular disease. But optional symptoms are the symptoms that increase the confidence bar that I will show you afterwards in the project. Let me explain the key design choices and engineering decisions.\n\nSo, what I am doing is my session is a scoped state, not a global memory. Each user session maintains its own isolated patient state. So, no state is global.\n\nEvery request mutates only its session state. The LanGraph for control flow is not a prompt. The LLM cannot skip steps because transitions are enforced by the graph.\n\nAlso, it is a rubric-based diagnosis evaluation. Diagnosis correctness is checked against revealed evidence and not model confidence. And separation of intent classification and symptom revelation.\n\nUnderstanding a question does not automatically reveal a symptom. Also, design choice 5 is like you can see why the model leans towards a diagnosis. Live system and demo demonstrator is here.\n\nThis is the backend evidence. Here I created a session through a session ID and thread ID. Then I ask particularly symptoms that are revealed gradually.\n\nAlso, here is the diagnosis which is allowed only after a sufficient evidence. LanGraph transition is questioning it to ready for diagnosis. Here is the testing of my model."
},
{
  "chunk_id": 4,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 1]\nThese are the diseases and these are the required symptoms that I was talking about. The fever, cough, headache, nausea, vomiting. And these are the optional symptoms that increase the patient memory.\n\nWhich increase basically the confidence bar. Also, the minimum question needed to diagnose which should be required symptoms basically. These are the special interaction test cases like greeting and your duration ambiguity handling like how many days have it been that I will show.\n\nLet me show you that too like right now. Like here I will ask hi. So patient will be replying by this.\n\nThen I will ask for his symptoms which it says that that's not been an issue. Now nausea is a symptom which is between food poisoning.\n\n[Speaker 2]\nI'm following your presentation well. Please continue your explanation of the required and optional symptoms.\n\n[Speaker 1]\nSo food poisoning is a symptom between food poisoning and migraine and nausea. So what happens is I will elaborate further more to a specific disease to increase diagnostic confidence. Like I will be asking do you have stomach ache which it shows it doesn't have us.\n\nSo we will be constraining the symptoms.\n\n[Speaker 2]\nYou mentioned elaborating further on a specific disease to increase diagnostic confidence. Please continue.\n\n[Speaker 1]\nSo we are seeing that nausea and light sensitivity is going specifically. So what we'll be doing next is asking do you have this. So these three symptoms.\n\n[Speaker 2]\nCould you elaborate on what you mean by going specifically in the context of nausea and light sensitivity?\n\n[Speaker 1]\nMm hmm. So what's happening is like in flask backend. Right now we were having symptoms like an empty array.\n\nAnd then what's happening is the state the land graph like I am asking the symptoms and the symptoms however the symptoms as are getting revealed they are going in the your flask backend. And what's happening there is it is transitioning that state for the particular session ID of this patient which is 998620F0. And what's happening if I create a new session like a new patient.\n\nSo that would be having a different session ID. And the land graph would be connecting to different session ID then. But now right now we are doing this.\n\nSo what I will be doing is diagnosis migraine. Yeah. So diagnosis migraine is good.\n\nRight now I will be showcasing the good test cases. Then I will be also showing the error test cases. Like if the diagnosis I have done is bad or something.\n\nTreatment. So different treatments like for migraine there are three treatments dark room painkillers and rest. And for food poisoning ORS hydrogen diseases are these."
},
{
  "chunk_id": 5,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nI'm following your presentation. Please continue.\n\n[Speaker 1]\nNow once the treatment is completed the session is stopped. So the session ID of this particular patient is done. So what will happen next is like we will be showing the diagnosis migraine.\n\nWe will be going in patient 2. We will ask again from you. Also the each patient would be having random disease.\n\nI have done random in my script.js file. Through that the each patient would be having a different disease. Like a random disease not a different.\n\n[Speaker 2]\nYou mentioned assigning each patient a random disease using back quote script. What's the symptoms of flu?"
},
{
  "chunk_id": 6,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 1]\nIt was fever cough. So now I can diagnose it with flu. Now I will be showcasing what if my diagnosis is wrong.\n\nLike if I diagnose.\n[Speaker 2]\nI'm following your question.\n\n[Speaker 1]\nWith migraine.\n\n[Speaker 2]\nPlease continue. And I'll do my best to help you understand the symptoms.\n\n[Speaker 1]\nLike if I do a diagnosis as something else. So what will happen like it will show that verdict is incorrect. So I will now show.\n\nThis is the error handling that I have done in my project. Like what if the particular doctor gets to diagnosis us patient wrongly. Diagnosis flu.\n\nNow it shows that."
},
{
  "chunk_id": 7,
  "user_id": "ashmit",
  "meeting_name": "ashmit_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned that the system shows what verdict is incorrect. Can you elaborate on how the system determines that a diagnosis is incorrect. And what specific steps are taken in the error handling process.\n\nIt meant of this is. Yeah. So session is you elaborate on what aspects of the treatment you are referring to.\n\n[Speaker 1]\nNow I will be showing what if the treatment is wrong. Like if I.\n\n[Speaker 2]\nEvaluation has been successfully saved. You may now disconnect. We are done here.\n\nLike what if I do now. We are done here. Please return to your task."
},
{
  "chunk_id": 1,
  "user_id": "sarthak",
  "meeting_name": "sarthak_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nI am an evaluator. I'll be evaluating your presentation based on the project and task requirements. Please begin when you're ready.\n\nNote, this session is being recorded.\n\n[Speaker 1]\nHello everyone, this is a project agent in AI. So it is an AI chatbot application that simulates the patient behavior, which interacts with a user who is acting as a doctor. The message sent by the user goes to the backend through FastAPI, which acts like a bridge between the frontend and the backend.\n\nThen the message goes to the land graph, where it is classified. This intent classification helps the land chain to create a response accordingly. We also maintain patient memory so that the bot does not forget the context of the conversation.\n\nThe tasks performed by the land graph, land chain, and patient memory together are known as the agent, which is created for a particular session ID. Tasks are completed with the help of an LLM accessed via API key. The reply generated by the agent goes back to the frontend through FastAPI.\n\nTechnology used. The frontend is deployed on Vercel. The backend is deployed on Render.\n\nThe Identity AI is built with LangChain and LangGraph. The LLM is served via Grok for fast response generation. The backend is coded in Python and the frontend is built using HTML, CSS and JavaScript.\n\nKey design choices. The message which goes to the LangGraph can be classified as an intake message, a treatment message or a closing message. Once a message is classified as closing, a conversation-ended flag becomes true, and any further input receives a session-ended response.\n\nA new session ID can be created using the reset button."
},
{
  "chunk_id": 2,
  "user_id": "sarthak",
  "meeting_name": "sarthak_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned that the message goes to the LangGraph where it is classified. Could you elaborate on how this LangGraph is implemented and what features of the message it uses for intent classification?\n\n[Speaker 1]\nI would like to show you the code for intent classification. We use an LLM here, which is Grok, accessed through the API key. We set the temperature to zero so that we get deterministic output.\n\nIn the prompt, we define rules based on key terms. For example, intake when the user says hi or hello, treatment when treatment is recommended, and closing when the user politely ends the conversation.\n\nThe message is classified and a JSON value is returned by the LLM indicating whether the message is intake, treatment, or closing.\n\n[Speaker 2]\nYou mentioned setting the temperature to zero. Could you elaborate on why you chose this specific value and what effect it has?\n\n[Speaker 1]\nTemperature zero ensures deterministic output. Since intent classification only determines user intent, randomness is undesirable. This helps LangChain generate clearer responses based on intent."
},
{
  "chunk_id": 3,
  "user_id": "sarthak",
  "meeting_name": "sarthak_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nI'm following your presentation. Please continue.\n\n[Speaker 1]\nHere is another demonstration of chatbot functionality. The session ID created on the frontend goes to the backend with the user message. If that user ID already has an agent, the same agent is reused; otherwise, a new agent is created.\n\n[Speaker 2]\nYou mentioned that the same agent is reused. Can you elaborate on how you ensure the correct agent state is maintained?\n\n[Speaker 1]\nWe maintain a dictionary that stores session IDs as keys and agent objects as values. When a request comes, the dictionary is checked. If the session ID exists, the same agent is used, ensuring isolation between sessions."
},
{
  "chunk_id": 4,
  "user_id": "sarthak",
  "meeting_name": "sarthak_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nYou mentioned that the object stored is the agent created for each session ID. Could you elaborate on how this agent object is structured?\n\n[Speaker 1]\nThe agent object is created using a create_agent function, which invokes the LangGraph. The agent is activated and replies are generated through it.\n\n[Speaker 2]\nYou mentioned that the agent is basically a patient which contains a LangGraph. Could you elaborate?\n\n[Speaker 1]\nThe LangGraph maintains the control flow. It classifies intent so that LangChain can generate responses based on intent. The graph structure ensures the flow between intake, treatment, and closing states."
},
{
  "chunk_id": 5,
  "user_id": "sarthak",
  "meeting_name": "sarthak_ppt_evaluation",
  "meeting_type": "ppt_evaluation",
  "text": "[Speaker 2]\nI'm following your presentation. Please continue. So here is a demonstration of our application.\n\n[Speaker 1]\nI can start the conversation by asking about symptoms. The bot responds accordingly. I can ask follow-up questions, and responses are generated based on context.\n\nThe session ID is created after the first message and reused for subsequent messages. I can give treatment intent, accept treatment, and then end the conversation.\n\nOnce the conversation ends, further messages return a consultation-ended response.\n\n[Speaker 2]\nEvaluation has been successfully saved. You may now disconnect."
},

{
  "chunk_id": 1,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_1",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nSo, this is my vision glass. This is the range parameter that how many centimeters or how many units it can do and this is for example, you cannot see 60 degrees.\n\nLike we can only see what is ahead of us. That in radius like that it is there. And this is how much the object is visible.\n\nSo, this is a vision glass basically.\n\n[Speaker 2]\nSo, this is what we observed. Try to keep it simple, keep everything simple, make sure the system is not forgotten."
},
{
  "chunk_id": 2,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_1",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nWe are adding rooms with width and height. A person can only see up to 5 meters. If an object is at 6 meters, it should not be visible.\n\nThis is implemented in demo.py. We created a world object and added walls. If a wall is in the field of view, objects behind it are not visible.\n\nEntities have types and locations. There are multiple agents, and currently we are printing their visible objects."
},
{
  "chunk_id": 3,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_1",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nIn Jericho, each node represents a room. We need to define which objects exist in each room and specify coordinates for vision to work.\n\n[Speaker 2]\nWhen Jericho returns a message, it will list visible objects.\n\nTry to keep the first cut simple. First configure Jericho with your system without extra complexity. Show Jericho working first, then extend it later."
},
{
  "chunk_id": 1,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_2",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\nI've shared a couple of links with you just now. First, there's this paper, Generative Agents, right? You've seen that, right?\n\nSo what they did is they put lots of relevant agents in a word and let them walk around and do things and make things happen, right? Read it carefully, how they've done that. You read it?\n\nThe second one is this one, which you know about text-based games, right? Adventure games and Zork and stuff like that, right? So this put LLM inside those games and said, can you play them?\n\nSo read this paper, right? And this paper has a link to their code, which is not, I've just tried to get it unlocked from the IIIT network, but it's right now some blockages there, so I'm telling you on my phone, right? I said this is not a game site, this is actually a research paper site, right?\n\nSo just because it's called .ai or something, it's, but basically they have various games and they have the code for that, et cetera. This uses an environment called Jericho, right? Which allows basically, if you look at the quick start, you basically can, you know, you have to do all this stuff and then you get the games, right?\n\nAnd then you basically create the environment, right? So you have to get this Z5 environment from whatever this Jericho game suite, right? You have to put this somewhere and you do, right?\n\nAnd you just see if you can use this. Basically, this lets you play the game in Python. So this TextQuest actually uses this environment to do play games, right?\n\nSo what we want to do is first, all these games are single user games, right? So I'll start with that, that single user games are a problem. But first goal is let's get an LLM agent to play a game, right?\n\nThe game environment will have a description of what's happening, et cetera, et cetera, right? And what we need to do is once the LLM agent goes and plays the game and maybe you should make it talks to itself, maybe it talks to somebody else, et cetera, et cetera. So let's say it plays the game, one.\n\nSecond, let's see. There's this. I basically generated, I used the VO3 video generator of Google, right?\n\nSo you basically, you have to create it. So we'll come to the keys issue later, right? So you have first person view of talking to a customer and trying to sell them a washing machine, right?\n\nOr other case, I had various different. So basically, we generated some videos like this, right?\n\n[Speaker 2]\nSo this is a great deal.\n\n[Speaker 1]\nSo it generates a video. It's pretty costly, right? So each of these things will probably end up eight seconds But you have, have you created an account in Germany?\n\n[Speaker 2]\nAre you exhausted?\n\n[Speaker 1]\nAre you exhausted 300 credits? When did you create? Last month.\n\nYou still have some time. You have those 300 credits free, right? For example, I have over here my Germany account.\n\nIf you associate a billing, look at the credits. So when you create an account and make it a paid account, right? You get 26,000 rupees.\n\nIt lasts for three months, but that's okay. Enough for this course, right? So you have two months of your account left, right?\n\nAnd then you can use my account after that, right? So I hope you won't exhaust, but your time will exhaust. But then after that, you can use mine, right?\n\nSo you can be pretty, 26,000 means pretty much like, you know, 2,000 videos, right? Isn't it? No.\n\n200 videos, right? Those four videos are occurring, right? So 200 videos, I mean, each second may cost you 100 rupees to make a video or something, right?\n\nThat's what, for eight seconds, right? So it depends, right? You can see.\n\nBut don't be too free about making videos, okay? Now there's a 1,000 rupee joining fee to pay. You can get, I'll write a note.\n\nYou can get it back from there. And even if it goes above this, whatever it is, you can get back from there. But don't spend too much on the videos, okay?"
},
{
  "chunk_id": 2,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_2",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\n\nSo the idea is this. First goal, use the Jericho environment, study those papers, how they made the LLMs, do various things, etc. Now the LLM is not just doing it blindly, right?\n\nSo some of the ideas of the Simulca paper, you'll have to start thinking how to do, right? You'll keep the memory, what it's doing on, etc., etc., right? And so for the first goal, I think in January, we should get the situation where for one or two or three games, you can play a game.\n\nAnd then for any particular part of that game, we can generate a small video, right? So the game has all of the transcript. I did this, I did this, I saw this, I saw this.\n\nAnd then you generate a video of the player playing that game as if he's going there. So it's like you're creating that clip, right? And then that's step one, right?\n\nHopefully this month we can do that, right? So just one clip for one eight-second period. Then there may be multiple clips.\n\nThis concept of having continuity, having the same look and feel throughout the game, all that can be done, but we won't do that now. So that requires taking the first image and last image and making sure you give those two images and the video will interpolate between them. So it looks like the same environment you're playing through, right?\n\nThat is the part two, right? But after first month, part two ke pehle, what we want to do is we want to, that you can start, once you start understanding this environment, you start understanding how to make a non-playing character in this environment, right? Or something in this environment, which is, can speak or do something like using an agent, right?\n\nRather than the agent only taking actions and just creating a video with hardly any articulation, maybe music or whatever. There is somebody who's speaking back, right? Some animal says something, some, whatever, these are various types, right?\n\nSo how do you bring a non-player character into this environment that you do some R&D on, right? You do that while you're doing this basic work, right? And then the step two will be that now you have non-player characters who are LLM agents, right?\n\nThey are speaking, you are going, you are speaking to them as an agent, right? So you are articulating something to them, which is going into the, some is going into the game. Some is going into your transcript of the game.\n\nMeans what I speak normally doesn't go to the game. Game doesn't care what you speak. Game only cares did you go right, left, did you pick up the sword, did you do this, did you do that, right?\n\nIt doesn't care what you said. But you keep that speech here. What that guy said, once you have made changes to this, so that there's a non-player character who can speak, not just take action.\n\nThen their speech will come through the game and come back to us or come sideways somehow, some channel you have to make, right? And that becomes part of the transcript.\n\nSo next time you have video in part two, you will have not only just the environment, but you will also have other players talking to you in that game.\n\nAnd that is what gets animated. So it becomes like a mini story. So that's the basic thing.\n\nAnd if you can get that far, it would be very nice. Then we can see if there's time we can try to stitch the videos together to make them continual. And then, of course, if we get even that far, I don't think we will in one semester.\n\nBut suppose now you want to put your avatar into the game. That means you will register your face and your personality and say, now you will speak with my personality. That's just prompts, right?\n\nPrompts and that's all bells and whistles. This can be done later. Right.\n\nOK. A core engine has to be built. No UI, no UI."
},


{
  "chunk_id": 3,
  "user_id": "utsav",
  "meeting_name": "utsav_live_meeting_2",
  "meeting_type": "live_meeting",
  "text": "[Speaker 1]\n\nLet's just if I just add the code level, let's build this. Then we can make we can think about making that later. Right.\n\nIf you want to do it in some other time or whatever else you want, somebody else will do it. Is it clear? Tell me now what you expect to do in the project and what you expect to do.\n\n[Speaker 2]\nSo one thing I want to say is that I read the generated engine paper and I tried to build something like that. So from Python, what I will do is, let's say we build a 2D world. We will be adding rooms of cuboid shape or multiple cuboid shapes to make one room.\n\nAnd in those rooms we can add multiple agents that will be interacting with each other and talking to each other. So for that in Python, I coded a message queue, like message bus, that if one agent wants to send a message, it will send to this message bus. It will be thing based, like every agent should take some action or process what message they have received, read the inbox.\n\n[Speaker 1]\nSo you already have the core elements of a multi-agent thing, but you have to stitch it into this. Why? Because these games already have some descriptions of, you know, castles or...\n\nOh, who wrote the descriptions? Already have, because all the games are built in. Let's just use that, because somebody has written it."
}]