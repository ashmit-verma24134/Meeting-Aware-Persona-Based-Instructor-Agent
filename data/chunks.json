[
  {
    "chunk_id": 0,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] What do we actually, what have you understood? [Speaker 2] Basically I understood that in a meeting, how there are agents that transcribe everything and what is being said. Something similar to that, but like for the entire, everything that is being shown in the meeting, like what is being presented, who the people are. [Speaker 1] So, let me show you what, one of the recordings which I downloaded. So, let me share my screen. So, I'm showing you, I downloaded this recording from, you were in AI, right? I was in AI course. Of course you did, right. So, we did all this stuff on the iCAP. You did any recording on iCAP? [Speaker 1] So, I'll just, I downloaded one of them, right? One minute. So, let's say this one, right? So, this is a recording. Oops, why is it not showing? [Speaker 2] You don't need your laptop. [Speaker 1] One sec, one sec. Let's play. Okay. So, you can see this, right? So, this is a recording of one of your classmates. What happened here? So, this is, he's recording this and he's going through the whole thing, right? So, you can, so at some point he's opening this. At this point, the LLM has no clue what's going on, right? Later on, he also shows a demo of some kind, right? He does all this. And this is not from the course. This is from the iCAP, some demo that he gave, right? Like you gave a demo, right? It's one of those, right? This is not for the course. So, he gives at some point a demo. So, you can see he's giving a demo. Sometimes opening the course, sometimes giving a demo. It's all recorded on iCAP. I downloaded that for you. Now the question is this part is not, the LLM has no clue about this in iCAP, number one. Number two, the meeting recording which is being captured does not have any idea what's happening on the screen in general, right? In iCAP, it has the PDF. It",
    "created_at": "2026-02-02T05:41:15.678439+00:00"
  },
  {
    "chunk_id": 1,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "for you. Now the question is this part is not, the LLM has no clue about this in iCAP, number one. Number two, the meeting recording which is being captured does not have any idea what's happening on the screen in general, right? In iCAP, it has the PDF. It has whatever course the student has uploaded or this applicant has uploaded that it has, right? But it doesn't have what he's showing on the screen, okay? So, the first thing we can do is take this video. I can give you more. It just takes a bit of time to download them. I can give you more of them, but they're not too big. They are 50 MB, et cetera, et cetera, videos. We'll find a way to efficiently share the data. Slack is not the best way. What I can do is I can create a classroom for all of us and we'll just share on the classroom, right? I'll upload these videos there and all that material for this particular classroom. And we'll share everything over there. And then we'll, what you need to do on this is we need to now break it up into frames, right? So, first thing you need to do is take this video. I'll give you more. You've done CVM, right? You know how to process a video and break the frames. You can run a basic change detection. [Speaker 2] Change detection. [Speaker 1] Change detection. [Speaker 2] Oh, change detection. [Speaker 1] You just figure out where the basic frames are changing, right? So, there's one part where he's just showing this code, one part where he's showing this, one part. So, the basic different pieces are there. And then, so now you know this piece, this piece, this piece, the three different pieces are there. And now for each of these pieces, first step should be let us find out what he's doing. Oh, okay. Right? So, he's showing some code, right? So, this, that you can just take some sample frames from this, from that part of",
    "created_at": "2026-02-02T05:41:15.678448+00:00"
  },
  {
    "chunk_id": 2,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "piece, the three different pieces are there. And now for each of these pieces, first step should be let us find out what he's doing. Oh, okay. Right? So, he's showing some code, right? So, this, that you can just take some sample frames from this, from that part of that thing and just pass it to Vision LLM, right? Could be Gemini with Vision. Do you have Gemini key in this? I have Gemini. It will, you use a free quota to the extent possible. At any point of time you face a problem, actually there should be no problem because. It's 300 dollars. Huh? [Speaker 2] It's 300 dollars. [Speaker 1] Free, right? So, if for some reason your quota limits are, at least your rate limit is going to exceed, et cetera, I'll give you a key. For the moment you can just use that because you won't use it much. It's not like you're using it continuously, right? You use it just to extract from this. What is he doing? I mean, this is, are you showing a screen? Are you doing this? Are you doing that, et cetera, right? Next step we can go into the, say, okay, for a few videos we can actually use the LLM to generate the ground truth. That means what is on the screen exactly, fully. There is a scene with this, this, this, this. It will do that. That will cost you more tokens because you're giving more frames. That's the next level of change detection. First level is broadly what's happening. And within the demo, every time something significant changes, more detail, you'll again, you'll get more detail. This has changed. So, that's the ground truth. Once we get that, right, then it's a matter of making it efficient because you can't be giving every frame with LLM, right? So, now once you have ground truth from many videos, right, now we can see what kind of vision model can be used to, we can use a blip or something like Puneet was saying there, right, to",
    "created_at": "2026-02-02T05:41:15.678450+00:00"
  },
  {
    "chunk_id": 3,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "it efficient because you can't be giving every frame with LLM, right? So, now once you have ground truth from many videos, right, now we can see what kind of vision model can be used to, we can use a blip or something like Puneet was saying there, right, to extract that text from that, that, that pieces. And now we have a description of what's happening in the video plus texts. And then we have the third problem, which is you have the recording. And recording a transcription is not a problem. I mean, I use free tools for transcribing. They're going to give us their own whatever rocket read or something they have, right? But right now, we can just transcribe the actual audio, whatever is happening. From this video, we can transcribe. You can take the audio out of the video, right? And then we can give it to a transcription tool, like the Google scribe or something. You have to run it many times, maybe three a day. You can do one and a half hours a day. You can do it for free, right? So, you just transcribe. You have the video transcription of the recording. You have what is extracted from the frames. Now, we just have to stitch them together to make a better context. Right? And then the chatbot, which is going to ask questions on this context, will also be able to answer questions on what has happened in the video and also be able to, in the context of ICAP, also be able to ask questions about what he's doing on the screen. Now, what I've said is a very broad thing. There will be many nuances of issues in that, right? So, there will be issues of when to do the next level of. You can't give every frame to the LLM, nor can you. We can do that initially to generate ground truth, right? But the mixture of pain detection and text extraction and broad description of what is happening, how to make that more efficient, that will",
    "created_at": "2026-02-02T05:41:15.678452+00:00"
  },
  {
    "chunk_id": 4,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "do the next level of. You can't give every frame to the LLM, nor can you. We can do that initially to generate ground truth, right? But the mixture of pain detection and text extraction and broad description of what is happening, how to make that more efficient, that will be the core. First part, if we can just get done, let's say, in Jan only, with LLM only, without any vision other than change detection. Right? Next will be all about making it more explicit. Broad picture is clear? [Speaker 2] So, I think the first step is from this video that we did, we need to take the detection and take the broad... [Speaker 1] First you take the audio. [Speaker 2] Take the audio. [Speaker 1] Right? [Speaker 2] Not audio, video. [Speaker 1] Why audio? Transcription is there. And then you can get an idea of what he's doing at what point of time. Right? So, there is a change detection and there is the transcript of the audio along with the timestamp. So, you know that this part is different from this part is different from this part. For each of these different parts, right, you have the audio or the textual transcript of that part also. And then you can just feed it to the LLC. What is he doing at this point of time? Is he showing a demo? Not doing a demo? So, now you know he's showing a demo. [Speaker 2] Right. [Speaker 1] Now, for the audience showing code. Now, for showing code, showing demo, next level of analysis is to take. Right. So, what do you think you can do by next meeting? [Speaker 2] By next meeting, I think I can get a sense. [Speaker 1] No, no. A little bit more detail. [Speaker 2] Okay. So, [Speaker 1] How many videos will I get? I'll give you one right now. I'll give you two more, two, three more. I'll download them and give you a few. [Speaker 2] So, all of these videos are in different screens.",
    "created_at": "2026-02-02T05:41:15.678453+00:00"
  },
  {
    "chunk_id": 5,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "A little bit more detail. [Speaker 2] Okay. So, [Speaker 1] How many videos will I get? I'll give you one right now. I'll give you two more, two, three more. I'll download them and give you a few. [Speaker 2] So, all of these videos are in different screens. So, I'll do a change detection. [Speaker 1] So, you know which are the basic parts. You'll transcribe and say, okay, describe what is happening on each of these parts. And then, and then we fill that with the LLM and add some more context from the screen to that transcript. So, right now you're saying I'm doing this, I'm doing this, I'm doing this. Right. What is on the screen should be added to that transcript. [Speaker 2] Okay, okay. So, I asked the LLM to change this into a text. Text. F Okay. [Speaker 1] What is he actually doing on the screen? Okay, okay. So, then that means we insert into the transcript, right? Those pieces. So, now the transcript is a bigger thing. It's not just what the student is saying or the applicant is saying. It's also what he's doing on the screen. You can insert in any which way. So, by next meeting you can't do all this, I'm sure. What can you do? What realistically minimum you can do all this by next meeting? [Speaker 2] I think I can do it. One week is a long time. So, it's the first week. [Speaker 1] First week. [Speaker 2] No classes also. [Speaker 1] So, let's say let's have a stretch target. See how much you can do by next week. I'll give you a few videos. You're doing nothing on the screen right now. So, this video is useless. But this video we'll just record it and take a download anyway because that will be the source of future videos, right? Meeting videos, right? So, maybe other meetings like I'll be having a meeting in the evening with some other students. Maybe I'll record that also. But leave it for this week.",
    "created_at": "2026-02-02T05:41:15.678454+00:00"
  },
  {
    "chunk_id": 6,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "we'll just record it and take a download anyway because that will be the source of future videos, right? Meeting videos, right? So, maybe other meetings like I'll be having a meeting in the evening with some other students. Maybe I'll record that also. But leave it for this week. This week you just take some of the iCAP videos which I'm going to give you. Okay. And the future next meeting you will also be doing something on the screen. That will also get recorded. No, no, but that part will also be recorded. Right. So, the transcript of that will again be shoved into your system. We'll recursively use it every time. Whatever you have up to then and see whether it's useful to us. Yes. And the other guy will build a chat bot of the whole thing hopefully. Let's see. Right now you're just trying to enhance that recording or the transcript with more information. You want to enhance that transcript with more information basically from the screen. Okay. Right. What's going on? Clear? Okay. So, I will now give you videos. I will also give you a recording of this meeting. Right. Which you should also translate. Okay. Right. And anyway, keep it for record purposes. What will be done with the recording of that meeting? The other guy will figure out what to do. We'll make something. So, as of now, let's keep this time. [Speaker 1] If there's a reason for this time to be replaced by somebody else, we'll move it here and there. [Speaker 2] That's fine. [Speaker 1] But maybe we'll keep it 11.30. Okay. If you can make it. Or you have a problem getting here. [Speaker 2] No, no. If it is the first day, I was getting my application. Okay. [Speaker 1] Otherwise, you don't have a class tonight. No. In the morning. Right. So, 11.30. Because my class ended at 11 after answering questions, etc. By 11.30, I'll be here. Cool? Any other questions you have? [Speaker 2] No. So, I'll share my updates on Slack.",
    "created_at": "2026-02-02T05:41:15.678455+00:00"
  },
  {
    "chunk_id": 7,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "my application. Okay. [Speaker 1] Otherwise, you don't have a class tonight. No. In the morning. Right. So, 11.30. Because my class ended at 11 after answering questions, etc. By 11.30, I'll be here. Cool? Any other questions you have? [Speaker 2] No. So, I'll share my updates on Slack. Yes. Every day. [Speaker 1] Whenever you have questions, issues, you share on Slack, etc. And the whole idea of this second guy's project is that he'll build a Slack bot, right, which will take the transcript of this meeting. And your question, at least I'll answer it, but hopefully Slack bot also will answer it. Let's see where that goes. He's having some trouble registering for some reason. I don't know. [Speaker 2] He needs some help. [Speaker 1] Why don't you reach out to him? Do you have his email? [Speaker 2] He's in Slack. [Speaker 1] I'll put him on Slack. Cool? Good. All right. Thanks. We're done. [Speaker 2] Okay. That's it for the meeting? [Speaker 1] The meeting is typically not going to be more than 20 minutes. Unless there's a lot to show. [Speaker 2] Okay. [Speaker 1] Anything else you have to discuss? [Speaker 2] No. So, when will I have to train a model? [Speaker 1] You will have to train a model once we decide that. Because we cannot be giving this whole thing. You can't give every frame with NLM, no? We may not train a model. We will now. We'll have a discussion. Okay. Okay. Okay. We'll have to do it using a normal computer vision model. Which computer is the machine vision model to use? You also decide. Nobody trains from scratch or what? You always use something. [Speaker 2] You don't use the hero. [Speaker 1] That's object detection, not text. Blip. Blip. Blip. Blip was the text extraction model. Blip will probably do something. But search. How do you best extraction of text from these things? We will have to fine-tune some more which model to fine-tune we don't But that's after we generate a lot",
    "created_at": "2026-02-02T05:41:15.678457+00:00"
  },
  {
    "chunk_id": 8,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "object detection, not text. Blip. Blip. Blip. Blip was the text extraction model. Blip will probably do something. But search. How do you best extraction of text from these things? We will have to fine-tune some more which model to fine-tune we don't But that's after we generate a lot of ground truth using the LLMs to generate the ground truth for that right Anyway, you'll have the recording and transcript in case there's a doubt you can always refer to the transcript or ask me questions Okay, good You",
    "created_at": "2026-02-02T05:41:15.678458+00:00"
  },
  {
    "chunk_id": 9,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 0,
    "text": "[Speaker 1] So, you have to wait, because you are supposed to be at 11.38. No, I had received it was 12. No, I changed it. Anyway, just wait, it is not 11.30. We will meet today, but first let me finish this. So, this stores the information for every thing. What is that on the frame? What is the topic on the frame? You have to join the meeting and share. That is like Google meeting. [Speaker 2] Google meeting. [Speaker 1] There is a Google meeting up today. It is on calendar. Open your calendar. Now you can share the screen. Done. Shut the mic. So, there is evidence for each summary and every thing is sourced. Like the name, the serving, this is the decision item. These are different decisions taken by people. There are different actions taken by people. Then there is a chapter. Chapters are like a general summary and time frame. This was the title summary. People, decisions, action items. Let's say you have a speaker called Rahul. This will try to find the name of the speaker. This is the meeting stage in which you have speaker notes. You have all the speakers. All they are. [Speaker 1] You are telling me the how. You haven't told me what. What is it going to do? What have you done? What is all this about? You are going to the internals of it. [Speaker 2] First it takes out the aspect using Deepgram. And then it takes out. So, you take the audio, take the video. We take key frames from the video using another library called Cpredict. ICpredict. But this doesn't work. There is a fallback also. Let's see how the images are. And then we use Gemini to get the details from the key frames. Screenshots of the video and summary from the audio. And then the audio you are passing to the Gemini through the transcription. And then we make small time frames for movements. They can be slightly changing key frames changing or the person stops talking. There is a",
    "created_at": "2026-02-02T05:41:15.678659+00:00"
  },
  {
    "chunk_id": 10,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 1,
    "text": "the key frames. Screenshots of the video and summary from the audio. And then the audio you are passing to the Gemini through the transcription. And then we make small time frames for movements. They can be slightly changing key frames changing or the person stops talking. There is a break between the speech. We create a small time frame. And then we compare the post accessibility of all these time frames. Whichever time frame was similar, we will open it in the templates. Then we use these chapters and pass them to the editor to create summary. Then we also store different things like the name of the speaker. What else is happening. So, let's see what works so far. This was the output. These are the chapters that got generated. I have stored them into five active objects. This is the summary. The title we got. This is for this video. [Speaker 1] We have the key points. Some of the decisions that were made. And then these are the movement IDs. These are the movements to make up the chapter. We got this movement from 90 seconds to the hundred and third second. And we have the text also. These are the other chapters. This is the frame analysis for all the different frames that have been detected. We get a view like what are they. Is it slides. Is it mixed. Is it camera. What the topic is. The visual summary. What is on the screen. The text. The entities. And then if there is a name of the speaker present on the screen. These are the different key frames. These are the movements. The key frames. The audio. This library actually gives the different speakers. You can see the AI evaluator was there. It was given the name of the speaker 0. And then the student was speaking. It was given a label of 1. For example, let's say the student purchases something, or his name is visible on the screen, this speaker will get replaced by that. [Speaker 1] As regards the goals of",
    "created_at": "2026-02-02T05:41:15.678662+00:00"
  },
  {
    "chunk_id": 11,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 2,
    "text": "given the name of the speaker 0. And then the student was speaking. It was given a label of 1. For example, let's say the student purchases something, or his name is visible on the screen, this speaker will get replaced by that. [Speaker 1] As regards the goals of the project, what progress has been made so far? What are the goals and where have we reached? [Speaker 2] The goals was to create data which includes the demonstration which are done on the screen. There is something happening in terms of the meeting or in the presentation which is based on the conversation and any other context which is pre-provided, which is the PDF file that is already there. So that data is already there. The transcript is there. Everything is there. But what is happening on the screen at a particular point in time is not there. You have the transcript. You have the context. And now you have done something which has created all this structure. Idea was to firstly in batch mode augment that meeting transcript with some information about what the presenter is presenting on the screen in terms of a demonstration. Have you been able to say this is a demo going on versus something else? This is part of the presentation or slide one, slide two, slide three. [Speaker 2] I think it is. You could say for example this is a night demonstration and showcase the time code. [Speaker 1] Where is that demo happening? What key frame is it happening? You have generated a whole bunch of stuff. But is it can we actually get out in this presentation at this key frame from this frame to this frame somebody is giving a demo? [Speaker 2] In these tablets. This is what that evidence object was about. For this one you get this summary. The presentation introduces an AI chatbot in this file. [Speaker 1] Where do you get the references? From what moments? We talked about it will be shown on the screen. But you are not answering my",
    "created_at": "2026-02-02T05:41:15.678664+00:00"
  },
  {
    "chunk_id": 12,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 3,
    "text": "is what that evidence object was about. For this one you get this summary. The presentation introduces an AI chatbot in this file. [Speaker 1] Where do you get the references? From what moments? We talked about it will be shown on the screen. But you are not answering my question. You have done a lot of stuff. But I still do not see that in this entire transcript when is the presenter actually either showing a demo or showing some code? Do we have that information? It should be somewhere in the chapter but it is not given clearly. [Speaker 1] So what we wanted to do was, okay you have done something. It is fine. Interesting. Certainly interesting. But does it solve the problem? It may solve some other problem. It may say I have dumped all this, now I do not know what the hell this is. Some information has come out. But is it telling me that this transcript is there and then from this period to this period there is a demo going on? And number two, within that demo, what is going on is the description of what that person presented in that demo. Exactly what that narration. If you were to describe this demo to me, that this is what the person presented. This is what actually presented. Or this is slide one. A lot of stuff can come from the context. What you produced is actually more data but it's not informative in the sense that we wanted it to be. [Speaker 1] Yes, you could probably use this data. Information that I think is definitely there but I don't think that is happening. So what we wanted were two things. One is in batch mode, that this entire meeting should be enhanced. The transcript of that meeting needs to be enhanced with whatever is on the screen. Could be a presentation slide on the screen and a narration what that slide is. All that in some sense already there because the PDF of that somewhere is there. The file is also",
    "created_at": "2026-02-02T05:41:15.678665+00:00"
  },
  {
    "chunk_id": 13,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 4,
    "text": "enhanced. The transcript of that meeting needs to be enhanced with whatever is on the screen. Could be a presentation slide on the screen and a narration what that slide is. All that in some sense already there because the PDF of that somewhere is there. The file is also there on the screen. [Speaker 1] The system is asking questions only based on what he's speaking, but nothing to do with the screen. But suppose the guy is speaking, but the screen is showing something totally wrong? Or the system has no idea what's on the screen? So a lot of stuff has been generated, but from these two goals perspective, to what extent have we got there? That's what I think we need to think. Think like from this data generated algorithm, we can go. So what I think we need to do now is, we need to take the result of what has happened so far and say this is the input, and this is the desired output. [Speaker 1] And now I think we should stay on Slack. After this meeting, can you summarize on Slack what are the two inputs and outputs that we need? One is batch mode, one is data type. At least then Pankaj can take a look. He's also on Slack. Second thing is, this particular meeting that we are having today, get a transcript of that meeting. Summarize that transcript and post it on Slack. At least we know that we are keeping them up to date in what is happening. [Speaker 2] They just need to get a batch summary. [Speaker 1] Batch summary will be done fast. I think this deliverable will be done very fast. And the next thing will be to, by the way, they will have to supply their tool and then we have to do it with their tool. Meanwhile, before they supply that tool, let's see if we can then, after the batch summary is done, maybe a week after next, we can work on can we make it real-time, which I can",
    "created_at": "2026-02-02T05:41:15.678666+00:00"
  },
  {
    "chunk_id": 14,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 5,
    "text": "have to supply their tool and then we have to do it with their tool. Meanwhile, before they supply that tool, let's see if we can then, after the batch summary is done, maybe a week after next, we can work on can we make it real-time, which I can integrate. Batch summary should be done based on meetings, but now let's generate that batch summary for not just that meeting, but let's generate for the other meetings which are shared. And so now we have results for three, four meetings. Let that be the target for next week. A batch summary process, this is what the person is telling me about on the screen at this particular time, where the demos are, et cetera. So then now that can be the basis for what Akshit can put into his chat box. [Speaker 2] I have a request. So they write out the meetings that are not shared with anyone speaker. If it's possible, could I get some? [Speaker 1] You will. So this will be there. Then our previous meeting is also there. Usman, two speakers, and then the other guy's meeting will also be there. Soon there'll be two speakers, and earlier there was no presentation. Now they'll be starting making presentations. I'd like you to make a GitHub repo for this. Add me also. Don't make it public. Make it private. Add me, and then we'll add Pankaj. And then we'll see. Same events. Action items, write down on Slack. Goals, write down on Slack. Just do a summary of today's meeting and put it on Slack. Today's meeting, when it's recorded and ready, I'll upload it, and then you can get it. [Speaker 1] Ultimately, my goal is, I want this integrated real-time. Their goal, they have to specify after seeing this exactly what they want. The core engine should be generic, but there'll be a real-time version, and there'll be a batch version. Batch version will be based on what you're doing now, but batch version will be more toward what they want. And the",
    "created_at": "2026-02-02T05:41:15.678667+00:00"
  },
  {
    "chunk_id": 15,
    "user_id": "agrim",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 6,
    "text": "have to specify after seeing this exactly what they want. The core engine should be generic, but there'll be a real-time version, and there'll be a batch version. Batch version will be based on what you're doing now, but batch version will be more toward what they want. And the real-time version is what is required by them. How long does it take, the whole pipeline? [Speaker 2] This video is 10 minutes and around two minutes. I think if it's real-time, you can do, like, one frame takes very long. It's very quick, probably one frame. It's like 80 frames or something, it's a video for two minutes. But chain detection, in terms of the keyframe, whether it's a keyframe or not a keyframe, based on what is there. I mean, that architecture will have to be well done. To see is this something new happening, and is it a demo? If it's a demo, I start augmenting. If it's not a demo, I don't want to augment. [Speaker 1] Overall, we're able to think through this. But first, let's get past this phase. From this to a system is a journey. This is easy. Getting into a system which is usable in real-time, or a system which is usable for their purpose, is a journey. Transcription is happening, it's batched. Every 10 seconds, it's being batched. So, it's not a simple architecture. It's a complex architecture. Anyway, those steps have been taken. All right, we'll end this one. I think we'll provide the batch and release on the tomorrow after.",
    "created_at": "2026-02-02T05:41:15.678668+00:00"
  },
  {
    "chunk_id": 35,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 0,
    "text": "[Speaker 2] You mentioned that the patient's behavior is controlled using a LanGraph based state machine. Please elaborate more on your point. [Speaker 1] What LanGraph state machine is doing is like in our backend, what is happening is each session is created. Let me show you in the PPT. Like in our system, each session is created and what LanGraph is doing, it is transitioning the state of that session ID. Like I am storing some variables like diagnosis, symptoms, and questions asked in your backend, Flask backend. And what LanGraph is doing is it is transitioning, it is updating the memory of each session. Like we can consider our Flask as a bucket and which have some certain boxes and LanGraph as a man that is updating those boxes. So now I will be explaining the system architecture. Each frontend session maps to a unique backend session object here. So what happens is like our Flask backend session holds all the patient state, symptom states, and questions asked and nothing is shared globally in it. So then what happened is for every message, we invoke a LanGraph based patient agent using graph.invokeState. This ensures deterministic state-driven behavior instead of prompt-based randomness. And because state is a session scope, multiple users can interact simultaneously without any memory leakage or cross-user contamination. And so the technologies that I use is HTML for CS frontend, CSS and JavaScript. I kept it simple for lightweight framework. For backend, I use Python and Flask. For agent framework, I use LanGraph and LanChain. And for data, I have used in-memory session store because right now we are not having much of the data. In future scopes, I have added this that I would be adding the backend too. But right now I have done it in in-memory session store only like to reduce the heaviness. [Speaker 2] You mentioned that LanGraph ensures deterministic state-driven behavior. Could you elaborate on how backquote graph invoke underscore state backquote achieves this deterministic behavior, contrasting it with prompt-based randomness? [Speaker 1] The deterministic behavior is achieved by the",
    "created_at": "2026-02-02T05:41:15.679391+00:00"
  },
  {
    "chunk_id": 36,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 1,
    "text": "it in in-memory session store only like to reduce the heaviness. [Speaker 2] You mentioned that LanGraph ensures deterministic state-driven behavior. Could you elaborate on how backquote graph invoke underscore state backquote achieves this deterministic behavior, contrasting it with prompt-based randomness? [Speaker 1] The deterministic behavior is achieved by the rules like let me show you in this diseases and treatments.py files. Actually, what's happening is I have certainly putted and constrained. Right now, I have not added much of the diseases. I have only constrained it to three diseases. The deterministic behavior is because I have added these symptoms and optional symptoms. I have limited them and I have putted them into diseases and treatments.py. So, this is achieving the deterministic behavior. Also, LanGraph is only ensuring the symptoms from these files. [Speaker 2] You mentioned adding rich symptoms and opiates optional symptoms to achieve deterministic behavior. Could you elaborate on how these specific additions ensure a more predictable outcome in the simulation? [Speaker 1] So, what the required symptoms are doing is required symptoms are the symptoms that are specifically needed to diagnose a particular disease. But optional symptoms are the symptoms that increase the confidence bar that I will show you afterwards in the project. Let me explain the key design choices and engineering decisions. So, what I am doing is my session is a scoped state, not a global memory. Each user session maintains its own isolated patient state. So, no state is global. Every request mutates only its session state. The LanGraph for control flow is not a prompt. The LLM cannot skip steps because transitions are enforced by the graph. Also, it is a rubric-based diagnosis evaluation. Diagnosis correctness is checked against revealed evidence and not model confidence. And separation of intent classification and symptom revelation. Understanding a question does not automatically reveal a symptom. Also, design choice 5 is like you can see why the model leans towards a diagnosis. Live system and demo demonstrator is here. This is the backend evidence. Here I created a session through a session ID and thread",
    "created_at": "2026-02-02T05:41:15.679393+00:00"
  },
  {
    "chunk_id": 37,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 2,
    "text": "symptom revelation. Understanding a question does not automatically reveal a symptom. Also, design choice 5 is like you can see why the model leans towards a diagnosis. Live system and demo demonstrator is here. This is the backend evidence. Here I created a session through a session ID and thread ID. Then I ask particularly symptoms that are revealed gradually. Also, here is the diagnosis which is allowed only after a sufficient evidence. LanGraph transition is questioning it to ready for diagnosis. Here is the testing of my model. [Speaker 1] These are the diseases and these are the required symptoms that I was talking about. The fever, cough, headache, nausea, vomiting. And these are the optional symptoms that increase the patient memory which increase basically the confidence bar. Also, the minimum question needed to diagnose which should be required symptoms basically. These are the special interaction test cases like greeting and your duration ambiguity handling like how many days have it been that I will show. Let me show you that too like right now. Like here I will ask hi. So patient will be replying by this. Then I will ask for his symptoms which it says that that's not been an issue. Now nausea is a symptom which is between food poisoning. [Speaker 2] I'm following your presentation well. Please continue your explanation of the required and optional symptoms. [Speaker 1] So food poisoning is a symptom between food poisoning and migraine and nausea. So what happens is I will elaborate further more to a specific disease to increase diagnostic confidence. Like I will be asking do you have stomach ache which it shows it doesn't have us. So we will be constraining the symptoms. [Speaker 2] You mentioned elaborating further on a specific disease to increase diagnostic confidence. Please continue. [Speaker 1] So we are seeing that nausea and light sensitivity is going specifically. So what we'll be doing next is asking do you have this. So these three symptoms. [Speaker 2] Could you elaborate on what you mean by going specifically in the context of",
    "created_at": "2026-02-02T05:41:15.679394+00:00"
  },
  {
    "chunk_id": 38,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 3,
    "text": "diagnostic confidence. Please continue. [Speaker 1] So we are seeing that nausea and light sensitivity is going specifically. So what we'll be doing next is asking do you have this. So these three symptoms. [Speaker 2] Could you elaborate on what you mean by going specifically in the context of nausea and light sensitivity? [Speaker 1] Mm hmm. So what's happening is like in flask backend. Right now we were having symptoms like an empty array. And then what's happening is the state the land graph like I am asking the symptoms and the symptoms however the symptoms as are getting revealed they are going in the your flask backend. And what's happening there is it is transitioning that state for the particular session ID of this patient which is 998620F0. And what's happening if I create a new session like a new patient. So that would be having a different session ID. And the land graph would be connecting to different session ID then. But now right now we are doing this. So what I will be doing is diagnosis migraine. So diagnosis migraine is good. Right now I will be showcasing the good test cases. Then I will be also showing the error test cases. Like if the diagnosis I have done is bad or something. Treatment. So different treatments like for migraine there are three treatments dark room painkillers and rest. And for food poisoning ORS hydrogen diseases are these. [Speaker 2] I'm following your presentation. Please continue. [Speaker 1] Now once the treatment is completed the session is stopped. So the session ID of this particular patient is done. So what will happen next is like we will be showing the diagnosis migraine. We will be going in patient 2. We will ask again from you. Also the each patient would be having random disease. I have done random in my script.js file. Through that the each patient would be having a different disease. Like a random disease not a different. [Speaker 2] You mentioned assigning each patient a random disease using back quote script. What's",
    "created_at": "2026-02-02T05:41:15.679395+00:00"
  },
  {
    "chunk_id": 39,
    "user_id": "ashmit",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 4,
    "text": "Also the each patient would be having random disease. I have done random in my script.js file. Through that the each patient would be having a different disease. Like a random disease not a different. [Speaker 2] You mentioned assigning each patient a random disease using back quote script. What's the symptoms of flu? [Speaker 1] It was fever cough. So now I can diagnose it with flu. Now I will be showcasing what if my diagnosis is wrong. Like if I diagnose. [Speaker 2] I'm following your question. [Speaker 1] With migraine. [Speaker 2] Please continue. And I'll do my best to help you understand the symptoms. [Speaker 1] Like if I do a diagnosis as something else. So what will happen like it will show that verdict is incorrect. So I will now show. This is the error handling that I have done in my project. Like what if the particular doctor gets to diagnosis us patient wrongly. Diagnosis flu. Now it shows that. [Speaker 2] You mentioned that the system shows what verdict is incorrect. Can you elaborate on how the system determines that a diagnosis is incorrect and what specific steps are taken in the error handling process. It meant of this is. So session is you elaborate on what aspects of the treatment you are referring to. [Speaker 1] Now I will be showing what if the treatment is wrong. Like if I. [Speaker 2] Evaluation has been successfully saved. You may now disconnect. We are done here. Like what if I do now. We are done here. Please return to your task.",
    "created_at": "2026-02-02T05:41:15.679396+00:00"
  },
  {
    "chunk_id": 16,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 0,
    "text": "[Speaker 2]: I am speaking in English also in between, I am speaking in Hindi also in between. So, it was being messy. [Speaker 1]: That's okay, but you don't have to read it. [Speaker 2]: Okay. [Speaker 1]: LLM has to read it. [Speaker 2]: LLM has to read it. [Speaker 1]: Will you do it yourself? [Speaker 2]: So, sir, like I am speaking in Hindi in between. I am speaking in English from here and speaking in Hindi in between. So, the Hindi one that has come in between. So, that should be in English only or should it be in Hindi separately? [Speaker 1]: They say that you do multimodal transcription. Yes. And then the chat board that you will make, it will read it and give the full response in English. [Speaker 2]: So, it will understand the Hindi one. [Speaker 1]: Let's see. [Speaker 2]: Okay. [Speaker 1]: Do that only. Now see, you are speaking in Hindi, is it recording? Yes. Let's see what happens. [Speaker 2]: Okay. Right. Also, sir, there was one more thing. Like I am saying something 100%, so 100% it was not coming right. [Speaker 1]: See, there is transcription. Okay. There is language detection. Yes, language detection. There is translation. [Speaker 2]: Yes. [Speaker 1]: We will put all of them on one. Right. [Speaker 2]: Okay. [Speaker 1]: This is the meeting. More meetings will come. We will do all of that. So, now what have you understood is the project. Not this. You are asking what is the problem. Okay. What is the definition of the problem? Yes. [Speaker 2]: So, sir, what I have understood in total is like I have to create a LLM or a chatbot that is like representing you or any instructor or as a personal base that we can keep in a meeting or that we can keep in a meeting. It can take inputs from the meeting, from the transcripts that we provide to him or from your past data and how you speak and also I am",
    "created_at": "2026-02-02T05:41:15.678985+00:00"
  },
  {
    "chunk_id": 17,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 1,
    "text": "any instructor or as a personal base that we can keep in a meeting or that we can keep in a meeting. It can take inputs from the meeting, from the transcripts that we provide to him or from your past data and how you speak and also I am collecting your email replies and all just to report that data. And then if you are not available, like if you are available at that moment and some person asks, so you can reply. But if you are not available at that moment, so whatever that the chatbot has understood from all the past data and all. So, he could reply best by accessing the internet to the student who is like asking doubt or asking his grades. How did you get the internet? I didn't get the grades. What do you mean by internet? I mean, sir, like I said, whatever happened, he can access the internet and tell something better. [Speaker 1]: Yes, that is okay. So, one is it should respond. Suppose you say I am confused, should I be doing this in the project or not doing this in the project? My assumption is correct or wrong. Then on the basis of the meeting recordings and my past responses, you should give the best possible answer. Number one. Second, if we have a doubt, can I am doing, I am going to use this pre-trained model, let us say or something. I am going to use this internet resource. If you were to ask me, can you suggest something better? I may think what I know. But then LMA also knows what is on the internet. So, it may also find on the internet. [Speaker 2]: So, it can do that. [Speaker 1]: So, the first that is good. So, what you are going to build is going to be number one, a chat, an agent based with have maybe multiple agents. So, one is going to try to find out what the guy is asking. One is going to try to look at the meeting",
    "created_at": "2026-02-02T05:41:15.678989+00:00"
  },
  {
    "chunk_id": 18,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 2,
    "text": "that is good. So, what you are going to build is going to be number one, a chat, an agent based with have maybe multiple agents. So, one is going to try to find out what the guy is asking. One is going to try to look at the meeting recordings and see what is going to be evaluated the response. Multiple mini agents will be there. [Speaker 2]: And then we will be collaborating. Collaborating to give the answer. [Speaker 1]: That is one thing you want to do. Second, that is, but and that is going to have as a resource. The transcriptions of various meetings. The transcriptions may have multilingual content. So, you have to deal with the multilingual content. That is the one aspect. Let us first keep two years English content. [Speaker 2]: Just English. [Speaker 1]: Part one. Right. [Speaker 2]: Okay. [Speaker 1]: Part one, let us see English content. The Hindi part doesn't give its okay. Right. Or only Hindi content. Whatever. We will just see. Okay. Maybe there is one file which is Hindi. One file is English. We can break it up. We will do something. Right. We will do something to make it work. Right. So, there is somebody. You ask a question from this meeting. What happened in this meeting? What were the goals? What were the. That is what the read.ai audit does. [Speaker 2]: Something like that. It will answer. Right. [Speaker 1]: Rather than give me a summary. Read.ai what does it do? It gives me a summary. I have to read it. Right. Summarize this meeting and all that. Right. But I have to read the damn summary. [Speaker 2]: So, can we do something that it speaks? [Speaker 1]: That is a different thing. Right. Problem with these meeting summarizations is it takes up my time to read the damn meeting. I have to read the meeting summary. It takes time. But I have a doubt. And I asked for a doubt. Right. So, it will tell me. Okay. So, first goal. For",
    "created_at": "2026-02-02T05:41:15.678990+00:00"
  },
  {
    "chunk_id": 19,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 3,
    "text": "thing. Right. Problem with these meeting summarizations is it takes up my time to read the damn meeting. I have to read the meeting summary. It takes time. But I have a doubt. And I asked for a doubt. Right. So, it will tell me. Okay. So, first goal. For month of January. Okay. Right. It has to be finished. Is. Let us just call it a simple chatbot. Web interface. [Speaker 2]: Right. Okay. [Speaker 1]: Not slack back another thing. Web interface. Meetings. Jo bhi ho rahi hai. Uspe if anybody asks a question. Right. Based on this meeting. It should give a proper concise answer. [Speaker 2]: Okay. Right. [Speaker 1]: Let us get that. Right. Okay. Second. Immediate. Once that is done. So, this is basically. There is a UI piece. There will be the core chat interface. We should keep it clean. So, that it can be called from anywhere. Right. And there is the repository of meetings, transcripts, etc. Properly organized. So, that everyone can read. Right. Second. Very small piece. Perhaps in half of February. Connected to slack. [Speaker 2]: Okay. That is the thing. Connected to slack. [Speaker 1]: So, the slackbot. So, that on slack. When we are putting. I will put you on slack also. Okay. Your chatbot. First will be web based. Second will be. Chat. Second will be slack. So, it will be on slack. You are connected to the same channel that we are talking. [Speaker 2]: Okay. [Speaker 1]: So, why. It will decide. So, there the other thing comes. Once you connect it. It is not. If you have to. You could have. That bot could respond to a DM. Like the chat message. Chat. Chat. DM. DM. [Speaker 2]: Each DM different. [Speaker 1]: No, no. Suppose I DM the bot. Okay. Then I am asking a question. That is one. Second is I am having a conversation. The chatbot decides to come in. [Speaker 2]: Okay. By himself. Okay. It is listening to conversation. [Speaker 1]: So, there is a different type of",
    "created_at": "2026-02-02T05:41:15.678992+00:00"
  },
  {
    "chunk_id": 20,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 4,
    "text": "No, no. Suppose I DM the bot. Okay. Then I am asking a question. That is one. Second is I am having a conversation. The chatbot decides to come in. [Speaker 2]: Okay. By himself. Okay. It is listening to conversation. [Speaker 1]: So, there is a different type of agent there. Deciding. Is there something where I need to talk? [Speaker 2]: I could help. Or I can elevate the conversation. Right. [Speaker 1]: Okay. Alright. Okay. At this point. Okay. Right. And March. Half of March is holiday. But anyway. You have to work in holiday. March is. If I have. I am doing some work. You are doing. Agrim is doing. Somebody else is doing. And it has a code base. [Speaker 2]: Okay. [Speaker 1]: Right. If this bot has access to this code base. Can it see? Is it doing? Is it doing the right work or not right? Based on. [Speaker 2]: Evaluating the code base. [Speaker 1]: Based on the meeting. Is it doing. What I am supposed to do. Or doing something else. Oh. [Speaker 2]: Okay. [Speaker 1]: Okay. [Speaker 2]: It's like. It's evaluating the code. Is that. [Speaker 1]: For example. I don't ask questions. About your code. Except in the five. Ten minutes. I can look at the code. Right. Right. It can look at the code. Continuously. Okay. Let us say. We just give it. Git access. It will periodically. Do a git pull. Right. And it will periodically. Look. And if it wants to ask a question. About the code. It will ask questions. [Speaker 2]: It will ask questions. [Speaker 1]: What are you doing here? What is that? What is that? So you feel like. There is a reviewer of the code. Somebody who is trying to. Understand the code. And whatever. It gets the answers. It keeps putting that back. [Speaker 2]: Okay. He is basically evaluating. If it is correct or not. [Speaker 1]: It is correct. And it is clarifying. [Speaker 2]: And clarifying. Right. Okay. [Speaker 1]: So you feel.",
    "created_at": "2026-02-02T05:41:15.678993+00:00"
  },
  {
    "chunk_id": 21,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 5,
    "text": "is trying to. Understand the code. And whatever. It gets the answers. It keeps putting that back. [Speaker 2]: Okay. He is basically evaluating. If it is correct or not. [Speaker 1]: It is correct. And it is clarifying. [Speaker 2]: And clarifying. Right. Okay. [Speaker 1]: So you feel. That there is somebody. Over there. Okay. So. So. So. So. So. So. So. So. So. No need. Especially if. [Speaker 2]: So. I. [Speaker 1]: I will give you the transcript of the first meeting, transcribe the meeting, run the chat on that, run it on the command line, make the interview, this agent, that agent, who will actually answer questions on that chat. Read more in UI, UI is wasting time actually. Because once this part works, the UI will come on the web, it will come on the site, it will come separately. It should answer the questions properly. [Speaker 2]: So sir, right now just in English. [Speaker 1]: Whatever is in it, first type it in English. Right. Let's see what happens. Let's see. Whatever is there. You can go to troublescribe, you can give three videos, three recordings of 30 minutes per day. Do it. I will keep giving you meetings, as meetings happen. Two are done today. Three are done. I will give you three today. This is it. There are two more. Start with those three first. [Speaker 2]: Right. [Speaker 1]: And it will be done by next week. Now what is your schedule? Meeting schedule. [Speaker 2]: So what? [Speaker 1]: What is your class schedule? When are you free? [Speaker 2]: Sir, right. On Monday, there is a DBMS. Till when will you be free on Monday? On Monday, from 11 to 12.30, there is a DBMS. And then there is a KELA. 1, 2, 3. Then free. [Speaker 1]: So Monday or Thursday? Hello. Hello. Hello. Yes. Yes. No, we don't want anything. [Speaker 2]: Thank you. Sir, from 11 to 12.30, there is a DBMS. Then DBMS 2, 2, 3. Then free. [Speaker 1]: And till when",
    "created_at": "2026-02-02T05:41:15.678994+00:00"
  },
  {
    "chunk_id": 22,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 6,
    "text": "there is a KELA. 1, 2, 3. Then free. [Speaker 1]: So Monday or Thursday? Hello. Hello. Hello. Yes. Yes. No, we don't want anything. [Speaker 2]: Thank you. Sir, from 11 to 12.30, there is a DBMS. Then DBMS 2, 2, 3. Then free. [Speaker 1]: And till when is there a DBMS on Monday? There is no class on Friday. Till when is there a DBMS on Monday? [Speaker 2]: DBMS on Monday. Sir, from 11 to 12.30. So can we meet after 12.30? [Speaker 1]: Yes. When is your next class after that? [Speaker 2]: Sir, it is on 3.00. Till 3.00, right? From 3.00 to 4.30, there is a DBMS. You can come at 12.30. Yes. [Speaker 1]: So from 12.30 to 12.35, where is the class? Class is on C1.0. So it will take 5 minutes to come. You will come by 12.35, right? So we will meet at 12.30 on Monday. [Speaker 2]: Meet. Okay. [Speaker 1]: I will send the invite also. Okay. [Speaker 2]: Every Monday. Sir, when will you send the meeting? [Speaker 1]: I am making a classroom. [Speaker 2]: Okay. [Speaker 1]: I will make a classroom for this project. Actually, I have made a classroom for the other project. I will put you in that. It is called Multimodal Meeting Agent. [Speaker 2]: Okay. [Speaker 1]: Right. I will put you in that classroom. Okay. I will give you access to that in Slack. I will make a separate Slack for your project. Okay. I will make a new channel for your Slack, your project in Slack. But you will have access to both. But I will put you in one classroom. Okay. Because I am uploading a lot there. Why should I upload separately? I will put you in one place. Okay. And not only meetings, there are other videos, recordings, etc. All these things are there. You will have to take out the audio from the meeting recording. [Speaker 2]: Yes. [Speaker 1]: Then it will be a video. [Speaker 2]: You will have to take",
    "created_at": "2026-02-02T05:41:15.678995+00:00"
  },
  {
    "chunk_id": 23,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 7,
    "text": "you in one place. Okay. And not only meetings, there are other videos, recordings, etc. All these things are there. You will have to take out the audio from the meeting recording. [Speaker 2]: Yes. [Speaker 1]: Then it will be a video. [Speaker 2]: You will have to take out the audio of the video. Yes, I will do that. [Speaker 1]: You will have to take that out. Yes, I will do that. And you will have to index it. Right. Let's get one agent which will work on these three meetings. These three meetings. Right. You will have to say which meeting are we talking about and who am I? If I am Agreem, we will talk about the Agreem meeting. If it is you, you will talk about this meeting. If somebody is a third person, you will talk about that meeting. You will have to ask questions about that meeting. Okay. And that meeting will increase. You have three projects now. [Speaker 2]: Okay. So you will have to ask about each meeting separately. [Speaker 1]: Yes. Okay. When you ask, which project are you in? Not which meeting. [Speaker 2]: Okay. [Speaker 1]: Who are you? Say if you are Ashmit, then\u2026 [Speaker 2]: If you are Ashmit, then we will have access to that meeting. [Speaker 1]: Sir, not just the last one. There will always be a meeting next week. There will be a meeting every week. There will be many meetings for the same person, same student. It will be based on login. Okay. Who is logging in? Who is which? [Speaker 2]: Okay. Right. [Speaker 1]: Which is your\u2026 [Speaker 2]: Login will be based on that. Okay. [Speaker 1]: Okay? Yes. See? Yes. Done? Yes, sir. Next week, 12.30. Next week. All right. Thank you so much. All right. Come on.",
    "created_at": "2026-02-02T05:41:15.678996+00:00"
  },
  {
    "chunk_id": 24,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "meeting_qa",
    "chunk_index": 8,
    "text": "Next week. All right. Thank you so much. All right. Come on.",
    "created_at": "2026-02-02T05:41:15.678997+00:00"
  },
  {
    "chunk_id": 25,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] Sit down, show me what's going on. Share it with us. So sir, I have started with generating the No, I have started with the manual chunking of the transcripts. I have to get all the transcripts from the video and then what I have done is I have read those transcripts and what I see the pattern was like I made the chunks accordingly like when you What was the goal? Goal was what? So goal was this that I will divide the transcripts in chunks and then what I will do, I will like to embed them So I will take those transcripts and embed them in a file like embedding I will do the embedding of the transcripts to chunks and then I will store them in a FIOS data structure. What FIOS data structure? You will do the So sir, it is like finding the similarities of the query and the transcript chunks or the cosine intervals Ok, fine. But that was for what purpose? Sir, to find the specific, like I have asked some question so I have embedded that question and then in the FIOS data structure, that question and the transcript chunk of that specific part will be matched and then I will give that transcript chunk to the LLM and he will explain accordingly This was what? This you have done? Ok, let's see how it goes Now there is another way of doing it You give the entire transcript to the LLM give the question and he will give some answer Sir, actually what was happening though I had given entire transcript to the LLM, it was translucent again It was like, I asked some question it was giving me some other answer It was happening so that's why I divided it into chunks So you have a baseline? This did not work? And this did work? And for how many meetings? Sir, only one How many meetings have I shared with you? Sir, you have shared me 6 meetings So whatever meetings I have shared you should have",
    "created_at": "2026-02-02T05:41:15.679218+00:00"
  },
  {
    "chunk_id": 26,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "I divided it into chunks So you have a baseline? This did not work? And this did work? And for how many meetings? Sir, only one How many meetings have I shared with you? Sir, you have shared me 6 meetings So whatever meetings I have shared you should have the baseline which is, I should give the entire transcript to the LLM and the asking question and give these problems you should document that Ok, then I will give him chunks Then you give chunks and you document because otherwise it's not scientific You have to say, I did this I tried it on 6 meetings, this happened then I tried this, this happened We need to be systematic in the process Ok, now show me what you have done this is for in the future, do it systematically so everything you try you record it, store it so that we can later on even that is something you are meeting transcript you should be able to say, you did this, this happened all that is results right now you are giving only transcript later you will also give results you will also give code you will also give everything 80 around Oxford whatever you are going to do for others that is the beginning your life will become easier so sir, first I have manually chunked them like when the topic was changing I read all the scripts so I chunked them accordingly then I have stored them in JSON file like every chunk in future I am thinking to automate this manually or automatically sir, but I was thinking when I automated this it was not like suddenly doing well because what was happening the topic changed and it was overlapping some topics but question is how much does that affect the final result even with those errors may be final result if I will give whole transcript so it can first tell me what have you done so what I have done is I have took a meeting so what I have done is I have",
    "created_at": "2026-02-02T05:41:15.679222+00:00"
  },
  {
    "chunk_id": 27,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "much does that affect the final result even with those errors may be final result if I will give whole transcript so it can first tell me what have you done so what I have done is I have took a meeting so what I have done is I have took a meeting so what I have done is I have given user id to the specific chunk and meeting name and meeting time and I have given text in the specific chunk so I have done this for all the chunks then what I have done is generate ratings is like sir, I have called that bg model which was embedding the files this is chinese model yes sir use a better model they do but this bg model is not very good I have used it anyway, its ok this is part of demo you should find a model which has good paper on it and all that chinese model is used for demo so everybody uses it actually bg is not the best everybody uses it I know but its not the best model just because they have a demo people use it so sir what I have done is I have specifically created chunk embeddings so what is happening is like it was happening that meeting was crossing with each other so what I have done is I have created those meeting name and those were crossing each other so it was not happening afterwards then I have stored this is like storing embeddings what sir? sir what was happening like Agrim and Ashmit are two different persons so what was happening is like it was firstly hallucinating at Agrim's meeting and if I ask one question, it was giving his answer and if I ask so I have created those meeting id so these were the same embeddings then in that it was the chunk embeddings that happened this was just like cleaning for the cleaning purpose for the overlapping purpose like sir what I have done is in a specific chunk it is taking",
    "created_at": "2026-02-02T05:41:15.679224+00:00"
  },
  {
    "chunk_id": 28,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "so I have created those meeting id so these were the same embeddings then in that it was the chunk embeddings that happened this was just like cleaning for the cleaning purpose for the overlapping purpose like sir what I have done is in a specific chunk it is taking the last 30 words and it is like overlapping that in the next chunk so that if something is ending with specific so it can follow so you have overlapping things overlapping things so chunk embeddings on that so sir now I have two things like this was the first one thing, now what happened here is like it was creating the data structure, it was taking chunks it was taking chunks of the probably this chunk embeddings and it was taking the vectors now what happened here is I have removed all vectors of all specific chunks but the file data structure is only showing numbers so in future I will use that, right now I am not using that, what I will do in future is I will create different vectors for each user so that it will not overlapping afterwards you have to have some more metadata exactly, afterwards I will be using it but right now I am just storing those so in that now in generate answer now in this what I have done is first show it working [Speaker 2] so [Speaker 1] so so sir what tree chunks is doing is right now I have not used that vector index what it is doing is it is taking your user query and that it is creating in embeddings all user specific like I have taken one user I have taken user id of a user it is embedding that query and then from the stored transcripts that chunk embeddings I am not using vector index it is matching it is going in file data structure and then it is matching that user query embedding and stored chunk embedding then what's happening is ranking so I am ranking the most matched answers so you",
    "created_at": "2026-02-02T05:41:15.679225+00:00"
  },
  {
    "chunk_id": 29,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "the stored transcripts that chunk embeddings I am not using vector index it is matching it is going in file data structure and then it is matching that user query embedding and stored chunk embedding then what's happening is ranking so I am ranking the most matched answers so you are doing a direct right now I am doing direct in the function right now in this video you are retrieving all of them and just storing them right now all the user specific what's happening is like if I have taken the user id of Ashmit so he will only retrieve Ashmit's embeddings and then you are just doing a comparison with all of them and finding the top top 1 or top 3 then you are giving that to the relevant so then supervisor agent supervisor agent is just the pipeline like I am taking that it has a function to find the nearest next you are finding the top 3 like that it does the ranking by itself it does the ranking by itself it does the matching by itself so then supervisor agent is just the pipeline that I just showed it is just taking the input and giving the output so let's take a random transcript I will take like which transcript is it sir this is my live meeting we can take some data like which was just ask some question about your meeting sir we can ask at what date was the next meeting I will connect this to the what is happening here why it is taking so long sir I think wifi wifi is here because of the meeting so sir I have entered the user ID Ashwin now it will be taking the embedding of those what I am doing in the session ID right now I will make memories I have not implemented a lambda but what I will do later is like I have asked some question like what was the date or what something so I will then ask the follow up question like explain me this",
    "created_at": "2026-02-02T05:41:15.679226+00:00"
  },
  {
    "chunk_id": 30,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "in the session ID right now I will make memories I have not implemented a lambda but what I will do later is like I have asked some question like what was the date or what something so I will then ask the follow up question like explain me this question in specific detailed answer so it will take the previous question to the state to the state and then I will create that in next week this is random right now right so we can ask so it gave the answer it took the specific chance and it finds the ID and it gave the answer sir ask that the next meeting will be scheduled for next week what was the main action item for next meeting sir I will one again right now so what was the main action item discussed discussed and finalized we need to build a simple web based meeting agent that answer question for the meeting ok but what was the action item the immediate action item for the next week for the next week [Speaker 2] this is general [Speaker 1] which is correct for the immediate next week sir one more doubt that immediate deliverable immediate deliverable for the following for the following meeting for the following meeting but it is like the menu simple web based meeting to be integrated first option we agree that web based was the first one we agree on web based before next meeting yes sir you said web based and then i said web based is a problem and then i said command line command line why is it web based? because web based was planned who suggested web based meeting? web based was there in the paper in the paper first we put a slash and then in the end command line search you said a slash integration command line command line is written it is there but it is not retrieved in the chunk it is not retrieved in the chunk the immediate thing the point is this embedding retrieves the long chunk",
    "created_at": "2026-02-02T05:41:15.679227+00:00"
  },
  {
    "chunk_id": 31,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "slash and then in the end command line search you said a slash integration command line command line is written it is there but it is not retrieved in the chunk it is not retrieved in the chunk the immediate thing the point is this embedding retrieves the long chunk you have to devote this because this is a very important thing if the long chunk comes the answer will also be different what were you asking? [Speaker 2] sir i was asking can i normalize the transcripts [Speaker 1] like this transcript not this one [Speaker 2] this [Speaker 1] this is his transcript like this it was like hallucinating once can i normalize transcripts is there any model that can normalize transcripts this was some issue from the transcript this was a good one we told that we will put in the bit it can read full and then it can check the code accordingly this is yours someone came in between remember somebody came in there is somebody outside someone was standing outside sir but in video when i was listening to this transcript we were talking about something but that was not coming there was a noise sir i was asking is there any model that can normalize transcripts no you can go in Gemini and say get rid of stuff which appears to be noisy no no no so first i give transcripts to LLM then i store the transcripts so what is the action item now next time first of all i am thinking to automate that second one more thing so sir web based don't do web based it will take time keep it command line but keep it so that everything is properly in your own database and you can you log in to your chats it should tell you that meetings are coming for you both guys will be users each meeting will have all participants you log in all your meetings will be there and then you should be able to ask questions one by one by one and keep",
    "created_at": "2026-02-02T05:41:15.679228+00:00"
  },
  {
    "chunk_id": 32,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "in to your chats it should tell you that meetings are coming for you both guys will be users each meeting will have all participants you log in all your meetings will be there and then you should be able to ask questions one by one by one and keep the context i ask one question same thing chat based but keep the context explain me the previous question then sometimes you have to go back from this context it may have to go and get one more chat right that's what you have to think clean it sometimes it has to get more chat sometimes it will just ask based on the current context it may need to go back and get one more meeting it is deciding what to do whether to go or not that decision is being made by two agents one is going to give the response other guy is going to decide to give more context ok either the previous answers or chunks previous answers and chunks do i need more information or not make it make it a proper system that i have so many things stored in the database i can retrieve based on the user id then keep asking questions and chat basically that thing should be the user the terminal layer should be very simple it should be something which you can take that backend piece and connect it to a slack bot also we will put slack bot in our meeting in our channel so backend we will host it and every time we will push have you made the repo? push it make me a collaborator and we will put a company also and they can also be a group they can also sit here let us get this working in a way for all our meetings yours, other guys whatever is there for these three groups where i am recording the meeting we are able to have on slack a conversation with the previous what is happening let us get a system working first by that time",
    "created_at": "2026-02-02T05:41:15.679229+00:00"
  },
  {
    "chunk_id": 33,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "this working in a way for all our meetings yours, other guys whatever is there for these three groups where i am recording the meeting we are able to have on slack a conversation with the previous what is happening let us get a system working first by that time the company will wake up and they will provide their profit notes whatever they are recording we can use that ok sir the agreement the agreement now he is now able to pass the videos where is the demo what is the code that will also come in the transcript what did you show on the screen last time it will come clear any other question because if you have a question you better ask otherwise you will get lost close it sir so sir right now i was thinking that i have to create a system i have to implement a lambda that it can also answer the follow up questions and also i will be continuing this right now i am it should continue [Speaker 2] and [Speaker 1] it should also decide whether or not to get a new job so basically is he asking a different question which requires me to go back and get a new job or is it on the same context or do i need to go and get for that there will be a separate agent and sir that will be happening for a specific user yes for a specific user so sir in the command line i will first input the user id and then it will be continuing the transcript and other thing is the meeting that you get back you will have to get back for that user for that particular conversation right now it is for user only so we will do that later do it continuously once it is continuous then we will add help and slack that will be the first system and how is it going to be stored yes it is being stored in the file i am extending it along with every",
    "created_at": "2026-02-02T05:41:15.679230+00:00"
  },
  {
    "chunk_id": 34,
    "user_id": "ashmit",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 9,
    "text": "only so we will do that later do it continuously once it is continuous then we will add help and slack that will be the first system and how is it going to be stored yes it is being stored in the file i am extending it along with every in the database there is the user id also there date everything is there meeting time yes meeting time and date will be there it is not just a vector you are also searching based on some other parameters so sir it will be storing that chat in a file for all questions and answers after the chat is off and this is chunk automation chunking automation understood then we will see how to deploy how to extend sure",
    "created_at": "2026-02-02T05:41:15.679231+00:00"
  },
  {
    "chunk_id": 40,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 0,
    "text": "At the very beginning of the presentation, at 00:00:00, the AI Evaluator briefly stated, \"Boarded.\" Concurrently, the screen displayed the title slide for a project. The slide is titled \"Agentic Patient Simulator,\" further described as \"A Multi-User Agentic Medical Interview System.\" It highlights the technologies used in its construction: \"Built with LangGraph, LangChain, Flask, and Vanilla Web.\" The presenter is identified as \"Ashmit Verma(2024134)\" from \"IIT Delhi.\" The slide is marked as \"Page 1 of 10.\" The screen also shows a UI with a \"Transcript\" area, a message input box, and a \"Send\" button. A prominent message within the transcript from an evaluator reads, \"I am an evaluator. I'll be evaluating your presentation based on the project and task requirements. Please begin\u2026\" indicating the start of the evaluation process. A system notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen\" and offers a \"Stop sharing\" option. At 00:00:01, the presentation transitions from a title slide to a live demonstration of the \"Agentic Patient Simulator\" web application. No one spoke during this specific moment. The application, accessible via 'localhost:5500', presents a multi-panel user interface. The left panel, labeled \"Sessions,\" allows for managing patient interactions, showing options like \"+ New Patient\" and a list of existing patients (Patient 1-4). The central panel is dedicated to the patient simulation itself, currently displaying a completed session for \"Patient 4 - c0d40d3.\" The interaction shows a doctor prescribing \"treatmentrest in dark,\" which the patient accepts, followed by a system confirmation of \"Treatment accepted.\" The right-hand side of the screen provides crucial patient data. The \"Patient Memory\" section details symptoms such as \"nausea,\" \"light_sensitivity,\" \"movement_worsens_headache,\" and \"headache.\" Below this, \"Diagnostic Confidence\" displays a probabilistic diagnosis, with \"Migraine - 80%\" being the most likely, followed by \"Flu - 11%\" and \"Food Poisoning - 9%.\" Both the patient interaction and diagnostic confidence sections are marked as \"Session completed.\" A \"Send\" button is present, suggesting further interaction is possible. The bottom of the screen also shows a notification that \"projects.icapp.co.in is sharing your screen,\" along with the current weather and date. At 00:00:34, the presentation transitioned to",
    "created_at": "2026-02-02T05:41:15.681802+00:00"
  },
  {
    "chunk_id": 41,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 1,
    "text": "the patient interaction and diagnostic confidence sections are marked as \"Session completed.\" A \"Send\" button is present, suggesting further interaction is possible. The bottom of the screen also shows a notification that \"projects.icapp.co.in is sharing your screen,\" along with the current weather and date. At 00:00:34, the presentation transitioned to a detailed slide titled \"Agentic Patient Simulator.\" No one spoke during this specific moment. The slide, identified as page \"1 / 10\" of a \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" introduces the project as \"A Multi-User Agentic Medical Interview System.\" It explicitly states that the system was \"Built with LangGraph, LangChain, Flask, and Vanilla Web,\" and credits Ashmit Verma (2024134) from IIIT Delhi as the developer. The slide prominently features a screenshot of the application's user interface. This UI includes a \"Sessions\" panel on the left, allowing for the management of patient interactions with options like \"+ New Patient,\" \"Patient 1,\" and \"Patient 2.\" The central part of the interface displays a chat interaction, showing a patient's acceptance of treatment (\"Patient: Okay Doctor, I'll follow this treatment.\") and a system confirmation (\"System: Treatment accepted. Session completed.\"), along with a \"Send\" button. The right-hand panel provides critical patient information, including a \"Patient Memory\" section listing symptoms such as \"\u2713 nausea,\" \"\u2713 light_sensitivity,\" \"\u2713 movement_ataxia_headache,\" and \"\u2713 headache.\" Below this, a \"Diagnostic Confidence\" section presents potential diagnoses with their probabilities: \"Flu - 17%,\" \"Food Poisoning - 9%,\" and \"Migraine - 80%.\" The slide also indicates that \"System Architecture\" will be discussed, as it appears as a heading at the bottom. The overall screen capture includes elements from the presenter's desktop environment, such as browser tabs, a file path, and the Windows taskbar. At 00:00:36, the presentation displays a detailed \"System Architecture\" slide, which is page \"2 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" No one spoke during this moment. The slide illustrates the \"Agentic Patient Simulator\" system, highlighting \"Key: Memory + Coherence.\" The architecture begins with a \"Frontend\" identified as the \"Doctor Web Interface,\" implemented using HTML, CSS, and Vanilla JavaScript. This interface includes a",
    "created_at": "2026-02-02T05:41:15.681807+00:00"
  },
  {
    "chunk_id": 42,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 2,
    "text": "\"Minimalist Clean Gears Green Project Review Corporate Presentation.\" No one spoke during this moment. The slide illustrates the \"Agentic Patient Simulator\" system, highlighting \"Key: Memory + Coherence.\" The architecture begins with a \"Frontend\" identified as the \"Doctor Web Interface,\" implemented using HTML, CSS, and Vanilla JavaScript. This interface includes a \"Chat Interface,\" \"Session Switcher,\" \"Patient Memory Panel,\" and \"Diagnostic Confidence Visualization.\" User interactions from the frontend are sent as \"HTTP Request (session_id, message)\" to a \"Flask Backend API.\" The backend API manages `/create_session` and `/chat` routes, performing \"Session validation\" and \"Request routing\" to safely validate `session_id` and direct requests. It is also responsible for \"Load / Update state\" and ensuring a \"Deterministic response.\" The system utilizes a \"Session Store\" to map `session_id` to `patient_state`, which encompasses `symptoms_revealed`, `questions_asked`, and the current `stage` (e.g., `ready_for_diagnosis`), representing the \"State/output.\" The core intelligence is provided by the \"LangGraph Patient Agent,\" which functions as a state machine. Its nodes include \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis Gate,\" and \"Treatment Phase,\" with state transitions managed by `graph.invoke(state)`. The slide also introduces a \"TreatmentAgent\" that \"Accepts treatment only after correct diagnosis\" and an \"EvaluatorAgent\" responsible for \"Rubric-based diagnosis validation,\" described as \"Deterministic & explainable,\" and \"used only when ready_for_diagnosis = true.\" The \"Architecture Overview\" section on the slide reiterates key design principles: the frontend sends user messages with a session ID, the backend validates and routes requests per session, each session maps to a unique LangGraph thread and dedicated LangGraph execution, patient behavior is driven by a state machine, the UI reflects the agent state in real time, and critically, there is \"No global state. No shared memory.\" All patient behavior is explicitly stated to be deterministic, state-driven, and session-scoped. At 00:00:42, the presentation displays a detailed \"System Architecture\" slide, which is page \"2 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" The slide outlines the Agentic Patient Simulator's components, starting with a \"Doctor Web Interface\" (Frontend) built with HTML, CSS, and Vanilla JavaScript, featuring a Chat Interface, Session Switcher, Patient Memory Panel, and Diagnostic Confidence Visualization. User interactions are sent as \"HTTP",
    "created_at": "2026-02-02T05:41:15.681809+00:00"
  },
  {
    "chunk_id": 43,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 3,
    "text": "the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" The slide outlines the Agentic Patient Simulator's components, starting with a \"Doctor Web Interface\" (Frontend) built with HTML, CSS, and Vanilla JavaScript, featuring a Chat Interface, Session Switcher, Patient Memory Panel, and Diagnostic Confidence Visualization. User interactions are sent as \"HTTP request (session_id, message)\" to a \"Flask Backend API.\" This API manages `/create_session` and `/chat` routes, performing \"Session validation\" and \"Request routing\" to validate `session_id` and ensure request state safety. It handles \"Load / Update state\" using a \"Session Store\" that maps `session_id` to `patient_state`, encompassing `symptoms_revealed`, `questions_asked`, `stage`, and `ready_for_diagnosis`. The system design emphasizes \"No global state. No shared memory.\" The core logic is driven by a \"LangGraph Patient Agent,\" functioning as a state machine with nodes for \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis Gate,\" and \"Treatment Phase,\" invoked via `graph.invoke(state)`. The slide highlights that \"Each frontend session maps to a unique LangGraph thread\" and \"All patient behavior is deterministic, state-driven, and session scoped.\" Supporting agents include a \"TreatmentAgent\" that accepts treatment only after a correct diagnosis, and an \"EvaluatorAgent\" for rubric-based diagnosis validation, which is \"Deterministic & explainable\" and \"Triggered only when ready_for_diagnosis == true.\" An \"Architecture Overview\" section summarizes the flow: Frontend sends user messages with a `session_id`, Backend validates and routes requests per session, each session maps to a dedicated LangGraph execution, patient behavior is driven by a state machine, and the UI reflects agent state in real time. From 00:00:44 to 00:00:49, the AI Evaluator noted that the patient's behavior is controlled using a LangGraph-based state machine and then, from 00:00:51 to 00:00:53, asked Ashmit Verma to elaborate. Ashmit Verma responded from 00:00:54 to 00:00:58, explaining that the LangGraph state machine is active in the backend, where each session is created, and then from 00:01:01 to 00:01:05, he offered to show this in the current or upcoming presentation slide. At 00:01:05, the presentation displays a slide titled \"Architecture Overview\" and \"Technologies & Tooling,\" which is page \"2 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma, speaking from 00:01:01 to 00:01:05, states,",
    "created_at": "2026-02-02T05:41:15.681810+00:00"
  },
  {
    "chunk_id": 44,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 4,
    "text": "he offered to show this in the current or upcoming presentation slide. At 00:01:05, the presentation displays a slide titled \"Architecture Overview\" and \"Technologies & Tooling,\" which is page \"2 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma, speaking from 00:01:01 to 00:01:05, states, \"what is happening is each session is created. Let me show you in the PPT.\" This utterance directly relates to the slide's explanation of session management. The architecture overview details how the Frontend sends user messages with a `session_id` to a Flask Backend API, which then validates and routes these requests, mapping each session to a unique LangGraph execution. The system emphasizes that \"No global state. No shared memory. Each frontend session maps to a unique LangGraph thread. All patient behavior is deterministic, state-driven, and session-scoped.\" The \"LangGraph Patient Agent\" is structured with \"State Machine Nodes\" for Question Handling, Symptom Revelation, Diagnosis Gate, and Treatment Phase, orchestrated by a 'graph'. The \"Session State\" is explicitly defined by `session_id`, `patient_memory`, `current_state`, and `current_action`. Additionally, the slide introduces a \"TreatmentAgent\" that only accepts treatment after a correct diagnosis and an \"EvaluatorAgent\" that performs rubric-based, deterministic, and explainable diagnosis validation with real-time evaluation. The \"Technologies & Tooling\" section specifies the use of \"HTML/CSS/Vanilla JavaScript\" for the frontend, ensuring a lightweight, framework-free UI with full control over session switching and real-time updates. At 00:01:06, Ashmit Verma explained that \"in our system, each session is created,\" which aligns with the slide's content. The slide, titled \"Live System Demonstration & Backend Evidence\" and marked as \"5 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" visually demonstrates this concept through chat snippets. The first snippet shows a user initiating a conversation with `\"I have a headache and a fever.\"`. The system, acting as an assistant, then asks for more details: `\"Okay, I understand. Can you tell me more about your headache? Is it throbbing, dull, or sharp? Where is it located? Do you have any other symptoms?\"`. The user responds with `\"It's a throbbing headache on my temples. I also feel nauseous.\"`. The",
    "created_at": "2026-02-02T05:41:15.681811+00:00"
  },
  {
    "chunk_id": 45,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 5,
    "text": "an assistant, then asks for more details: `\"Okay, I understand. Can you tell me more about your headache? Is it throbbing, dull, or sharp? Where is it located? Do you have any other symptoms?\"`. The user responds with `\"It's a throbbing headache on my temples. I also feel nauseous.\"`. The slide explicitly states that \"Each user initializes a unique backend session and LangGraph thread,\" reinforcing Ashmit Verma's point about session creation. It also highlights that \"Symptoms are revealed gradually and gated by medical relevance.\" A key process shown is the \"LangGraph transition: Questioning -> Diagnosis Gate (ready_for_diagnosis = true),\" indicating how the system progresses through its diagnostic states. The presence of \"projects.icapp.co.in is sharing your screen.\" at the bottom suggests a live demonstration context. At 00:01:08, Ashmit Verma elaborates on the system's session management, stating, \"Like, in our system, each session is created\" (00:01:06\u201300:01:10). He further explains that \"what Landgraf is doing, it is transitioning the state of that session ID\" (00:01:10\u201300:01:15), and mentions that he is actively \"storing\" this state (00:01:16\u201300:01:19). This explanation is visually supported by the current slide, titled \"Live System Demonstration & Backend Evidence,\" which is page \"5 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" The slide presents a detailed conversational exchange with an `agentic_ai.py` in a terminal interface. The AI initiates the interaction by asking \"What are you feeling?\", to which the user responds, \"I have a cough and a sore throat.\" The AI then systematically gathers more information through questions like \"How long have you been feeling this way?\" (user: \"About three days.\"), \"Do you have any other symptoms?\" (user: \"No, just the cough and sore throat.\"), \"Have you had a fever?\" (user: \"No.\"), \"Are you experiencing any body aches or fatigue?\" (user: \"A little tired, but nothing major.\"), and \"Have you been around anyone who is sick recently?\" (user: \"Yes, my roommate had a cold last week.\"). Based on this information, the AI provides a diagnosis: \"Based on what you've told me, it sounds like you might have a common cold or a mild upper respiratory infection.\" It",
    "created_at": "2026-02-02T05:41:15.681813+00:00"
  },
  {
    "chunk_id": 46,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 6,
    "text": "and \"Have you been around anyone who is sick recently?\" (user: \"Yes, my roommate had a cold last week.\"). Based on this information, the AI provides a diagnosis: \"Based on what you've told me, it sounds like you might have a common cold or a mild upper respiratory infection.\" It then offers medical advice: \"I recommend resting, staying hydrated, and over-the-counter medications for symptom relief. If your symptoms worsen or don't improve in a few days, please consider seeing a doctor.\" The conversation concludes with the user's \"No, that's all. Thank you!\" and the AI's \"You're welcome. Take care!\". The slide explicitly states, \"Each user initializes a unique backend session and LangGraph thread,\" directly correlating with Ashmit Verma's point about session creation. Furthermore, the slide provides backend evidence in the form of JSON tool calls, such as `{\"tool_calls\": [ { \"name\": \"get_symptoms\", \"arguments\": { \"symptoms\": [ \"cough\", \"sore throat\" ] } } ] }` and `{\"tool_calls\": [ { \"name\": \"get_symptoms\", \"arguments\": { \"symptoms\": [ \"fever\", \"body aches\", \"fatigue\" ] } } ] }`. It clarifies that \"Symptoms are revealed gradually and gated by medical relevance.\" The core LangGraph functionality is demonstrated by the \"LangGraph transition: Questioning -> Diagnosis\" which is activated by a \"Gate (ready_for_diagnosis = true),\" illustrating the state-driven progression of the agent based on collected medical information. At 00:01:20, Ashmit Verma utters the word \"some.\" The slide, titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" and focusing on the \"Agentic Patient Simulator,\" is page \"5 / 10\" and presents a \"Live System Demonstration & Backend Evidenc\" (partially visible). The slide highlights several key features of the system: it \"Prevents hallucinated symptoms,\" \"Keeps patient responses medically consistent,\" \"Allows transparent reasoning,\" \"Makes evaluation explainable,\" and \"Ensures fairness and consistency.\" The visual content includes two command-line interface (CLI) windows showcasing a live system demonstration. The first CLI window illustrates the creation of a new session via a `curl -X POST http://127.0.0.1:5000/create_session` command, which successfully returns a unique `session_id` (\"7e515220-47ca-43c9-a71c-df13b83cb273\") and `thread_id` (\"1ef9a2e2-9a42-4207-9231-31b37239b491\"). Following this, a series of `curl -X POST http://127.00.1:5000/chat` commands demonstrate a patient simulation. The initial message, \"How",
    "created_at": "2026-02-02T05:41:15.681814+00:00"
  },
  {
    "chunk_id": 47,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 7,
    "text": "a live system demonstration. The first CLI window illustrates the creation of a new session via a `curl -X POST http://127.0.0.1:5000/create_session` command, which successfully returns a unique `session_id` (\"7e515220-47ca-43c9-a71c-df13b83cb273\") and `thread_id` (\"1ef9a2e2-9a42-4207-9231-31b37239b491\"). Following this, a series of `curl -X POST http://127.00.1:5000/chat` commands demonstrate a patient simulation. The initial message, \"How are you feeling?\", receives the reply \"I've been feeling unusually tired lately,\" with the debug information showing `questions_asked: 1`, `ready_for_diagnosis: false`, `stage: 3`, and `symptoms_revealed: [\"fatigue\"]`. The conversation continues with the question \"Any cough or breathing issues?\", to which the simulated patient responds, \"I hadn't thought about it earlier, but I do have a cough.\" This updates the debug information to `questions_asked: 2`, `symptoms_revealed: [\"fatigue\", \"cough\"]`, and `next_action: \"continue_questioning\"`. A subsequent question, \"Have you been coughing lately?\", elicits the reply \"I've had a persistent cough for a few days,\" leading to `questions_asked: 3`, `ready_for_diagnosis: true`, `symptoms_revealed: [\"fatigue\", \"cough\", \"persistent cough\"]`, and `next_action: \"allow_diagnosis\"`. Explanatory text on the slide clarifies that \"Each user initializes a unique backend session and LangGraph thread\" and that \"Symptoms are revealed gradually and gated by medical relevance.\" The screen also shows system details like \"Microsoft Windows [Version 10.0.20108.7962]\" and a browser bar indicating \"projects.icapp.co.in is sharing your screen.\" At 00:01:21, the slide, titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" and focusing on \"Live System Demonstration & Backend Evidenc,\" is displayed. Ashmit Verma explains from 00:01:22 to 00:01:33 that variables such as 'diagnosis,' 'symptoms,' and 'questions asked' are managed within the Flask backend. He elaborates that LangGraph is crucial for transitioning and updating the memory of each session (00:01:35\u201300:01:41), likening the Flask backend to a 'bucket' containing 'boxes' that LangGraph, acting as a 'man,' updates (00:01:43\u201300:01:54). The slide visually supports this by stating, \"Each user initializes a unique backend session and LangGraph thread\" and \"Symptoms are revealed gradually and gated by medical relevance.\" It provides concrete examples of how the session state evolves: an initial state with a message \"How are you feeling?\" and empty `symptoms_revealed` and `diagnosis` arrays. Upon receiving the message \"I have a cough,\" the `symptoms_revealed` array is updated to `[\"cough\"]`. Subsequently,",
    "created_at": "2026-02-02T05:41:15.681815+00:00"
  },
  {
    "chunk_id": 48,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 8,
    "text": "are revealed gradually and gated by medical relevance.\" It provides concrete examples of how the session state evolves: an initial state with a message \"How are you feeling?\" and empty `symptoms_revealed` and `diagnosis` arrays. Upon receiving the message \"I have a cough,\" the `symptoms_revealed` array is updated to `[\"cough\"]`. Subsequently, with the same message \"I have a cough,\" the `diagnosis` field is updated to \"Common Flu,\" illustrating the state-driven progression. Ashmit Verma concludes by stating that he will now explain the system architecture (00:01:57\u201300:02:00). At 00:01:57, Ashmit Verma announces that he will now explain the system architecture, transitioning the discussion. The current slide, titled \"Live System Demonstration & Backend Evidence\" and marked as page 4/10 of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" visually supports this explanation by showcasing a detailed interaction with the system's backend. The slide displays command-line `curl` commands demonstrating how a user initiates a session and interacts with a patient simulator. First, a `curl -X POST http://127.0.0.1:5000/create_session` command is executed, successfully returning a unique `session_id` (\"f515220-9fca-43cb-a1fc-0f412b83c2f9\") and `thread_id` (\"f1694bc2-9a41-4207-7251-31a1259e6a1\"). Following this, a series of `curl -X POST http://127.0.0.1:5000/chat` commands illustrate a conversational flow. In the initial chat, the user asks \"How are you feeling?\", and the system replies \"I've been feeling unusually tired lately.\" The debug output for this interaction shows `stage: 1` and identifies `\"fatigue\"` as a revealed symptom. The conversation progresses as the user's subsequent message, \"I've been feeling unusually tired lately.\", prompts the system to reply, \"I've been feeling unusually tired lately, and I have a cough.\" The debug information for this step indicates `questions_asked: [\"Are you experiencing any other symptoms?\"]`, `stage: 2`, and `symptoms_revealed: [\"fatigue\"]`. A third interaction shows the user stating, \"I've been feeling unusually tired lately, and I have a cough.\", to which the system responds, \"I've had a persistent cough for a few days.\" The debug details for this stage include `questions_asked: [\"How long have you had the cough?\"]`, `stage: 3`, and `symptoms_revealed: [\"fatigue\", \"cough\"]`. This sequence visually demonstrates the system's ability to gradually reveal symptoms and ask medically relevant follow-up questions. The slide explicitly states that",
    "created_at": "2026-02-02T05:41:15.681816+00:00"
  },
  {
    "chunk_id": 49,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 9,
    "text": "a persistent cough for a few days.\" The debug details for this stage include `questions_asked: [\"How long have you had the cough?\"]`, `stage: 3`, and `symptoms_revealed: [\"fatigue\", \"cough\"]`. This sequence visually demonstrates the system's ability to gradually reveal symptoms and ask medically relevant follow-up questions. The slide explicitly states that \"Each user initializes a unique backend session and LangGraph thread\" and that \"Symptoms are revealed gradually and gated by medical relevance,\" reinforcing the system's design for consistent and medically sound patient interactions. At 00:01:57, Ashmit Verma initiated the explanation of the system architecture, transitioning the presentation to a detailed overview. The current slide, titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" and labeled as page 2 of 10, visually supports this explanation with an \"Architecture Overview\" diagram for an \"Agentic Patient Simulator.\" Ashmit explained that each frontend session maps to a unique backend session object (00:02:00\u201300:02:04). He elaborated that the Flask backend session is responsible for holding all of the patient's state, including `symptoms_revealed`, `questions_asked`, and `stage`, and critically, \"No global state. No shared memory\" is maintained (00:02:09\u201300:02:16). The diagram illustrates this flow, showing a \"Doctor Web Interface\" (Frontend) built with HTML/CSS/Vanilla JavaScript, which includes a \"Chat Interface,\" \"Session Switcher,\" \"Patient Memory Panel,\" \"Diagnostic Confidence,\" and \"Visualization.\" This Frontend sends an \"HTTP Request (session_id, message)\" to a \"Flask Backend API.\" The API, with `/create_session` and `/chat` endpoints, handles \"Session validation,\" \"Request routing,\" safely validating `session_id` and routing requests, providing a \"Deterministic response,\" and managing \"Load/Update state.\" This backend then interacts with a \"Session Store\" where `session_id` is mapped to `patient_state`, which includes `symptoms_revealed`, `questions_asked`, `stage`, and `ready_for_diagnosis`. Ashmit further detailed that for every message, a \"LangGraph Patient Agent\" is invoked using `graph.invoke(state)` (00:02:17\u201300:02:25). This LangGraph agent contains \"State Machine Nodes\" for \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis Gate,\" and \"Treatment Phase.\" He emphasized that this design ensures \"deterministic, state-driven behavior instead of prompt based randomness\" (00:02:26\u201300:02:31). The slide reinforces these points, stating, \"Each frontend session maps to a unique LangGraph Patient Agent\" and \"All patient behavior is deterministic, state-driven, and session-scoped.\" Ashmit concluded by highlighting that because the state is",
    "created_at": "2026-02-02T05:41:15.681817+00:00"
  },
  {
    "chunk_id": 50,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 10,
    "text": "Phase.\" He emphasized that this design ensures \"deterministic, state-driven behavior instead of prompt based randomness\" (00:02:26\u201300:02:31). The slide reinforces these points, stating, \"Each frontend session maps to a unique LangGraph Patient Agent\" and \"All patient behavior is deterministic, state-driven, and session-scoped.\" Ashmit concluded by highlighting that because the state is session-scoped, multiple users can interact simultaneously without any memory leakage or cross-user contamination (00:02:31\u201300:02:40). The overall \"Architecture Overview\" summary on the slide states that the \"Frontend sends user messages with a session_id\" and the \"Backend validates and routes requests per session,\" ensuring \"Reply + Memory + Confidence.\" At 00:02:38\u201300:02:40, Ashmit Verma emphasizes the importance of preventing \"cross user contamination.\" This statement directly relates to the system architecture presented on the slide, which is titled \"System Architecture\" and is part of a \"Minimalist Clean Gears Green Project Review Corporate Presentation\" (page 2/10) by Ashmit Verma (2024134) from IIIT Delhi. The slide details the architecture of an \"Agentic Patient Simulator.\" It illustrates a \"Doctor Web Interface\" (Frontend) built with HTML/CSS/Vanilla JavaScript, which includes a \"Chat Interface,\" \"Session Switcher,\" \"Patient Memory Panel,\" and \"Diagnostic Confidence Visualization,\" along with \"Reply + Memory + Confidence\" features. This frontend communicates with a \"Flask Backend API\" via \"HTTP Request (session_id, message).\" The backend, with endpoints like \"/create_session\" and \"/chat,\" is responsible for \"Session validation\" and \"Request routing,\" ensuring that requests are safely routed based on the session ID and that responses are deterministic. It also handles loading and updating the state. A \"Session Store\" is used to map each `session_id` to a `patient_state`, which contains critical information such as `symptoms_revealed`, `questions_asked`, `stage`, and `ready_for_diagnosis`. The core logic is handled by a \"LangGraph Patient Agent,\" which takes \"State input\" and operates through \"State Machine Nodes\" including \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis Gate,\" and \"Treatment Phase,\" invoked via `Graph invoke(state)`. A crucial architectural principle highlighted is that \"Each frontend session maps to a unique LangGraph thread,\" and \"All patient behavior is deterministic, state-driven, and session-scoped.\" This design, as Ashmit Verma's utterance implies, is key to preventing \"cross user contamination\" by ensuring that each patient's state and interaction history are",
    "created_at": "2026-02-02T05:41:15.681818+00:00"
  },
  {
    "chunk_id": 51,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 11,
    "text": "invoke(state)`. A crucial architectural principle highlighted is that \"Each frontend session maps to a unique LangGraph thread,\" and \"All patient behavior is deterministic, state-driven, and session-scoped.\" This design, as Ashmit Verma's utterance implies, is key to preventing \"cross user contamination\" by ensuring that each patient's state and interaction history are isolated within their unique session. The \"Architecture Overview\" section reiterates that the frontend sends user messages with a `session_id` and the backend validates and routes requests per session. An \"Evaluator Agent\" is also part of the system. This aligns with previous discussions where Ashmit explained that each frontend session maps to a unique backend session object and that the Flask backend holds all of the patient's state with \"No global state. No shared memory\" (from 00:01:59). At 00:02:42, Ashmit Verma briefly acknowledges the transition, saying \"And yeah.\" He then begins to specify the technologies used, mentioning \"HTML\" (00:02:45-00:02:48). This aligns with the current slide, which is titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" (slide 2/10) and provides an \"Architecture Overview\" for an \"Agentic Patient Simulator.\" The slide visually details the system's components and flow. The \"Doctor Web Interface\" (Frontend) is explicitly stated to be built with \"HTML/CSS/Vanilla JavaScript\" and includes features like a \"Chat Interface,\" \"Session Switcher,\" \"Patient Memory Panel,\" and \"Diagnostic Confidence Visualization,\" along with \"Reply Memory Confidence.\" This frontend interacts with a \"Flask Backend API\" using \"HTTP Request (session_id, messages).\" The backend manages `create_session` and `chat` functionalities, ensuring \"Session validation\" and \"Request routing\" to validate `session_id` and safely route states, leading to \"Deterministic response\" and handling \"Load/Update state.\" A \"Session Store\" is central, mapping `session_id` to `patient_state` (containing `symptoms_revealed`, `questions_asked`, `stage`, `ready_for_diagnosis`). This state feeds into the \"LangGraph Patient Agent,\" which uses \"State Machine Nodes\" for \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis Gate,\" and \"Treatment Phase,\" invoked by `graph.invoke(state)`. Crucially, the slide emphasizes \"No global state. No shared memory.\" and that \"Each frontend session maps to a unique LangGraph thread,\" ensuring \"All patient behavior is deterministic, state-driven, and session-scoped.\" The architecture also incorporates a \"TreatmentAgent\" that \"Accepts treatment only after correct diagnosis\" and an \"EvaluatorAgent\" for \"Rubric-based",
    "created_at": "2026-02-02T05:41:15.681819+00:00"
  },
  {
    "chunk_id": 52,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 12,
    "text": "by `graph.invoke(state)`. Crucially, the slide emphasizes \"No global state. No shared memory.\" and that \"Each frontend session maps to a unique LangGraph thread,\" ensuring \"All patient behavior is deterministic, state-driven, and session-scoped.\" The architecture also incorporates a \"TreatmentAgent\" that \"Accepts treatment only after correct diagnosis\" and an \"EvaluatorAgent\" for \"Rubric-based diagnosis validation,\" which is \"Deterministic & explainable\" and \"triggered only when ready_for_diagnosis == true.\" A summary list reiterates that the \"Frontend sends user messages with a session_id,\" the \"Backend validates and routes requests per session,\" \"Each session maps to a dedicated LangGraph execution,\" \"Patient behavior is driven by a state machine,\" and the \"UI reflects agent state in real time.\" This detailed architecture explanation follows Ashmit Verma's previous emphasis on preventing \"cross user contamination\" (00:02:38\u201300:02:40) and his earlier explanation that each frontend session maps to a unique backend session object, with the Flask backend holding all patient state and no global or shared memory (00:02:00\u201300:02:16). At 00:02:45\u201300:02:48, Ashmit Verma begins to detail the technologies used, specifically mentioning HTML. He continues at 00:02:49\u201300:02:52, adding that CSS and JavaScript were used for the front end. This aligns with the current presentation slide, which is titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" (slide 2/10) and is divided into \"Architecture Overview\" and \"Technologies & Tooling.\" The \"Architecture Overview\" section describes the system's components and their interactions for an \"Agentic Patient Simulator.\" It highlights a \"Session Store\" that manages `version_id - patient state`, encompassing `symptoms_revealed`, `questions_asked`, `diagnosis`, and `treatment_plan`, explicitly stating \"No global state. No shared memory.\" The \"LangGraph Patient Agent\" is described with \"State Machine Nodes\" for \"Question Handling,\" \"Symptom Revelation,\" \"Diagnosis, State,\" and \"Treatment Phase,\" utilizing `graph-invocation`. Key architectural principles are outlined: \"Each frontend session maps to a unique LangGraph thread\" and \"All patient behavior is deterministic, state-driven, and session-scoped.\" The overall flow involves the \"Frontend sends user messages with a session_id,\" the \"Backend validates and routes requests per session,\" \"Each session maps to a dedicated LangGraph execution,\" \"Patient behavior is driven by a state machine,\" and the \"UI reflects agent state in real time.\" The slide also introduces a \"TreatmentAgent\"",
    "created_at": "2026-02-02T05:41:15.681820+00:00"
  },
  {
    "chunk_id": 53,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 13,
    "text": "overall flow involves the \"Frontend sends user messages with a session_id,\" the \"Backend validates and routes requests per session,\" \"Each session maps to a dedicated LangGraph execution,\" \"Patient behavior is driven by a state machine,\" and the \"UI reflects agent state in real time.\" The slide also introduces a \"TreatmentAgent\" that \"Accepts treatment only after correct diagnosis\" and an \"EvaluatorAgent\" for \"Rubric-based diagnosis validation,\" which is \"Deterministic & explainable\" and triggers \"when ready for diagnosis = true.\" The \"Technologies & Tooling\" section, under \"TechStack,\" specifies \"HTML / CSS / Vanilla JavaScript\" for the \"Frontend,\" emphasizing its \"Lightweight and framework-free UI,\" \"Full control over session switching and real-time UI updates,\" and \"No hidden abstractions during evaluation.\" At 00:02:49\u201300:02:52, Ashmit Verma specifies that CSS and JavaScript were used for the front end, continuing his explanation of the project's technology stack. The current slide, titled \"TechStack\" (slide 3/10) from the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" provides a detailed breakdown of the technologies. For the Frontend, it lists HTML, CSS, and Vanilla JavaScript, emphasizing a \"Lightweight and framework-free UI,\" \"Full control over session switching and real-time UI updates,\" and \"No hidden abstractions during evaluation.\" The Backend is built with Python and Flask, described as providing \"Simple, explicit request routing,\" \"Easy session validation and isolation,\" and being \"Ideal for deterministic, state-driven APIs.\" The Agent Framework leverages LangGraph and LangChain. LangGraph is used to \"Model patient behavior as a state machine,\" \"Enforce stage-gated transitions (question -> diagnosis -> treatment),\" and \"Guarantee deterministic, session-scoped graph execution.\" LangChain is utilized for \"controlled LLM interactions,\" separating \"language understanding from decision logic,\" and avoiding \"embedding reasoning inside prompts.\" For Data/State, an \"In-memory session store\" is implemented, which is \"(session-scoped),\" maps \"session_id -> patient_state,\" has \"No global memory,\" and \"Enables true multi-user isolation.\" This aligns with previous discussions about preventing \"cross user contamination\" and ensuring session isolation. At 00:02:49\u201300:02:52, Ashmit Verma specifies that CSS and JavaScript were used for the front end of the project. He continues from 00:02:52\u201300:03:00, explaining that he kept the front end simple to ensure a lightweight framework. For the backend, he utilized Python",
    "created_at": "2026-02-02T05:41:15.681821+00:00"
  },
  {
    "chunk_id": 54,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 14,
    "text": "about preventing \"cross user contamination\" and ensuring session isolation. At 00:02:49\u201300:02:52, Ashmit Verma specifies that CSS and JavaScript were used for the front end of the project. He continues from 00:02:52\u201300:03:00, explaining that he kept the front end simple to ensure a lightweight framework. For the backend, he utilized Python and Flask, and for the agent framework, he employed LangGraph. The current slide, titled \"Technologies & Tooling\" (slide 3/10) from the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" visually supports this explanation by detailing the project's \"TechStack.\" The Frontend section lists HTML, CSS, and Vanilla JavaScript, highlighting features like a \"Lightweight and framework-free UI,\" \"Full control over session switching and real-time UI updates,\" and \"No hidden abstractions during evaluation.\" The Backend section specifies Python and Flask (Flask Backend API), noting its \"Simple, explicit request routing,\" \"Easy session validation and isolation,\" and suitability for \"deterministic, state-driven APIs.\" The Agent Framework section details LangGraph, which \"Models patient behavior as a state machine,\" \"Enforces stage-gated transitions (question -> diagnosis -> treatment),\" and \"Guarantees deterministic, session-scoped graph execution.\" It also includes LangChain, used for \"controlled LLM interactions,\" to \"Separate language understanding from decision logic,\" and to \"Avoid embedding reasoning inside prompts.\" Ashmit Verma further explains from 00:03:00\u201300:03:06 that for data and state management, an in-memory session store was used because there isn't much data currently. He clarifies (00:03:09\u201300:03:16) that in future scopes, he plans to add a backend for data storage, but for the present (00:03:16\u201300:03:25), it's implemented as an in-memory session store to reduce the overall system's heaviness. The slide's \"Data / State\" section confirms the use of an \"In-memory session store (session-scoped)\" which \"Maps session_id -> patient_state\" and has \"No global memory.\" At 00:03:39, the presentation slide titled \"Key Design Choices & Engineering Decisions\" (slide 4/10) is displayed, detailing five core design principles for the Agentic Patient Simulator. These principles are: \"Session-Scoped State (No Global Memory)\" to ensure isolated patient states per user, preventing cross-user contamination and enabling reproducible, debuggable behavior; \"Real-Time Confidence Visualization\" for dynamic diagnostic confidence updates to make reasoning visible and avoid black-box decisions; \"LangGraph for Control Flow,",
    "created_at": "2026-02-02T05:41:15.681822+00:00"
  },
  {
    "chunk_id": 55,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 15,
    "text": "design principles for the Agentic Patient Simulator. These principles are: \"Session-Scoped State (No Global Memory)\" to ensure isolated patient states per user, preventing cross-user contamination and enabling reproducible, debuggable behavior; \"Real-Time Confidence Visualization\" for dynamic diagnostic confidence updates to make reasoning visible and avoid black-box decisions; \"LangGraph for Control Flow, Not Prompts\" which implements patient behavior as a state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution; \"Separation of Intent Classification and Symptom Revelation\" to classify user questions before revealing symptoms, preventing hallucinations and ensuring medical consistency; and \"Rubric-Based Diagnosis Evaluation\" using a deterministic EvaluatorAgent for explainable, fair, and consistent diagnosis validation. During this time, from 00:03:38 to 00:03:49, the AI Evaluator asks Ashmit Verma to elaborate on how 'LangGraph invoke_state' achieves deterministic behavior, specifically contrasting it with prompt-based randomness. Ashmit Verma responds from 00:03:50 to 00:03:52, confirming that deterministic behavior is achieved. He further explains from 00:03:52 to 00:04:01 that this is done through specific rules and offers to demonstrate it by showing the 'diseases and treatments.PUI files'. This directly addresses the \"LangGraph for Control Flow, Not Prompts\" section on the slide, which states that LangGraph \"Guarantees deterministic execution.\" At 00:03:58, the presentation slide titled \"Key Design Choices & Engineering Decisions\" (slide 4/10) is displayed, detailing five core design principles for the Agentic Patient Simulator. These principles are: \"Session-Scoped State (No Global Memory)\" to ensure isolated patient states per user, preventing cross-user contamination and enabling reproducible, debuggable behavior; \"LangGraph for Control Flow, Not Prompts\" which implements patient behavior as a state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution; \"Real-Time Confidence Visualization\" for dynamic diagnostic confidence updates to make reasoning visible and avoid black-box decisions; \"Separation of Intent Classification and Symptom Revelation\" to classify user questions before revealing symptoms, preventing hallucinations and ensuring medical consistency; and \"Rubric-Based Diagnosis Evaluation\" which validates diagnosis using a deterministic EvaluatorAgent, removing subjective LLM judgment, making evaluation explainable, and ensuring fairness and consistency. From 00:03:57\u201300:04:01, Ashmit Verma specifically mentions \"diseases and treatments dot PUI files,\" likely referring to the implementation or storage of medical",
    "created_at": "2026-02-02T05:41:15.681823+00:00"
  },
  {
    "chunk_id": 56,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 16,
    "text": "preventing hallucinations and ensuring medical consistency; and \"Rubric-Based Diagnosis Evaluation\" which validates diagnosis using a deterministic EvaluatorAgent, removing subjective LLM judgment, making evaluation explainable, and ensuring fairness and consistency. From 00:03:57\u201300:04:01, Ashmit Verma specifically mentions \"diseases and treatments dot PUI files,\" likely referring to the implementation or storage of medical logic related to the LangGraph control flow or rubric-based evaluation. At 00:03:59, the screen transitions to a Visual Studio Code interface, displaying the `disease.py` file from the `agentic-patient-simulator` project. From 00:03:57 to 00:04:01, Ashmit Verma states, \"diseases and treatments dot PUI files,\" directly referencing the type of content now visible on screen. The `disease.py` file defines a Python dictionary named `diseases`, which structures medical conditions with their associated symptoms. For instance, \"migraine\" is defined with `\"required_symptoms\"` like \"headache\" and \"nausea\", and `\"contained_symptoms\"` such as \"fatigue\", \"body ache\", and \"vomiting\". Similarly, \"allergies\" lists `\"required_symptoms\"` including \"headache\", \"light_sensitivity\", \"runny_nose\", \"swollen_mucous_membranes\", \"inflamed_bronchi\", and \"reddened_by_sunburn\", along with `\"contained_symptoms\"` like \"sneezing\". \"food_poisoning\" includes `\"required_symptoms\"` of \"nausea\" and \"vomiting\", and `\"contained_symptoms\"` like \"diarrhea\" and \"stomach pain\". Each disease entry also has a `\"min_patients\"` field, currently set to 0. This code provides the foundational data for the patient simulator, likely used by the agents to understand and process medical conditions and symptoms. At 00:04:01, the screen displays the `main.py` file in a Visual Studio Code editor, showing the core simulation logic for the 'agentic-patient-simulator' project. Ashmit Verma, speaking from 00:03:57 to 00:04:01, mentions \"diseases and treatments dot PUI files,\" which refers to the content from the previous keyframe. He then continues from 00:04:01 to 00:04:03, stating, \"Actually, what's happening is I have certainly,\" indicating he is about to explain the current context or an issue. The `main.py` file imports `os`, `json`, `typing`, and custom agents (`Agent`, `PatientAgent`, `DiagnosisAgent`, `EvaluationAgent`), `Graph`, and `utils`. The `run_simulation` function takes a `config` dictionary and initializes `patient_agent`, `diagnosis_agent`, `evaluation_agent`, and a `Graph` instance. It then runs a simulation loop for a configured number of steps. Within each step, it prints the step number, handles `patient_agent.interact(diagnosis_agent)` to get a `patient_response`, prints it, then `diagnosis_agent.process_patient_response(patient_response)` to get a `diagnosis_response`, prints it, and `evaluation_agent.evaluate(diagnosis_response, patient_agent.get_state())`",
    "created_at": "2026-02-02T05:41:15.681824+00:00"
  },
  {
    "chunk_id": 57,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 17,
    "text": "`config` dictionary and initializes `patient_agent`, `diagnosis_agent`, `evaluation_agent`, and a `Graph` instance. It then runs a simulation loop for a configured number of steps. Within each step, it prints the step number, handles `patient_agent.interact(diagnosis_agent)` to get a `patient_response`, prints it, then `diagnosis_agent.process_patient_response(patient_response)` to get a `diagnosis_response`, prints it, and `evaluation_agent.evaluate(diagnosis_response, patient_agent.get_state())` to get an `evaluation_result`, which is also printed. The `graph` is updated with `graph.add_interaction(patient_agent.name, diagnosis_agent.name, patient_response, diagnosis_response)`. The loop checks `if evaluation_result[\"diagnosis_complete\"]:` to print \"Diagnosis complete!\" and `break`. After the loop, `save_config` is called to store `final_diagnosis`, `patient_state`, and `graph_data` in `simulation_results.json`. The script concludes with a standard `if __name__ == \"__main__\":` block that loads `config.json` and executes `run_simulation(config)`. At 00:04:02, the screen shows a Visual Studio Code editor displaying the `main.py` file of the `agentic-patient-simulator` project. Ashmit Verma, speaking from 00:04:01 to 00:04:03, states, \"Actually, what's happening is I have certainly,\" and continues from 00:04:04 to 00:04:08 that he has \"putted and constrained\" the system. He clarifies from 00:04:08 to 00:04:14 that he has limited the system to only three diseases, rather than adding many. He explains from 00:04:14 to 00:04:21 that the observed deterministic behavior is a result of the specific symptoms and optional symptoms he has incorporated. From 00:04:22 to 00:04:28, Ashmit Verma specifies that these limited symptoms are defined and managed within the `diseasesandtreatments.py` file. He concludes from 00:04:28 to 00:04:35 that this setup ensures deterministic behavior, and \"Landgraf is only ensuring the.\" The `main.py` file visible on screen reflects a sophisticated agentic architecture. It imports core Python modules like `os`, `json`, `typing`, and `dotenv`, alongside a comprehensive set of `langchain` components including `AgentExecutor`, `create_react_agent`, `PromptTemplate`, `ChatOpenAI`, `Tool`, `TavilySearchResults`, and `TavilySearchAPIWrapper`. Crucially, it integrates custom modules from the `agentic_patient_simulator` package, such as `AgentLLM`, `AgentMemory`, `AgentState`, `AgentClassifier`, `AgentPrompts` (from the `agent` subdirectory), `Graph` (from `graph`), and `Simulator` (from `simulator`). The `AgenticPatientSimulator` class is defined with an `__init__` method that loads configuration from `config.json`, initializes a `ChatOpenAI` language model, and then instantiates various agent components (`AgentLLM`, `AgentMemory`, `AgentState`, `AgentClassifier`, `AgentPrompts`) and a `Graph` object. These components are then passed to a `Simulator` instance, which orchestrates the patient simulation.",
    "created_at": "2026-02-02T05:41:15.681825+00:00"
  },
  {
    "chunk_id": 58,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 18,
    "text": "(from `simulator`). The `AgenticPatientSimulator` class is defined with an `__init__` method that loads configuration from `config.json`, initializes a `ChatOpenAI` language model, and then instantiates various agent components (`AgentLLM`, `AgentMemory`, `AgentState`, `AgentClassifier`, `AgentPrompts`) and a `Graph` object. These components are then passed to a `Simulator` instance, which orchestrates the patient simulation. The `_load_config` method handles JSON file loading, and the `run` method delegates the simulation execution to the `simulator` object. The script's entry point demonstrates how to create and run the simulator for a specific patient ID, \"patient_1\". The file explorer on the left shows a well-organized project structure with distinct directories for `agent`, `data`, `graph`, `memory`, `models`, `prompts`, and `simulator`, indicating a modular design. A system notification at the bottom prompts the user to install a Python extension, and the system clock shows 5:00 PM on December 27, 2025, with a weather forecast of 18\u00b0C and mostly sunny. At 00:04:34, the screen shows a Visual Studio Code editor displaying the `diagnosis_agent.py` file, which is part of the `agentic-patient-simulator` project. Ashmit Verma, speaking from 00:04:32 to 00:04:35, explains that the system's \"behavior\" is such that \"Landgraf is only ensuring the\" and then from 00:04:36 to 00:04:37, he clarifies that it ensures the \"symptoms from these files.\" This utterance directly relates to the code on screen, which defines the `DiagnosisAgent` class. This class, inheriting from `Agent`, is explicitly described in its initialization as being \"responsible for diagnosing the patient's condition based on the provided symptoms and medical history.\" The `__init__` method configures this agent with `MemoryStrategy()`, `ModelStrategy()`, `PromptStrategy()`, and `UtilsStrategy()`, suggesting how it processes information and makes diagnostic decisions. The file explorer on the left panel shows the `backend` directory structure, including `agent`, `memory`, `model`, `prompt`, and `utils` subdirectories, from which these strategies are imported. The presence of `diagnosis_agent.py` and Ashmit's comments about symptoms and behavior reinforce the idea that this agent is central to how the system processes patient data for diagnosis, likely using the symptom definitions discussed in previous keyframes (e.g., `diseasesandtreatments.py`). A banner at the bottom indicates that \"project.euapp.co.in is sharing your screen,\" and system details like \"5:01 PM\",",
    "created_at": "2026-02-02T05:41:15.681826+00:00"
  },
  {
    "chunk_id": 59,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 19,
    "text": "about symptoms and behavior reinforce the idea that this agent is central to how the system processes patient data for diagnosis, likely using the symptom definitions discussed in previous keyframes (e.g., `diseasesandtreatments.py`). A banner at the bottom indicates that \"project.euapp.co.in is sharing your screen,\" and system details like \"5:01 PM\", \"12/27/2023\", and \"18\u00b0C Mostly sunny\" are visible. At 00:04:37, the Visual Studio Code editor is open to the `diseases.py` file, which is part of the `agentic-patient-simulator` project. Ashmit Verma, speaking from 00:04:36 to 00:04:37, explains that the system derives \"symptoms from these files.\" This directly relates to the content on screen, which defines a Python dictionary named \"diseases\". This dictionary structures medical conditions like \"fever\", \"migraine\", and \"food_poisoning\". Each disease entry specifies \"required_symptoms\" (e.g., \"cough\", \"sore_throat\" for fever; \"headache\" for migraine; \"nausea\", \"vomiting\" for food poisoning), \"optional_symptoms\" (e.g., \"fatigue\", \"body_pain\", \"headache\" for fever; \"light_sensitivity\", \"nausea\", etc., for migraine; \"stomach_pain\", \"diarrhea\" for food poisoning), and a \"min_questions\" parameter (e.g., 3 for fever and migraine, 2 for food poisoning). The file explorer sidebar on the left shows the project directory structure, with `diseases.py` located within the `data` folder. This visual context confirms Ashmit's previous statements (from 00:04:08 to 00:04:14 and 00:04:22 to 00:04:28) about constraining the system to a limited number of diseases and defining their symptoms in a specific file. A Google Meet overlay indicates screen sharing, and a VS Code notification prompts for the \"Pylance\" extension. At 00:04:39, the screen displays a presentation slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation\" with the specific section \"Key Design Choices & Engineering Decisions\" indicated as slide 4 out of 10. The slide details five core design principles for the project. These principles are: 1) **Session-Scoped State (No Global Memory)**, which ensures that each user session maintains its own isolated patient state. The rationale provided is to prevent cross-user contamination, enable true multi-user support, and make the system's behavior reproducible and debuggable. 2) **Real-Time Confidence Visualization**, where diagnostic confidence dynamically updates as symptoms are revealed. This design choice aims to make the reasoning process visible, turn the agent into a teaching",
    "created_at": "2026-02-02T05:41:15.681827+00:00"
  },
  {
    "chunk_id": 60,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 20,
    "text": "state. The rationale provided is to prevent cross-user contamination, enable true multi-user support, and make the system's behavior reproducible and debuggable. 2) **Real-Time Confidence Visualization**, where diagnostic confidence dynamically updates as symptoms are revealed. This design choice aims to make the reasoning process visible, turn the agent into a teaching tool, and avoid black-box decision making. 3) **Separation of Intent Classification and Symptom Revelation**, meaning user questions are classified first, and symptoms are only revealed if they are valid. This prevents hallucinated symptoms, keeps patient responses medically consistent, and allows for transparent reasoning. 4) **LangGraph for Control Flow, Not Prompts**, where patient behavior is implemented as a LangGraph state machine. This approach enforces strict medical flow, prevents premature diagnosis, and guarantees deterministic execution. 5) **Rubric-Based Diagnosis Evaluation**, where diagnosis is validated using a deterministic EvaluatorAgent. The benefits highlighted are removing subjective LLM judgment, making evaluation explainable, and ensuring fairness and consistency. There were no utterances during this keyframe. At 00:04:43, the screen displays a presentation slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is slide 4 out of 10. This slide details five key design choices and engineering decisions for an \"Agentic Patient Simulator.\" These include: 1) **Session-Scoped State (No Global Memory)**, where each user session maintains its own isolated patient state to prevent cross-user contamination, enable true multi-user support, and make behavior reproducible and debuggable. 2) **Real-Time Confidence Visualization**, which dynamically updates diagnostic confidence as symptoms are revealed, aiming to make reasoning visible, turn the agent into a teaching tool, and avoid black-box decision making. 3) **Separation of Intent Classification and Symptom Revelation**, classifying user questions first and revealing symptoms only if valid, to prevent hallucinated symptoms, keep patient responses medically consistent, and allow transparent reasoning. 4) **LangGraph for Control Flow, Not Prompts**, implementing patient behavior as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. 5) **Rubric-Based Diagnosis Evaluation**, validating diagnosis with a deterministic EvaluatorAgent to remove subjective LLM judgment, make evaluation explainable, and ensure fairness and consistency. The presentation is being viewed in a browser, indicated by",
    "created_at": "2026-02-02T05:41:15.681828+00:00"
  },
  {
    "chunk_id": 61,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 21,
    "text": "as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. 5) **Rubric-Based Diagnosis Evaluation**, validating diagnosis with a deterministic EvaluatorAgent to remove subjective LLM judgment, make evaluation explainable, and ensure fairness and consistency. The presentation is being viewed in a browser, indicated by elements like \"Projects - Canva\" and a screen-sharing notification from \"projects.icapp.co.in.\" No speakers were detected, and no utterances were assigned to this keyframe. At 00:04:45, the screen displays slide 4 out of 10 of a presentation titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on \"Key Design Choices & Engineering Decisions\" for an \"Agentic Patient Simulator.\" The slide outlines five key design principles: 1) **Session-Scoped State (No Global Memory)**, which ensures each user session has an isolated patient state to prevent cross-user contamination, support multiple users, and make behavior reproducible and debuggable. 2) **Real-Time Confidence Visualization**, dynamically updating diagnostic confidence as symptoms are revealed to make reasoning visible, serve as a teaching tool, and avoid black-box decisions. 3) **LangGraph for Control Flow, Not Prompts**, implementing patient behavior as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. 4) **Separation of Intent Classification and Symptom Revelation**, classifying user questions first and only revealing symptoms if valid, to prevent hallucinated symptoms, maintain medical consistency, and allow transparent reasoning. 5) **Rubric-Based Diagnosis Evaluation**, validating diagnosis with a deterministic EvaluatorAgent to remove subjective LLM judgment, make evaluation explainable, and ensure fairness and consistency. From 00:04:46 to 00:04:52, the AI Evaluator commented on the project, stating, \"You mentioned adding rich symptoms and opiates optional symptoms to achieve deterministic behavior.\" This utterance directly relates to the slide's emphasis on deterministic execution and consistent medical responses, particularly under the LangGraph and Rubric-Based Diagnosis Evaluation sections. The screen also shows a \"projects.icapp.co.in is sharing your screen\" notification, along with the current time (5:01 PM, 12/27/2025) and weather (18\u00b0C Mostly sunny). At 00:04:50, the screen shows slide 4 of 10 from a \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" titled \"Key Design Choices & Decisions\" for an \"Agentic Patient",
    "created_at": "2026-02-02T05:41:15.681829+00:00"
  },
  {
    "chunk_id": 62,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 22,
    "text": "shows a \"projects.icapp.co.in is sharing your screen\" notification, along with the current time (5:01 PM, 12/27/2025) and weather (18\u00b0C Mostly sunny). At 00:04:50, the screen shows slide 4 of 10 from a \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" titled \"Key Design Choices & Decisions\" for an \"Agentic Patient Simulator.\" The slide details five crucial design principles. These include: 1) **Session-Scoped State (No Global Memory)**, which ensures each user session maintains an isolated patient state to prevent cross-user contamination, enable multi-user support, and make behavior reproducible and debuggable. 2) **LangGraph for Control Flow, Not Prompts**, where patient behavior is implemented as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. 3) **Real-Time Confidence Visualization**, which dynamically updates diagnostic confidence as symptoms are revealed, aiming to make reasoning visible, turn the agent into a teaching tool, and avoid black-box decision making. 4) **Separation of Intent Classification and Symptom Revelation**, where user questions are classified first, and symptoms are revealed only if valid, to prevent hallucinated symptoms, keep patient responses medically consistent, and allow transparent reasoning. 5) **Rubric-Based Diagnosis Evaluation**, where diagnosis is validated using a deterministic EvaluatorAgent to remove subjective LLM judgment, make evaluation explainable, and ensure fairness and consistency. During this keyframe, from 00:04:46 to 00:04:52, AI Evaluator specifically mentioned the importance of adding rich symptoms and optional symptoms to achieve deterministic behavior, which directly relates to the slide's focus on deterministic execution and consistent medical responses. The screen also displays browser UI elements such as \"Projects - Canva,\" \"ICAPP Internships,\" \"projects.icapp.co.in,\" a notification stating \"This tab is sharing your screen\" with \"Memory usage: 428 MB,\" and the current local weather and time: \"18\u00b0C,\" \"Mostly sunny,\" \"5:01 PM,\" and \"12/27/2025.\" At 00:04:50, the screen transitions to a live demonstration of the \"ICAPP\" web application, which serves as an \"Internships, Capstone Projects, and Apprenticeships Program\" portal. There were no utterances made during this keyframe. The user interface shows a left sidebar with navigation links such as \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout,\" indicating a personalized user experience for",
    "created_at": "2026-02-02T05:41:15.681830+00:00"
  },
  {
    "chunk_id": 63,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 23,
    "text": "web application, which serves as an \"Internships, Capstone Projects, and Apprenticeships Program\" portal. There were no utterances made during this keyframe. The user interface shows a left sidebar with navigation links such as \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout,\" indicating a personalized user experience for \"Ashmit Verma(2024134).\" The central focus is on a project called \"Agentic Patient Simulator,\" which is explicitly identified as \"A Multi-User Agentic Medical Interview System.\" This system is noted to be built with \"Flask and Vanilla Web\" for the front-end and leveraging \"LangGraph, LangChain\" for its underlying logic. A Chrome browser notification confirms that \"projects.icapp.co.in\" is actively sharing the screen, displaying a thumbnail of the \"Agentic Patient Simulator\" and consuming 428 MB of memory. A \"Transcript\" panel on the right side of the screen presents a specific technical question: \"deterministic state-driven behavior. Could you elaborate on how 'graph.invoke_state' achieves this deterministic behavior, contrasting it with prompt-based randomness?\" This suggests an interactive component where users can query the system's design or functionality. Below this question, a text input field labeled \"Type your message...\" and a \"Send\" button are visible, indicating a chat or interaction interface. The footer of the page provides information about \"Companies and Educators\" and an \"About ICAPP\" section, explaining that the program aims to provide students with hands-on experience and bridge the gap between academic knowledge and practical application, collaborating with industry partners. The page navigation shows \"Page 1 of 10\" with \"Previous\" and \"Next\" options, implying a multi-page project or demo flow. At 00:04:51, the screen presents a live demonstration of the \"ICAPP\" web application, which functions as an \"Internships, Capstone Projects, and Apprenticeships Program\" portal. The user interface features a left sidebar with navigation links such as \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout,\" indicating a personalized experience for the logged-in user, \"Ashmit Verma(2024134).\" The central area highlights a project titled \"A Multi-User Agentic Medical Interview System,\" also referred to as an \"Agentic Patient Simulator.\" This system is explicitly stated to be built with \"LangGraph, L Flask, and Vanilla Web.\" A central panel",
    "created_at": "2026-02-02T05:41:15.681831+00:00"
  },
  {
    "chunk_id": 64,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 24,
    "text": "\"Logout,\" indicating a personalized experience for the logged-in user, \"Ashmit Verma(2024134).\" The central area highlights a project titled \"A Multi-User Agentic Medical Interview System,\" also referred to as an \"Agentic Patient Simulator.\" This system is explicitly stated to be built with \"LangGraph, L Flask, and Vanilla Web.\" A central panel shows \"Processing...\" and \"Page 1 of 10,\" suggesting an active process or paginated content. On the right, a \"Transcript\" panel displays text related to the system's functionality, specifically mentioning \"deterministic Behavior also land graph is only ensuring the symptoms from these files\" and \"treatments Dot py so data this is achieving the deterministic Behavior.\" Below this transcript, an interactive section with a \"Type your message...\" input field and a \"Send\" button is visible. The footer provides details about ICAPP's mission to bridge academic knowledge with practical application and offers contact information. A browser notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen.\" During this keyframe, from 00:04:46 to 00:04:52, the AI Evaluator asks about adding rich symptoms and optional symptoms to achieve deterministic behavior, which directly correlates with the on-screen text in the transcript panel regarding \"deterministic Behavior\" and \"land graph ensuring the symptoms from these files.\" At 00:04:51, the screen presents a detailed view of the \"Agentic Patient Simulator\" project page within the ICAPP platform, which is an \"Internships, Capstone Projects, and Apprenticeships Program.\" The interface, attributed to Ashmit Verma (2024134) from IIIT Delhi, features a left navigation panel with links such as \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout.\" The central area describes the \"Agentic Patient Simulator\" as a \"Multi-User Agentic Medical Interview System\" developed using \"LangGraph, LangChain, Flask, and Vanilla Web Sessions.\" An interactive demo is embedded, displaying elements related to patient management, including \"Patient History,\" \"New Patient,\" and an \"Interview in progress.\" Details for \"Patient 1\" show \"Age: 77 years,\" \"Gender: Female,\" and \"Textbook completed,\" while \"Patient 2\" indicates \"Diagnostic Certainty: 90%\" and \"Interview completed.\" Options to \"Create custom interview in 2 min\" and \"Total Remaining: 100\" are also visible. Concurrently, from 00:04:46 to 00:04:52, the AI Evaluator stated, \"You",
    "created_at": "2026-02-02T05:41:15.681832+00:00"
  },
  {
    "chunk_id": 65,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 25,
    "text": "in progress.\" Details for \"Patient 1\" show \"Age: 77 years,\" \"Gender: Female,\" and \"Textbook completed,\" while \"Patient 2\" indicates \"Diagnostic Certainty: 90%\" and \"Interview completed.\" Options to \"Create custom interview in 2 min\" and \"Total Remaining: 100\" are also visible. Concurrently, from 00:04:46 to 00:04:52, the AI Evaluator stated, \"You mentioned adding rich symptoms and opiates optional symptoms to achieve deterministic behavior.\" This statement is directly reflected in the \"Transcript\" panel on the right, which displays the full question: \"You mentioned adding 'rich symptoms and opiates optional symptoms' to achieve deterministic behavior. Could you elaborate on how these specific additions ensure a more predictable outcome in the simulation?\" The AI Evaluator continued this question from 00:04:53 to 00:04:59. A text input field \"Type your message...\" and \"Send\" button are available for interaction, along with \"Previous,\" \"Page 1 of 10,\" and \"Next\" navigation. A system notification confirms that \"projects.icapp.co.in is sharing your screen.\" At 00:04:54, the screen shows the \"Agentic Patient Simulator\" within the ICAPP platform, shifting from a previous interview-in-progress view to a patient data and symptom input interface. The central area of the application now prominently features sections for \"Symptoms\" and \"Patient History.\" Under \"Patient History,\" detailed input fields are available for \"Patient Name,\" \"Age,\" \"Gender,\" \"Weight,\" \"Height,\" \"Blood Type,\" \"Allergies,\" \"Medical Conditions,\" and \"Medications,\" allowing for comprehensive patient profile creation. The \"Symptoms\" section includes an \"Add Symptom\" button, indicating the ability to define specific symptoms for the simulation. Concurrently, the AI Evaluator, from 00:04:53 to 00:04:59, posed a question in the \"Transcript\" panel, asking for an explanation on how the addition of \"tech symptoms and optional symptoms\" contributes to achieving a more predictable and deterministic outcome within the simulation. This question is directly visible on screen, along with a \"Type your message\" input and \"Send\" button for interaction. A partially visible overlay, likely a project detail card, identifies the \"Interview System\" as being \"Built with LangGraph, LangChain, Flask, and Vanilla Web\" and attributes it to \"Ashmit Verma(2024134) IIIT Delhi.\" The overall context suggests a transition to demonstrating how patient data and symptoms are configured to influence the simulation's",
    "created_at": "2026-02-02T05:41:15.681834+00:00"
  },
  {
    "chunk_id": 66,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 26,
    "text": "A partially visible overlay, likely a project detail card, identifies the \"Interview System\" as being \"Built with LangGraph, LangChain, Flask, and Vanilla Web\" and attributes it to \"Ashmit Verma(2024134) IIIT Delhi.\" The overall context suggests a transition to demonstrating how patient data and symptoms are configured to influence the simulation's behavior. At 00:04:58, the screen transitioned to a presentation slide titled \"Key Design Choices & Engineering Decisions,\" which is page 4 out of 10. This slide details five key architectural principles for the Agentic Patient Simulator. Concurrently, from 00:04:53 to 00:04:59, the AI Evaluator asked Ashmit Verma to elaborate on how specific additions contribute to a more predictable simulation outcome. Ashmit Verma began his response at 00:04:59, saying, \"So what the,\". The slide content explains that \"Session-Scoped State (No Global Memory)\" is used to ensure each user session has an isolated patient state, preventing cross-user contamination, enabling true multi-user support, and making behavior reproducible and debuggable. \"LangGraph for Control Flow, Not Prompts\" is employed, implementing patient behavior as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. The system also features \"Real-Time Confidence Visualization,\" where diagnostic confidence updates dynamically as symptoms are revealed, making reasoning visible, turning the agent into a teaching tool, and avoiding black-box decision making. \"Rubric-Based Diagnosis Evaluation\" is used to validate diagnosis with a deterministic EvaluatorAgent, removing subjective LLM judgment, making evaluation explainable, and ensuring fairness and consistency. Finally, there is a \"Separation of Intent Classification and Symptom Revelation,\" where user questions are classified first, and symptoms are revealed only if valid, to prevent hallucinated symptoms, keep patient responses medically consistent, and allow transparent reasoning. The slide also hints at a \"Live System De\" and \"end Evidence\" section. At 00:05:00, the screen displays a web page from \"projects.icapp.co.in\" showcasing the \"Agentic Patient Simulator\" project. This project is identified as a \"Multi-User Agentic Medical Interview System\" developed by Ashmit Verma (2024134) from IIIT Delhi, built using technologies such as LangGraph, LangChain, Flask, and Vanilla Web. The user interface includes a left navigation panel with links for \"Home,\" \"My Application,\" \"My",
    "created_at": "2026-02-02T05:41:15.681835+00:00"
  },
  {
    "chunk_id": 67,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 27,
    "text": "the \"Agentic Patient Simulator\" project. This project is identified as a \"Multi-User Agentic Medical Interview System\" developed by Ashmit Verma (2024134) from IIIT Delhi, built using technologies such as LangGraph, LangChain, Flask, and Vanilla Web. The user interface includes a left navigation panel with links for \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout.\" An embedded image illustrates the simulator's user interface, featuring various input fields and a \"Page 1 of 10\" navigation, suggesting an interactive or multi-step process. A prominent \"Transcript\" section on the right displays a question directed at Ashmit Verma: \"You mentioned adding 'Rich symptoms and opiates optional symptoms' to achieve deterministic behavior. Could you elaborate on how these specific additions ensure a more predictable outcome in the simulation?\" Concurrently, from 00:04:59 to 00:05:05, Ashmit Verma began to explain, stating, \"So what the, required symptoms are doing is required symptoms are the symptoms that are specifically\". This utterance directly addresses the question visible in the transcript, indicating he is elaborating on how \"required symptoms\" contribute to deterministic behavior. The lower part of the page provides details about ICAPP Internships, Capstone Projects, and Apprenticeship Programs, emphasizing their role in providing hands-on experience and bridging the gap between academic knowledge and industry demands. A browser notification confirms that \"projects.icapp.co.in is sharing your screen.\" At 00:05:03, the screen displays a presentation slide titled \"Key Design Choices & Engineering Decisions,\" which is page 4 out of 10 of a \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma explains that \"required symptoms\" are those specifically needed to diagnose a particular disease, while \"optional symptoms\" increase the confidence bar, which he plans to demonstrate later in the project (00:05:00-00:05:21). He then proceeds to explain the key design choices and engineering decisions detailed on the slide (00:05:21). The slide outlines five core principles: 1. **Session Scoped State (No Global Memory)**: Ashmit Verma elaborates that each user session maintains its own isolated patient state, ensuring no global state and that every request mutates only its session state (00:05:22-00:05:35). The slide further explains this prevents cross-user contamination, enables true multi-user support,",
    "created_at": "2026-02-02T05:41:15.681836+00:00"
  },
  {
    "chunk_id": 68,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 28,
    "text": "outlines five core principles: 1. **Session Scoped State (No Global Memory)**: Ashmit Verma elaborates that each user session maintains its own isolated patient state, ensuring no global state and that every request mutates only its session state (00:05:22-00:05:35). The slide further explains this prevents cross-user contamination, enables true multi-user support, and makes behavior reproducible and debuggable. 2. **LangGraph for Control Flow, Not Prompts**: Ashmit Verma states that the LLM cannot skip steps because transitions are enforced by the LangGraph (00:05:36-00:05:43). The slide clarifies that patient behavior is implemented as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution. 3. **Real-Time Confidence Visualization**: The slide indicates that diagnostic confidence updates dynamically as symptoms are revealed, making reasoning visible, turning the agent into a teaching tool, and avoiding black-box decision making. Ashmit Verma also mentions that design choice five allows users to see why the model leans towards a diagnosis (00:06:04-00:06:09). 4. **Separation of Intent Classification and Symptom Revelation**: Ashmit Verma explains that understanding a question does not automatically reveal a symptom (00:05:55-00:06:04). The slide adds that user questions are classified first, and symptoms are revealed only if valid, preventing hallucinated symptoms, keeping patient responses medically consistent, and allowing transparent reasoning. 5. **Rubric-Based Diagnosis Evaluation**: Ashmit Verma highlights that diagnosis correctness is checked against revealed evidence and not model confidence (00:05:44-00:05:53). The slide details that diagnosis is validated using a deterministic EvaluatorAgent, which removes subjective LLM judgment, makes evaluation explainable, and ensures fairness and consistency. Ashmit Verma concludes by mentioning the \"Live system and demo,\" indicating that the \"demonstrator is here\" and referring to \"back end evidence\" (00:06:10-00:06:16). He also states that he created a session through a session ID and thread ID (00:06:18-00:06:22). The slide also shows a partial title \"Live System De...end Evidence.\" The presentation is viewed in a web browser, with \"projects.icapp.co.in is sharing your screen\" visible. At 00:06:19, the screen displays a presentation slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is page 4 of 10. Ashmit Verma explains that he created a session using a session ID",
    "created_at": "2026-02-02T05:41:15.681837+00:00"
  },
  {
    "chunk_id": 69,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 29,
    "text": "The presentation is viewed in a web browser, with \"projects.icapp.co.in is sharing your screen\" visible. At 00:06:19, the screen displays a presentation slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is page 4 of 10. Ashmit Verma explains that he created a session using a session ID and a thread ID (00:06:18-00:06:22), directly relating to the \"Session-Scoped State (No Global Memory)\" feature highlighted on the slide. This feature ensures that each user session maintains its own isolated patient state, preventing cross-user contamination, enabling true multi-user support, and making behavior reproducible and debuggable. The slide further details other key design choices for the Agentic Patient Simulator: \"LangGraph for Control Flow, Not Prompts,\" which implements patient behavior as a LangGraph state machine to enforce strict medical flow, prevent premature diagnosis, and guarantee deterministic execution; \"Real-Time Confidence Visualization,\" where diagnostic confidence updates dynamically as symptoms are revealed to make reasoning visible, turn the agent into a teaching tool, and avoid black-box decision making; \"Rubric-Based Diagnosis Evaluation,\" which validates diagnosis using a deterministic EvaluatorAgent to remove subjective LLM judgment, make evaluation explainable, and ensure fairness and consistency; and \"Separation of Intent Classification and Symptom Revelation,\" where user questions are classified first and symptoms are revealed only if valid, preventing hallucinated symptoms, keeping patient responses medically consistent, and allowing transparent reasoning. A screen sharing notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen.\" At 00:06:21, the screen displays a presentation slide titled \"Live System Demonstration & Backend Evidence,\" which is page 5 of 10 of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma states from 00:06:18 to 00:06:22 that he created a session using a session ID and a thread ID, which directly relates to the backend evidence being presented. The slide highlights the system's benefits under a \"Why\" section, emphasizing that it \"Prevents hallucinated symptoms,\" \"Keeps patient responses medically consistent,\" \"Allows transparent reasoning,\" \"Removes subjective LLM judgment,\" \"Makes evaluation explainable,\" and \"Ensures fairness and consistency.\" The visual content includes code-like snippets demonstrating a conversational flow, starting with a message \"Hello! How are you feeling?\" and a",
    "created_at": "2026-02-02T05:41:15.681838+00:00"
  },
  {
    "chunk_id": 70,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 30,
    "text": "a \"Why\" section, emphasizing that it \"Prevents hallucinated symptoms,\" \"Keeps patient responses medically consistent,\" \"Allows transparent reasoning,\" \"Removes subjective LLM judgment,\" \"Makes evaluation explainable,\" and \"Ensures fairness and consistency.\" The visual content includes code-like snippets demonstrating a conversational flow, starting with a message \"Hello! How are you feeling?\" and a response \"I'm feeling a bit under the weather. I have a cough and a sore throat.\" Further text explains that \"Each user initializes a unique backend session and LangGraph thread,\" and \"Symptoms are revealed gradually and gated by medical relevance.\" A key backend process is shown as a \"LangGraph transition: Questioning -> Diagnosis Gate (ready_for_diagnosis = true),\" indicating the system's progression towards diagnosis once sufficient information is gathered. The browser interface also shows elements like \"Projects - Canva,\" \"ICAPP Internships,\" and a file path \"C:/Users/ASUS/Downloads/AGENTIC_CHATBOT.pdf,\" along with system information like the current time (5:02 PM) and date (12/27/2025). At 00:06:22, the screen displays a presentation slide titled \"Live System Demonstration & Backend Evidence,\" which is page 5 of 10 of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma explains from 00:06:18 to 00:06:22 that he created a session through a session ID and thread ID, which is visually supported by the `POST /api/v1/chat/sessions` request shown on the slide, including `Content-Type: application/json` and a `{\"session_id\": \"6f7f8e9a-4b3c-4d5e-8f0a-1b2c3d4e5f6a\"}` payload. He further elaborates from 00:06:23 to 00:06:28 that he asks specific questions, and symptoms are revealed gradually. The slide demonstrates this process with JSON examples: an initial patient message \"How are you feeling?\" results in `{\"ready_for_diagnosis\": false,\"symptoms_revealed\": []}`. When the patient states \"I have a cough and shortness of breath,\" the system updates to `{\"ready_for_diagnosis\": false,\"symptoms_revealed\": [\"cough\", \"shortness of breath\"]}`. Adding \"I've also had a fever for a few days\" further updates the state to `{\"ready_for_diagnosis\": false,\"symptoms_revealed\": [\"cough\", \"shortness of breath\", \"fever\"]}`. The slide explicitly notes that \"Each user initializes a unique backend session and LangGraph thread\" and \"Symptoms are revealed gradually and gated by medical relevance.\" It also shows a `LangGraph transition: Questioning -> Diagnosis Gate (ready_for_diagnosis = true)` and emphasizes that \"Diagnosis is allowed only after sufficient evidence and",
    "created_at": "2026-02-02T05:41:15.681839+00:00"
  },
  {
    "chunk_id": 71,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 31,
    "text": "\"fever\"]}`. The slide explicitly notes that \"Each user initializes a unique backend session and LangGraph thread\" and \"Symptoms are revealed gradually and gated by medical relevance.\" It also shows a `LangGraph transition: Questioning -> Diagnosis Gate (ready_for_diagnosis = true)` and emphasizes that \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\" A \"Testing\" section is visible, along with a screen-sharing notification from \"projects.icapp.co.in.\" At 00:06:26, the screen displays a presentation slide titled \"Live System Demonstration & Backend Evider,\" which is page 5 of 10 of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma explains that symptoms are revealed gradually (00:06:25-00:06:28). He highlights the diagnosis (00:06:30-00:06:31), which is only permitted after sufficient evidence has been gathered (00:06:31-00:06:34). He further clarifies that the LangGraph transition moves from questioning to a diagnosis gate when the system is `ready_for_diagnosis` (00:06:35-00:06:40). The slide visually supports this by showing API interactions for a diagnostic chatbot. The top-left code block demonstrates creating a session via `curl -X POST http://127.0.0.1:8000/create_session`, which returns a `session_id` (\"7b132220-4f7c-41c9-a710-9f1204f3c973\") and `thread_id` (\"5ef3adc2-9287-9231-53b3d2070491\"). An initial chat message \"How are you feeling?\" receives the reply \"I've been feeling unusually tired lately.\", with the system identifying \"fatigue\" as a `symptoms_revealed` at `stage: 1`, and `ready_for_diagnosis: false`. The slide text confirms, \"Each user initializes a unique backend session and LangGraph\". The bottom code block shows a subsequent interaction where the user states \"I've got a cough.\", leading to `stage: 2`, `symptoms_revealed` now including \"fatigue\" and \"cough\", and `ready_for_diagnosis: true`. The system then provides a `diagnosis: \"Flu\"` and asks, \"Okay, I see. Do you have any other symptoms, like a sore throat or fever?\". A partially visible section on the right confirms a `final_state` with `diagnosis: \"Flu\"`, `reason: \"All required symptoms are present.\"` and `verdict: \"Correct\"`. The slide also explicitly states, \"Symptoms are revealed gradually and gated by medical relevance.\" At 00:06:38, the screen displays a slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which details the system's interaction flow and internal state. Ashmit Verma explains from 00:06:35 to 00:06:40 that the \"Landgraft tradition is questioning gate to ready ready for",
    "created_at": "2026-02-02T05:41:15.681840+00:00"
  },
  {
    "chunk_id": 72,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 32,
    "text": "revealed gradually and gated by medical relevance.\" At 00:06:38, the screen displays a slide titled \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which details the system's interaction flow and internal state. Ashmit Verma explains from 00:06:35 to 00:06:40 that the \"Landgraft tradition is questioning gate to ready ready for diagnosis,\" which directly corresponds to the on-screen text \"LangGraph transition: Questioning\" and \"Gate (ready_for_diagnosis =.\" The slide presents a user query, \"I am having a headache and a fever,\" and the assistant's follow-up question, \"I understand. How long have you been experiencing these symptoms?\" The system has successfully identified and stored `\"symptoms_revealed\": [\"headache\", \"fever\"]`. Crucially, the system's current state is `\"ready_for_diagnosis\": false,`, indicating it is not yet prepared to make a diagnosis. The slide also emphasizes that \"Each user initializes a unique backend session and LangGraph thread\" and that \"Symptoms are revealed gradually and gated by medical relevance,\" ensuring a structured and medically sound diagnostic process before \"Diagnosis is confirmed and validated.\" At 00:06:40, the screen displays slide 5 of 10 from the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on an \"Agentic Patient Simulator.\" Ashmit Verma explains from 00:06:35 to 00:06:40 that the system has reached a \"Landgraft tradition is questioning gate to ready ready for diagnosis,\" which is visually confirmed by the on-screen text \"LangGraph transition: Questioning Gate (ready_for_diagnosis =.\" The slide's JSON-like content shows a \"final_state\" where \"ready_for_diagnosis\" is set to `true`, indicating the system is prepared for diagnosis. This state was reached after a series of \"questions_asked\" like \"How are you feeling?\", \"Do you have a fever?\", \"Any cough or breathing issues?\", and \"Have you been coughing lately?\", which elicited \"last_answers\" revealing \"symptoms_revealed\" including \"fatigue,\" \"fever,\" and \"cough.\" An \"evaluation\" section confirms a \"verdict\": \"correct\" because \"All required symptoms are present.\" Ashmit Verma then starts to introduce the \"testing of my\" (00:06:41-00:06:43), corresponding to the new \"Testing\" section on the slide, which includes categories for \"Diseases,\" \"Required Symptoms,\" \"Optional Symptoms,\" and \"Min. Questions Needed,\" outlining the criteria for diagnosis. The slide also reiterates that \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\"",
    "created_at": "2026-02-02T05:41:15.681841+00:00"
  },
  {
    "chunk_id": 73,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 33,
    "text": "starts to introduce the \"testing of my\" (00:06:41-00:06:43), corresponding to the new \"Testing\" section on the slide, which includes categories for \"Diseases,\" \"Required Symptoms,\" \"Optional Symptoms,\" and \"Min. Questions Needed,\" outlining the criteria for diagnosis. The slide also reiterates that \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\" At 00:06:41, Ashmit Verma introduces the \"Testing\" phase of his project, stating, \"Here is the testing of my,\" as the screen transitions to slide 6 of 10 from the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" The slide visually supports this by displaying a JSON object snippet showing `\"ready_for_diagnosis\": true,` and `\"stage\": 2,` with `\"symptoms_revealed\": [\"fatigue\", \"fever\", \"cough\"]`. A key principle for the system is stated: \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\" The slide further elaborates on the testing criteria through a table. This table outlines \"Diseases\" such as \"FLU\" and \"Migraine,\" specifying their \"Required Symptoms\" (e.g., \"Fever, Cough\" for FLU, \"Headache\" for Migraine), \"Optional Symptoms\" (e.g., \"Fatigue, Body Pain, Headache\" for FLU, \"Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache\" for Migraine), and the \"Min. Questions Needed\" for diagnosis (3 for FLU, 2 for Migraine). This content demonstrates the deterministic validation process for diagnosis. At 00:06:43, the screen displays slide \"5 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on the \"Agentic Patient Simulator\" and its diagnostic testing. Ashmit Verma, from 00:06:41 to 00:06:43, introduced this section by stating, \"Here is the testing of my, model.\" He then pointed out from 00:06:44 to 00:06:45, \"These are the diseases,\" and further elaborated from 00:06:46 to 00:06:57, \"and these are the required system symptoms that I was talking about, the fever, cough, headache, nausea, vomiting. And these are the optional symptoms that increase the patient memory.\" The slide visually supports this by showing a detailed diagnostic process. A code-like block on the left illustrates a successful diagnosis: \"diagnosis\": \"flu\", with an \"evaluation\" confirming \"reason\": \"all required symptoms are present\", and \"verdict\": \"correct\". The system's \"final_state\" shows \"ready_for_diagnosis\": true, at \"stage\": 3, after having asked questions like \"How are you feeling?\", \"Do",
    "created_at": "2026-02-02T05:41:15.681842+00:00"
  },
  {
    "chunk_id": 74,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 34,
    "text": "by showing a detailed diagnostic process. A code-like block on the left illustrates a successful diagnosis: \"diagnosis\": \"flu\", with an \"evaluation\" confirming \"reason\": \"all required symptoms are present\", and \"verdict\": \"correct\". The system's \"final_state\" shows \"ready_for_diagnosis\": true, at \"stage\": 3, after having asked questions like \"How are you feeling?\", \"Do you have a fever?\", \"Any cough or breathing issues?\", and \"Have you been coughing lately?\". The patient's \"last_answers\" revealed \"symptoms_revealed\" including \"fatigue\", \"fever\", and \"cough\". On the right, the slide highlights the \"LangGraph transition: Questioning\" and a \"Gate (ready_for_diagnosis =\". A key principle is stated: \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\" Below this, a \"Testing\" section is introduced with headers for \"Diseases,\" \"Required Symptoms,\" \"Optional Symptoms,\" and \"Min. Questions Needed,\" setting the stage for a detailed discussion of how different diseases are evaluated based on symptoms. At 00:06:49, the screen displays slide 5 of 10 from the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on the \"Testing\" phase of the \"Agentic Patient Simulator.\" Ashmit Verma, speaking from 00:06:46 to 00:06:57, highlights the \"required system symptoms\" like \"fever, cough, headache, nausea, vomiting\" and the \"optional symptoms that increase the patient memory.\" The slide visually supports this by showing a code-like block indicating a \"LangGraph transition: Questioning Gate\" where the system is `ready_for_diagnosis: true` at `stage: 3`. It lists `questions_asked` including \"How are you feeling?\", \"Do you have a fever?\", \"Any cough or breathing issues?\", and \"Have you been coughing lately?\", which have led to `symptoms_revealed` such as \"fatigue,\" \"fever,\" and \"cough.\" The slide also explicitly states, \"Diagnosis is allowed only after sufficient evidence and validated deterministically.\" A table further details diagnostic criteria, with \"FLU\" requiring \"Fever, Cough\" and having \"Fatigue, Body Pain, Headache\" as optional symptoms, needing a minimum of 3 questions. At 00:06:50, the screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on the \"Testing\" phase of the \"Agentic Patient Simulator.\" No utterances were made during this specific keyframe. The slide features a comprehensive table detailing diagnostic criteria for various diseases. The table is structured",
    "created_at": "2026-02-02T05:41:15.681843+00:00"
  },
  {
    "chunk_id": 75,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 35,
    "text": "screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on the \"Testing\" phase of the \"Agentic Patient Simulator.\" No utterances were made during this specific keyframe. The slide features a comprehensive table detailing diagnostic criteria for various diseases. The table is structured with columns for \"Diseases,\" \"Required Symptoms,\" \"Optional Symptoms,\" and \"Min. Questions Needed.\" It provides specific entries for FLU (Required: Fever, Cough; Optional: Fatigue, Body Pain, Headache; Min. Questions: 3), Migraine (Required: Headache; Optional: Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness; Min. Questions: 2), and Food Poisoning (Required: Nausea, Vomiting; Optional: Diarrhea, Stomach Pain; Min. Questions: 2). Crucially, the slide also explains the model's logic: \"How this is used in the model:-\" explicitly states that \"Required symptoms \u2192 Must be revealed before diagnosis i.\" At 00:06:52, the screen continues to display slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" focusing on the \"Testing\" phase of the \"Agentic Patient Simulator.\" No utterances were made during this specific keyframe. The slide features a comprehensive table detailing diagnostic criteria for various diseases. The table is structured with columns for \"Diseases,\" \"Required Symptoms,\" \"Optional Symptoms,\" and \"Min. Questions Needed.\" It provides specific entries for FLU (Required: Fever, Cough; Optional: Fatigue, Body Pain, Headache; Min. Questions: 3), Migraine (Required: Headache; Optional: Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness; Min. Questions: 2), and Food Poisoning (Required: Nausea, Vomiting; Optional: Diarrhea, Stomach Pain; Min. Questions: 2). Crucially, the slide also explains the model's logic: \"Required symptoms\" \"Must be revealed before diagnosis.\" This aligns with previous discussions by Ashmit Verma about \"required system symptoms\" and \"optional symptoms that increase the patient memory\" in the diagnostic process. The slide is presented within a browser window, indicating active screen sharing. At 00:06:54, the screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is part of the \"Agentic Patient Simulator\" project. Ashmit Verma, speaking from 00:06:46 to 00:06:57, highlights the \"required system symptoms\" like \"fever, cough, headache, nausea, vomiting\" and",
    "created_at": "2026-02-02T05:41:15.681844+00:00"
  },
  {
    "chunk_id": 76,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 36,
    "text": "active screen sharing. At 00:06:54, the screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is part of the \"Agentic Patient Simulator\" project. Ashmit Verma, speaking from 00:06:46 to 00:06:57, highlights the \"required system symptoms\" like \"fever, cough, headache, nausea, vomiting\" and the \"optional symptoms that increase the patient memory.\" The slide visually supports this explanation with a detailed table. This table categorizes diseases (FLU, Migraine, Food Poisoning) by their \"Required Symptoms\" (e.g., Fever, Cough for FLU; Headache for Migraine; Nausea, Vomiting for Food Poisoning), \"Optional Symptoms\" (e.g., Fatigue, Body Pain, Headache for FLU; Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness for Migraine; Diarrhea, Stomach Pain for Food Poisoning), and \"Min. Questions Needed\" (3 for FLU, 2 for Migraine, 2 for Food Poisoning). Furthermore, the slide explicitly details \"How this is used in the model:-\", stating that \"Required symptoms\" must be revealed before diagnosis, \"Optional symptoms\" increase the confidence score of a disease, and \"Minimum questions\" ensure progressive questioning. The presentation is viewed in a browser, with \"projects.icapp.co.in is sharing your screen\" visible at the bottom. At 00:06:55, the screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" titled \"Testing,\" which is part of the \"Agentic Patient Simulator\" project. Ashmit Verma, speaking from 00:06:46 to 00:06:59, explains the \"required system symptoms\" like \"fever, cough, headache, nausea, vomiting\" and the \"optional symptoms that increase the patient memory,\" clarifying that they \"increase basically.\" The slide visually supports this by presenting a table that details \"Diseases\" (FLU, Migraine, Food Poisoning), their \"Required Symptoms\" (e.g., Fever, Cough for FLU; Headache for Migraine; Nausea, Vomiting for Food Poisoning), \"Optional Symptoms\" (e.g., Fatigue, Body Pain, Headache for FLU; Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness for Migraine; Diarrhea, Stomach Pain for Food Poisoning), and \"Min. Questions Needed\" (3 for FLU, 2 for Migraine, 2 for Food Poisoning). Below the table, the slide begins to explain \"How this is used in the model:-\", specifically stating that \"Required symptoms\" \"\u2192 Must be revealed before diagnosis",
    "created_at": "2026-02-02T05:41:15.681845+00:00"
  },
  {
    "chunk_id": 77,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 37,
    "text": "by Darkness for Migraine; Diarrhea, Stomach Pain for Food Poisoning), and \"Min. Questions Needed\" (3 for FLU, 2 for Migraine, 2 for Food Poisoning). Below the table, the slide begins to explain \"How this is used in the model:-\", specifically stating that \"Required symptoms\" \"\u2192 Must be revealed before diagnosis i\". However, unlike the previous keyframe, the detailed explanations for \"Optional symptoms\" and \"Minimum questions\" are no longer fully visible on the slide. The screen also shows open browser tabs for \"Projects - Canva\", \"Minimalist Clean Gears Green\", \"Agentic Patient Simulator\", and \"ICAPP Internships\", along with a notification that \"projects.icapp.co.in is sharing your screen.\" At 00:06:59, the screen displays slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is part of the \"Agentic Patient Simulator\" project. The slide is titled \"Testing\" and presents a table detailing \"Diseases\" (FLU, Migraine, Food Poisoning), their \"Required Symptoms\" (e.g., Fever, Cough for FLU; Headache for Migraine; Nausea, Vomiting for Food Poisoning), \"Optional Symptoms\" (e.g., Fatigue, Body Pain, Headache for FLU; Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness for Migraine; Diarrhea, Stomach Pain for Food Poisoning), and \"Min. Questions Needed\" (3 for FLU, 2 for Migraine, 2 for Food Poisoning). Ashmit Verma, speaking from 00:06:59 to 00:07:01, mentions \"the confidence bar,\" which relates to the explanation on the slide about how symptoms are used in the model. Specifically, the slide states that \"Required symptoms\" must be revealed before diagnosis, and \"Optional symptoms\" increase the confidence score of a disease. The slide also shows a file path \"File C:/Users/ASUS/Downloads/AGENTIC_CHATBOT.pdf\" and a screen sharing notification from \"projects.icapp.co.in\" at the bottom. At 00:07:02, the screen displays slide \"5 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation,\" which is associated with the \"Agentic Patient Simulator\" project. The slide presents a detailed table outlining diagnostic criteria for various diseases. For FLU, the required symptoms are Fever and Cough, optional symptoms include Fatigue, Body Pain, and Headache, and a minimum of 3 questions are needed. For Migraine, Headache is required, with optional symptoms like Light Sensitivity, Nausea,",
    "created_at": "2026-02-02T05:41:15.681847+00:00"
  },
  {
    "chunk_id": 78,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 38,
    "text": "project. The slide presents a detailed table outlining diagnostic criteria for various diseases. For FLU, the required symptoms are Fever and Cough, optional symptoms include Fatigue, Body Pain, and Headache, and a minimum of 3 questions are needed. For Migraine, Headache is required, with optional symptoms like Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, and being Relieved by Darkness, requiring a minimum of 2 questions. Food Poisoning requires Nausea and Vomiting, with optional symptoms of Diarrhea and Stomach Pain, also needing a minimum of 2 questions. Ashmit Verma, speaking from 00:07:02 to 00:07:04, continues his explanation by stating, \"Also, the minimum question needed to,\" referring to the \"Minimum questions\" section of the slide. This section further clarifies how the model uses this information: required symptoms must be revealed before diagnosis, optional symptoms increase the confidence score of a disease, and minimum questions ensure progressive questioning. A browser interface and a screen sharing notification from \"projects.icapp.co.in\" are also visible. At 00:07:05, Ashmit Verma begins to discuss the process to \"diagnose,\" further clarifying from 00:07:06 to 00:07:09 that this involves \"which should be required symptoms, basically.\" This utterance coincides with a visual transition to a live demonstration of the \"Agentic Patient Simulator\" web application, accessible via `localhost:5500`. The demo interface is divided into three key sections. On the left, a \"Sessions\" panel lists various patients, with \"Patient 4 cd8403d3\" currently active, and an option to add a \"+ New Patient.\" The central area displays a chat interaction where a \"Doctor\" has prescribed \"treatment rest in dark,\" which the \"Patient\" has accepted, stating, \"Okay doctor, I'll follow this treatment.\" The \"System\" then confirms \"Treatment accepted. Session completed,\" indicating a completed interaction cycle. On the right, a \"Patient Memory\" section details the symptoms identified for the patient, including \"\u2713 nausea,\" \"\u2713 light_sensitivity,\" \"\u2713 movement_worsens_headache,\" and \"\u2713 headache.\" Directly below this, the \"Diagnostic Confidence\" section shows the system's assessment: \"Flu - 11%,\" \"Food Poisoning - 9%,\" and a high \"Migraine - 80%.\" This high confidence for Migraine aligns with the listed symptoms, many of which were previously discussed as optional or required symptoms for",
    "created_at": "2026-02-02T05:41:15.681848+00:00"
  },
  {
    "chunk_id": 79,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 39,
    "text": "movement_worsens_headache,\" and \"\u2713 headache.\" Directly below this, the \"Diagnostic Confidence\" section shows the system's assessment: \"Flu - 11%,\" \"Food Poisoning - 9%,\" and a high \"Migraine - 80%.\" This high confidence for Migraine aligns with the listed symptoms, many of which were previously discussed as optional or required symptoms for Migraine in earlier slides. The demo visually illustrates how the system uses these symptoms to arrive at a diagnosis and suggest a treatment, directly following Ashmit Verma's explanation of required symptoms for diagnosis. At 00:07:10, the screen transitions to slide \"6 / 10\" of the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator\" project. Ashmit Verma, speaking from 00:07:10 to 00:07:13, explains that these are special interaction test cases, such as greeting, and then mentions \"your duration\" from 00:07:14 to 00:07:15. He continues from 00:07:15 to 00:07:21, referring to ambiguity handling, specifically stating, \"how many days have it been that I will show. Let me show you that too,\" indicating he is about to demonstrate or explain this further. The slide visually presents a detailed table outlining diagnostic criteria for various diseases. For FLU, the required symptoms are Fever and Cough, optional symptoms include Fatigue, Body Pain, and Headache, and a minimum of 3 questions are needed. For Migraine, Headache is required, with optional symptoms like Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, and being Relieved by Darkness, requiring a minimum of 2 questions. Food Poisoning requires Nausea and Vomiting, with optional symptoms of Diarrhea and Stomach Pain, also needing a minimum of 2 questions. A section titled \"How this is used in the model:-\" further clarifies that required symptoms must be revealed before diagnosis, optional symptoms increase the confidence score of a disease, and minimum questions ensure progressive questioning. The screen also shows a browser interface with a \"Stop sharing\" button, indicating an active screen share from 'projects.icapp.co.in', along with system information. At 00:07:19, the presentation displays slide 7 of 10, titled \"Special Interaction Test Cases,\" within the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator.\" Ashmit Verma,",
    "created_at": "2026-02-02T05:41:15.681849+00:00"
  },
  {
    "chunk_id": 80,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 40,
    "text": "a browser interface with a \"Stop sharing\" button, indicating an active screen share from 'projects.icapp.co.in', along with system information. At 00:07:19, the presentation displays slide 7 of 10, titled \"Special Interaction Test Cases,\" within the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator.\" Ashmit Verma, speaking from 00:07:15 to 00:07:21, explains that he will demonstrate \"ambiguity handling,\" specifically mentioning \"how many days have it been that I will show. Let me show you that too.\" He reinforces this intention from 00:07:23 to 00:07:24 by stating, \"like, right now only.\" The slide visually supports this by presenting a table for test cases, with an example of \"Greeting Normalization\" showing a \"User Input\" of \"Hello, how are you feeling?\" and a \"Patient State\" of \"Initial interaction, no symptoms disclosed.\" A partial second user input, \"I'm not feeling well right now, please help me,\" is also visible. The slide also highlights key aspects of the model's dynamic updates, which include \"symptom memory,\" \"disease confidence bars,\" and \"diagnosis permission,\" along with principles like \"Minimum questions\" and ensuring \"progressive questioning.\" The screen also shows a notification that \"projects.icapp.co.in is sharing your screen.\" At 00:07:24, the screen displays slide 7/10 of the \"Minimalist Clean Gears Green Project Review Corporate Presentation.\" Ashmit Verma, speaking from 00:07:23 to 00:07:24, states \"like, right now only,\" which aligns with his earlier commitment to demonstrate ambiguity handling. The slide visually supports this by detailing specific interaction handling scenarios for an AI system. It covers \"Greeting Normalization,\" showing how an initial interaction like \"Hello, how are you feeling?\" is handled when there are \"Initial interaction, no symptoms disclosed,\" and also includes a user input of \"I'm not feeling well right now, please help me.\" A new section, \"Duration Ambiguity Handling,\" is introduced, illustrating how the system might ask \"How many days has it been?\" when faced with \"Insufficient temporal context,\" and a user's response could be \"It's hard to say right now.\" Furthermore, the slide presents \"Treatment Validation,\" detailing the phase after \"Diagnosis completed, treatment phase,\" with an example treatment \"Treatment: rest in dark\" and potential user",
    "created_at": "2026-02-02T05:41:15.681850+00:00"
  },
  {
    "chunk_id": 81,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 41,
    "text": "ask \"How many days has it been?\" when faced with \"Insufficient temporal context,\" and a user's response could be \"It's hard to say right now.\" Furthermore, the slide presents \"Treatment Validation,\" detailing the phase after \"Diagnosis completed, treatment phase,\" with an example treatment \"Treatment: rest in dark\" and potential user feedback like \"Okay, I will do that.\" (if relevant) or \"I don't think this is related to m\" (if irrelevant). The bottom of the screen also indicates that \"projects.icapp.co.in is sharing your screen\" and shows local environmental details including \"18\u00b0C,\" \"Mostly sunny,\" \"5:03 PM,\" and \"12/27/2025.\" At 00:07:26, the screen displays slide 7 of 10 from the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator.\" Ashmit Verma, speaking from 00:07:26 to 00:07:28, says, \"Like here, I will ask, hi,\" which directly relates to the \"Greeting Normalization\" section on the slide, demonstrating an initial interaction with the patient simulator. The slide visually supports this by outlining specific interaction handling scenarios. The \"Greeting Normalization\" scenario shows the system asking \"Hello, how are you feeling?\" during an \"Initial interaction, no symptoms disclosed,\" with a user responding, \"I'm not feeling well right now, please help me.\" The \"Duration Ambiguity Handling\" section illustrates the system asking \"How many days has it been?\" when faced with \"Insufficient temporal context,\" and a user's response of \"It's hard to say right now.\" Crucially, the \"Treatment Validation\" section is now fully visible, detailing how the system presents a treatment like \"Treatment: rest in dark\" (example) once \"Diagnosis completed, treatment phase.\" It also shows potential user responses: \"If relevant: 'Okay, I will do that.'\" and \"If irrelevant: 'I don't think this is related to m'\" (partially visible). The bottom of the slide also contains text elements \"Kev Con\" and \"Future Scope.\" The overall display includes Google Chrome browser interface elements, such as tabs for \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"Agentic Patient Simulator,\" and \"ICAPP Internships,\" confirming the presentation is being viewed as a PDF from a local path. A notification at the bottom confirms screen sharing is active. At 00:07:28, the screen displays",
    "created_at": "2026-02-02T05:41:15.681851+00:00"
  },
  {
    "chunk_id": 82,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 42,
    "text": "Google Chrome browser interface elements, such as tabs for \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"Agentic Patient Simulator,\" and \"ICAPP Internships,\" confirming the presentation is being viewed as a PDF from a local path. A notification at the bottom confirms screen sharing is active. At 00:07:28, the screen displays slide 7 of 10 from the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator\" project. Ashmit Verma, speaking from 00:07:26 to 00:07:28, says, \"Like here, I will ask, hi,\" which directly relates to the \"Greeting Normalization\" section on the slide, demonstrating an initial interaction with the patient simulator. The slide visually supports this by outlining specific interaction handling scenarios. The \"Greeting Normalization\" scenario shows the system asking \"Hello, how are you feeling?\" during an \"Initial interaction, no symptoms disclosed,\" with a user responding, \"I'm not feeling well right now, please help me.\" The \"Duration Ambiguity Handling\" section illustrates the system asking \"How many days has it been?\" when faced with \"Insufficient temporal context,\" and a user's response of \"It's hard to say right now.\" Furthermore, the \"Treatment Validation\" section details the \"Diagnosis completed, treatment phase,\" providing an example treatment like \"Treatment: rest in dark\" and potential user responses such as \"Okay, I will do that\" if relevant, or \"I don't think this is related to my\u2026\" if irrelevant. The slide also features a prominent heading \"Key Contributions & Future Scope.\" Other on-screen elements include browser tabs for \"Projects - Canva\" and \"ICAPP Internships,\" a downloaded file \"AGENTIC_CHATBOT.pdf,\" and a \"projects.icapp.co.in is sharing your screen\" banner. The Windows taskbar shows the date 12/27/2025, time 5:03 PM, and weather. At 00:07:29, the presentation transitions to slide 6 of 10, titled \"Testing,\" which is part of the \"Minimalist Clean Gears Green Project Review Corporate Presentation\" for the \"Agentic Patient Simulator.\" Ashmit Verma, speaking from 00:07:30 to 00:07:35, explains the interaction flow, stating, \"So patient will be replying by this. Then I will ask for his symptoms.\" This utterance directly relates to the content of the \"Testing\" slide, which outlines the system's diagnostic logic based on symptoms. The slide",
    "created_at": "2026-02-02T05:41:15.681852+00:00"
  },
  {
    "chunk_id": 83,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 43,
    "text": "Patient Simulator.\" Ashmit Verma, speaking from 00:07:30 to 00:07:35, explains the interaction flow, stating, \"So patient will be replying by this. Then I will ask for his symptoms.\" This utterance directly relates to the content of the \"Testing\" slide, which outlines the system's diagnostic logic based on symptoms. The slide presents a detailed table with three diseases: FLU, Migraine, and Food Poisoning. For each disease, it specifies the \"Required Symptoms\" (e.g., \"Fever, Cough\" for FLU, \"Headache\" for Migraine, \"Nausea, Vomiting\" for Food Poisoning), \"Optional Symptoms\" (e.g., \"Fatigue, Body Pain, Headache\" for FLU, \"Light Sensitivity, Nausea, Movement Worsens Headache, Unilateral Headache, Relieved by Darkness\" for Migraine, \"Diarrhea, Stomach Pain\" for Food Poisoning), and the \"Min. Questions Needed\" for diagnosis (3 for FLU, 2 for Migraine, and 2 for Food Poisoning). A critical instruction on the slide, under \"How this is used in the model:-\", states that \"Required symptoms Must be revealed before diagnosis,\" emphasizing the necessity of core symptoms for the AI to make a diagnosis. The visual context also includes a screen sharing notification from \"projects.icapp.co.in\" and the Windows taskbar showing the date, time, and weather. At 00:07:31, the presentation shifts to a live demonstration of the \"Agentic Patient Simulator\" web application, running on `localhost:5500`. Ashmit Verma, speaking from 00:07:30 to 00:07:35, explains the interaction, stating, \"So patient will be replying by this. Then I will ask for his symptoms.\" This directly corresponds to the central panel of the demo, which shows a doctor-patient dialogue. In the displayed interaction, the doctor has already suggested \"treatmentrest in dark,\" and the patient has accepted with \"Okay doctor, I'll follow this treatment.\" The system confirms \"Treatment accepted\" and marks the \"Session completed.\" The left panel of the simulator lists various patient sessions, with \"Patient 4\" currently selected. The right panel provides crucial diagnostic information: \"Patient Memory\" for \"Patient 4 - cd8403d3\" shows confirmed symptoms like nausea, light sensitivity, movement worsening headache, and headache, with the treatment already completed. The \"Diagnostic Confidence\" section reveals the simulator's assessment, indicating an 80% probability for Migraine, significantly higher than Flu (11%) or Food Poisoning (9%), confirming the",
    "created_at": "2026-02-02T05:41:15.681853+00:00"
  },
  {
    "chunk_id": 84,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 44,
    "text": "information: \"Patient Memory\" for \"Patient 4 - cd8403d3\" shows confirmed symptoms like nausea, light sensitivity, movement worsening headache, and headache, with the treatment already completed. The \"Diagnostic Confidence\" section reveals the simulator's assessment, indicating an 80% probability for Migraine, significantly higher than Flu (11%) or Food Poisoning (9%), confirming the diagnosis and the session's completion. At 00:07:32, the live demonstration of the \"Agentic Patient Simulator\" web application continues, running on `localhost:5500`. Ashmit Verma, speaking from 00:07:30 to 00:07:35, explains the interaction flow, stating, \"So patient will be replying by this. Then I will ask for his symptoms.\" This utterance aligns with the current state of the application, which has reset to an initial state. The interface is divided into three columns: the left column shows \"Sessions\" with a \"+ New Patient\" button; the central column, titled \"Agentic Patient Simulator,\" displays \"No session selected\" and \"Diagnosis not allowed yet,\" with an input field prompting \"Create a patient to begin\" and a \"Send\" button. The right column contains sections for \"Patient Memory\" and \"Diagnostic Confidence.\" This setup indicates the system is ready for a new patient interaction, where Ashmit Verma intends to demonstrate the process of asking for symptoms and receiving patient replies. At 00:07:38, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` shows a progression from the previous state. Ashmit Verma, speaking from 00:07:38 to 00:07:40, continues his explanation by stating, \"which it says that.\" The application's interface is clearly visible, divided into three sections. The left panel, \"Sessions,\" now shows \"Patient 1\" (ID 99862010) as the active session, along with a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel has transitioned from an initial state to an active chat. It displays \"Doctor: Hi\" and indicates that the \"Patient is typing,\" preparing for a response. An input field below the chat prompts the user to \"Ask a question (e.g. Do you have fever?)\" and includes a \"Send\" button. The right panel still contains \"Patient Memory\" and \"Diagnostic Confidence\" sections. The status \"Diagnosis not allowed yet\" remains visible. The screen also displays various contextual elements such",
    "created_at": "2026-02-02T05:41:15.681854+00:00"
  },
  {
    "chunk_id": 85,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 45,
    "text": "An input field below the chat prompts the user to \"Ask a question (e.g. Do you have fever?)\" and includes a \"Send\" button. The right panel still contains \"Patient Memory\" and \"Diagnostic Confidence\" sections. The status \"Diagnosis not allowed yet\" remains visible. The screen also displays various contextual elements such as \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"ICAPP Internships,\" \"A School,\" and a sharing notification from \"projects.icapp.co.in.\" The local time displayed has updated to \"5:04 PM.\" At 00:07:39, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. Ashmit Verma, speaking from 00:07:38 to 00:07:40, states, \"which it says that,\" and then from 00:07:41 to 00:07:43, he adds, \"that's not been a issue.\" The application's interface is clearly visible, divided into three sections. The left panel, \"Sessions,\" shows \"Patient 1\" (ID 99862010) as the active session, along with a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel now displays an active chat where the \"Doctor: HI\" has been met with a patient's response: \"Patient: I'm just not feeling very well.Please help me !!\" An input field below the chat prompts the user to \"Ask a question (e.g. Do you have fever?)\" and includes a \"Send\" button. The right panel contains \"Patient Memory\" and a \"Diagnostic Confidence\" section, which now lists \"Flu - 0%,\" \"Food Poisoning - 0%,\" and \"Migraine - 0%.\" The status \"Diagnosis not allowed yet\" is also present. At 00:07:46, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. No speakers were detected and no utterances were assigned to this keyframe. The application's interface is clearly visible, divided into three main columns. The left panel, \"Sessions,\" shows \"Patient 1\" (ID 99862010) as the active session, along with a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel displays an ongoing chat. The conversation has progressed from the previous keyframe, now showing \"Doctor: HI,\" the patient's initial complaint \"Patient: I'm just not feeling very well. Please help me !!\", and the doctor's follow-up question \"Doctor: Do you have fever?\". The system indicates \"Patient is typing,\" suggesting a response",
    "created_at": "2026-02-02T05:41:15.681855+00:00"
  },
  {
    "chunk_id": 86,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 46,
    "text": "displays an ongoing chat. The conversation has progressed from the previous keyframe, now showing \"Doctor: HI,\" the patient's initial complaint \"Patient: I'm just not feeling very well. Please help me !!\", and the doctor's follow-up question \"Doctor: Do you have fever?\". The system indicates \"Patient is typing,\" suggesting a response is being generated. Below the chat, an input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are available for the doctor to interact. The right panel, \"Patient Memory,\" displays \"Diagnostic Confidence\" with current percentages for \"Flu - 0%,\" \"Food Poisoning - 0%,\" and \"Migraine - 0%.\" The status \"Diagnosis not allowed yet\" is also visible. A screen sharing notification from `projects.icapp.co.in` is present at the bottom of the browser window, along with local time and weather information (\"18\u00b0C\", \"Mostly sunny\", \"5:04 PM\", \"12/27/2025\"). At 00:07:47, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. Ashmit Verma, speaking from 00:07:48 to 00:07:52, refers to \"A novious symptom, which is between\" while the AI Evaluator completes the sentence from 00:07:51 to 00:07:54, adding \"food poisoning and your presentation well.\" The AI Evaluator then prompts Ashmit Verma to \"Please continue\" from 00:07:54 to 00:07:56. The application's interface is clearly visible, divided into three main columns. The left panel, \"Sessions,\" shows \"Patient 1\" with the ID \"9986200\" as the active session, along with a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel displays the ongoing chat. The conversation has progressed, now showing \"Doctor: Hi,\" the patient's initial complaint \"Patient: I'm just not feeling very well.Please help me !!\", the doctor's follow-up question \"Doctor: Do you have fever?\", and the patient's new response: \"Patient: No, I don't think that's been an issue.\" An input field below the chat prompts the user to \"Ask a question (e.g. Do you have fever?)\" and includes a \"Send\" button. The right panel contains \"Patient Memory\" and a \"Diagnostic Confidence\" section, which lists \"Flu - 0%,\" \"Food Poisoning - 0%,\" and \"Migraine - 0%.\" The status \"Diagnosis not allowed yet\" is also present. Other on-screen elements include \"Projects",
    "created_at": "2026-02-02T05:41:15.681856+00:00"
  },
  {
    "chunk_id": 87,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 47,
    "text": "a question (e.g. Do you have fever?)\" and includes a \"Send\" button. The right panel contains \"Patient Memory\" and a \"Diagnostic Confidence\" section, which lists \"Flu - 0%,\" \"Food Poisoning - 0%,\" and \"Migraine - 0%.\" The status \"Diagnosis not allowed yet\" is also present. Other on-screen elements include \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"ICAPP Internships,\" \"A School,\" \"projects.icapp.co.in is sharing your screen,\" \"Stop sharing,\" \"Hide,\" \"18\u00b0C,\" \"Mostly sunny,\" \"Q Search,\" \"5:04 PM,\" and \"12/27/2025.\" At 00:07:56, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. The AI Evaluator, speaking from 00:07:54 to 00:07:56, prompted the presenter to \"Please continue.\" The AI Evaluator then further specified from 00:07:56 to 00:07:59, asking for a continuation of the \"explanation of the required and optional symptoms.\" The application's interface is clearly visible, structured into three main columns. The left panel, labeled \"Sessions,\" displays \"Patient 1\" with the ID \"9986200\" as the active session, alongside a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel shows the ongoing chat. The conversation has progressed, with the patient initially stating, \"I'm just not feeling very well.Please help me !!\" The doctor then asked, \"Do you have fever?\", to which the patient replied, \"No, I don't think that's been an issue.\" Following this, the doctor posed a new question, \"Do you have nausea?\". Below the chat, the system indicates \"Patient is typing,\" suggesting that the patient simulator is generating a response. An input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are present for user interaction. The right panel, titled \"Patient Memory,\" features a \"Diagnostic Confidence\" section, which currently shows \"Flu - 0%,\" \"Food Poisoning - 0%,\" and \"Migraine - 0%.\" The status \"Diagnosis not allowed yet\" is also displayed. At 00:07:57, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. The AI Evaluator, speaking from 00:07:56 to 00:07:59, continued to ask for an explanation of the \"required and optional symptoms.\" Ashmit Verma then responded from 00:08:02 to 00:08:05, stating that \"food poisoning is a symptom between,\" and further elaborated",
    "created_at": "2026-02-02T05:41:15.681856+00:00"
  },
  {
    "chunk_id": 88,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 48,
    "text": "demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. The AI Evaluator, speaking from 00:07:56 to 00:07:59, continued to ask for an explanation of the \"required and optional symptoms.\" Ashmit Verma then responded from 00:08:02 to 00:08:05, stating that \"food poisoning is a symptom between,\" and further elaborated from 00:08:05 to 00:08:08 that it's \"of more food poisoning and migraine and nausea.\" The application's interface is clearly visible, structured into three main columns. The left panel, labeled \"Sessions,\" displays \"Patient 1 - 99862010\" as the active session, alongside a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel shows the ongoing chat conversation. The dialogue has progressed significantly from the previous keyframe. The doctor asked, \"Doctor: Do you have fever?\", to which the patient replied, \"Patient: No, I don't think that's been an issue.\" Following this, the doctor asked, \"Doctor: Do you have nausea?\", and the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" The right panel now explicitly lists \"nausea\" under \"Patient Memory.\" The \"Diagnostic Confidence\" section has updated values: \"Flu - 0%\", \"Food Poisoning - 44%\", and \"Migraine - 56%\", with the status \"Diagnosis not allowed yet.\" An input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are available for further interaction. At 00:08:08, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. Ashmit Verma, speaking from 00:08:05 to 00:08:08, explained that the symptoms are \"of more food poisoning and migraine and nausea.\" He then began to explain, from 00:08:08 to 00:08:10, \"So what happens is,\" The application's interface is clearly visible, structured into three main columns. The left panel, labeled \"Sessions,\" displays \"Patient 1 - 99862040\" as the active session, alongside a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel shows the ongoing chat conversation. The dialogue has progressed, with the doctor asking, \"Doctor: Do you have fever?\", to which the patient replied, \"Patient: No, I don't think that's been an issue.\" The doctor then asked, \"Doctor: Do you have nausea?\", and the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" The",
    "created_at": "2026-02-02T05:41:15.681857+00:00"
  },
  {
    "chunk_id": 89,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 49,
    "text": "ongoing chat conversation. The dialogue has progressed, with the doctor asking, \"Doctor: Do you have fever?\", to which the patient replied, \"Patient: No, I don't think that's been an issue.\" The doctor then asked, \"Doctor: Do you have nausea?\", and the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" The right panel, labeled \"Patient Memory,\" now explicitly shows \"\u2713 nausea\" as a confirmed symptom for \"Patient 1 - 99862040.\" Below this, the system indicates \"Diagnosis not allowed yet\" and presents \"Diagnostic Confidence\" levels: \"Flu - 0%\", \"Food Poisoning - 44%\", and \"Migraine - 56%\". The screen also displays \"Memory usage: 362 MB.\" An input field \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are present below the chat. A browser notification at the bottom indicates \"projects.icapp.co.in is sharing your screen.\" Other on-screen elements include \"Projects - Canva\", \"Minimalist Clean Gears Green\", \"ICAPP Internships\", \"18\u00b0C\", \"Mostly sunny\", \"Search\", \"5:04 PM\", and \"12/27/2025\". At 00:08:09, the live demonstration of the \"Agentic Patient Simulator\" web application on `localhost:5500` continues. Ashmit Verma, speaking from 00:08:08 to 00:08:10, initiated his explanation by stating, \"So what happens is,\" and then elaborated from 00:08:10 to 00:08:16 that he \"will elaborate further more to a specific disease to increase diagnostic confidence.\" He provided an example from 00:08:16 to 00:08:21, mentioning he \"will be asking, asking, do you have a stomach ache?\" Ashmit Verma continued from 00:08:25 to 00:08:30, explaining that if the patient \"shows it doesn't have us, so we will be constraining the symptom.\" The AI Evaluator then added from 00:08:30 to 00:08:34, \"my elaborating further on a specific disease to increase diagnostic confidence.\" The application's interface is structured into three main columns. The left \"Sessions\" panel displays \"Patient 1\" with the ID \"99962010\" as the active session, alongside a \"+ New Patient\" option. The central \"Agentic Patient Simulator\" panel shows the ongoing chat conversation where the doctor asks about fever and nausea, and the patient confirms feeling nauseous. The text \"Diagnosis not allowed yet\" is visible. The right panel includes \"Patient Memory\" listing \"nausea\" and \"Diagnostic Confidence\" showing \"Flu - 0%,\" \"Food",
    "created_at": "2026-02-02T05:41:15.681858+00:00"
  },
  {
    "chunk_id": 90,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 50,
    "text": "option. The central \"Agentic Patient Simulator\" panel shows the ongoing chat conversation where the doctor asks about fever and nausea, and the patient confirms feeling nauseous. The text \"Diagnosis not allowed yet\" is visible. The right panel includes \"Patient Memory\" listing \"nausea\" and \"Diagnostic Confidence\" showing \"Flu - 0%,\" \"Food Poisoning - 44%,\" and \"Migraine - 56%.\" A new text element \"A School\" is also present on the screen. At 00:08:34, the \"Agentic Patient Simulator\" web application is actively displayed, showing the ongoing diagnostic conversation. The AI Evaluator, from 00:08:30 to 00:08:34, explained the strategy of \"elaborating further on a specific disease to increase diagnostic confidence.\" This aligns with the doctor's current line of questioning in the central chat panel. The conversation shows the doctor asking, \"Doctor: Do you have nausea?\", to which the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" Following this, the doctor has now inquired, \"Doctor: Do you have stomach ache?\", and the system indicates that the \"Patient is typing\" a response. The left \"Sessions\" panel continues to show \"Patient 1 - 99862010\" as the active patient. On the right, \"Patient Memory\" lists \"nausea,\" and \"Diagnostic Confidence\" remains at Flu - 0%, Food Poisoning - 44%, and Migraine - 56%. The application is running on `localhost:5500` and is being shared via `projects.icapp.co.in`. At 00:08:35, the \"Agentic Patient Simulator\" web application continues to be displayed, showing the ongoing diagnostic conversation. The AI Evaluator, from 00:08:38 to 00:08:39, prompted the continuation of the simulation by stating, \"Please continue.\" The application's interface is structured into three main columns. The left \"Sessions\" panel shows \"Patient 1\" with the ID \"Patient 1-99862010\" as the active patient. The central \"Agentic Patient Simulator\" panel displays the chat. Following the doctor's previous question, \"Doctor: Do you have stomach ache?\", the patient has now responded, \"Patient: No, I don't think that's been an issue.\" This follows the patient's earlier confirmation, \"Patient: Yes, I've been feeling nauseous.\" The right panel, \"Patient Memory,\" lists \"nausea,\" and \"Diagnostic Confidence\" shows Flu at 0%, Food Poisoning at 44%, and Migraine at 56%. The on-screen text also includes \"Ask a",
    "created_at": "2026-02-02T05:41:15.681859+00:00"
  },
  {
    "chunk_id": 91,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 51,
    "text": "\"Patient: No, I don't think that's been an issue.\" This follows the patient's earlier confirmation, \"Patient: Yes, I've been feeling nauseous.\" The right panel, \"Patient Memory,\" lists \"nausea,\" and \"Diagnostic Confidence\" shows Flu at 0%, Food Poisoning at 44%, and Migraine at 56%. The on-screen text also includes \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button, along with general web browser elements like \"Projects - Canva\", \"Minimalist Clean Gears Green\", \"ICAPP Internships\", \"A School\", \"localhost:5500\", \"projects.icapp.co.in is sharing your screen.\", \"Stop sharing\", \"Hide\", \"18\u00b0C\", \"Mostly sunny\", \"Search\", \"5:04 PM\", and \"12/27/2025\". At 00:08:49, the \"Agentic Patient Simulator\" web application continues to be displayed on `localhost:5500`. No utterances were made by any speakers during this specific keyframe. The application's interface is visible, though partially obscured by an open Chrome browser menu. The left \"Sessions\" panel shows \"+ New Patient\" and the active \"Patient 1\" with the ID \"99962010.\" The central \"Agentic Patient Simulator\" chat window shows the ongoing diagnostic conversation where the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" The doctor then asked, \"Doctor: Do you have stomach ache?\", to which the patient replied, \"Patient: No, I don't think that's been an issue.\" A \"Send\" button and the text \"Do bright\" are present below the chat. The right panel displays \"Patient Memory\" with \"nausea\" listed, and \"Diagnostic Confidence\" indicating Flu at 0%, Food Poisoning at 44%, and Migraine at 56%, along with a \"Diagnosis not allowed yet\" message. The most notable visual change from the previous keyframe is the appearance of an open Chrome browser menu on the right side of the screen, covering part of the application. This menu includes options like \"New tab Ctrl+T,\" \"New window Ctrl+N,\" \"New Incognito window Ctrl+Shift+N,\" user account details for \"A Ashmit (iitd.ac.in),\" and various browser functions such as \"History,\" \"Downloads Ctrl+J,\" \"Settings,\" and \"Exit.\" The bottom of the screen also shows a notification that \"projects.icapp.co.in is sharing your screen,\" along with system information like the time \"5:05 PM\" and date \"12/27/2025.\" At 00:08:50, the \"Agentic Patient Simulator\" web application, running on `localhost:5500`, is displayed, though partially obscured by",
    "created_at": "2026-02-02T05:41:15.681860+00:00"
  },
  {
    "chunk_id": 92,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 52,
    "text": "\"History,\" \"Downloads Ctrl+J,\" \"Settings,\" and \"Exit.\" The bottom of the screen also shows a notification that \"projects.icapp.co.in is sharing your screen,\" along with system information like the time \"5:05 PM\" and date \"12/27/2025.\" At 00:08:50, the \"Agentic Patient Simulator\" web application, running on `localhost:5500`, is displayed, though partially obscured by an open Chrome browser menu. No utterances were made by any speakers during this specific keyframe. The simulator's left \"Sessions\" panel shows \"+ New Patient\" and the active \"Patient 1\" with the ID \"99862010.\" The central \"Agentic Patient Simulator\" chat window continues the diagnostic conversation, where the patient confirmed, \"Patient: Yes, I've been feeling nauseous.\" The doctor then asked, \"Doctor: Do you have stomach ache?\", to which the patient replied, \"Patient: No, I don't think that's been an issue.\" A \"Send\" button and the text \"Do I right\" are present below the chat, along with a \"Diagnosis not allowed yet\" message. The right panel displays \"Patient Memory\" with \"nausea\" listed, and \"Diagnostic Confidence\" shows \"Flu - 0%\" and \"Food Poisoning - 44%\". The Chrome browser's three-dot menu is fully open, presenting a comprehensive list of options including \"New tab Ctrl+T,\" \"New window Ctrl+N,\" \"New Incognito window Ctrl+Shift+N,\" and user account details for \"A Ashmit (iiitd.ac.in) Signed in >.\" Detailed sub-menus are visible for \"Passwords and autofill >,\" \"History >,\" \"Downloads Ctrl+J,\" and \"Bookmarks and lists >,\" which includes options like \"Bookmark this tab... Ctrl+D,\" \"Bookmark all tabs... Ctrl+Shift+D,\" \"Show bookmarks bar Ctrl+Shift+B,\" \"Show all bookmarks,\" \"Bookmark manager Ctrl+Shift+O,\" \"Import bookmarks and settings...,\" \"Reading list,\" \"Other bookmarks >,\" and \"Mobile bookmarks >.\" Additional browser functionalities like \"Extensions >,\" \"Delete browsing data... Ctrl+Shift+Del,\" \"Zoom 100% +,\" \"Print... Ctrl+P,\" \"Search with Google Lens,\" \"Translate...,\" \"Find and edit >,\" \"Cast, save and share >,\" \"More tools >,\" \"Help >,\" \"Settings >,\" and \"Exit\" are also visible. A screen sharing notification from \"projects.icapp.co.in\" is present at the bottom, offering \"Stop sharing\" and \"Hide\" options. The bottom right corner shows environmental information: \"18\u00b0C Mostly sunny\" and the current date and time \"5:05 PM 12/27/2025.\" At 00:08:51, the \"Agentic Patient Simulator\" web application is fully visible, no",
    "created_at": "2026-02-02T05:41:15.681861+00:00"
  },
  {
    "chunk_id": 93,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 53,
    "text": "also visible. A screen sharing notification from \"projects.icapp.co.in\" is present at the bottom, offering \"Stop sharing\" and \"Hide\" options. The bottom right corner shows environmental information: \"18\u00b0C Mostly sunny\" and the current date and time \"5:05 PM 12/27/2025.\" At 00:08:51, the \"Agentic Patient Simulator\" web application is fully visible, no longer obscured by the Chrome browser menu as in the previous keyframe. Ashmit Verma, from 00:08:52 to 00:08:55, points out that \"we are seeing that nausea\" and from 00:08:55 to 00:08:57, he adds \"and light sensitivity.\" He then clarifies from 00:08:57 to 00:09:00 that this \"is going specifically.\" The application's interface shows a \"Sessions\" panel on the left with \"Patient 1 - 99862010\" selected. The central chat window continues the diagnostic conversation where the patient reports, \"Yes, I've been feeling nauseous.\" The doctor asks, \"Do you have stomach ache?\" and the patient replies, \"No, I don't think that's been an issue.\" An input field below the chat now contains \"Do bright\" (changed from \"Do I right\" in the previous frame) and a \"Send\" button. The right panel's \"Patient Memory\" section explicitly lists \"\u2713 nausea,\" aligning with Ashmit's observation. The \"Diagnostic Confidence\" panel shows \"Flu \u2014 0%,\" \"Food Poisoning \u2014 44%,\" and \"Migraine \u2014 56%,\" with the Migraine confidence now fully visible due to the removal of the browser overlay. The presence of \"Do bright\" in the input field and Ashmit's mention of \"light sensitivity\" suggest a potential next step in the diagnostic process related to migraine symptoms. At 00:09:01, the \"Agentic Patient Simulator\" web application is shown, running on `localhost:5500`. Ashmit Verma, from 00:09:02 to 00:09:06, states, \"So what we'll be doing next is asking,\" indicating an upcoming action in the simulation, likely referring to the doctor asking the next question. The application's interface is structured into three columns. The left \"Sessions\" panel displays \"+ New Patient\" and \"Patient 1\" (with ID \"99862010\") as the active patient. The central \"Agentic Patient Simulator\" panel features the ongoing chat. The doctor previously asked, \"Doctor: Do you have stomach ache?\", and the patient responded, \"Patient: No, I don't think that's been an issue.\"",
    "created_at": "2026-02-02T05:41:15.681863+00:00"
  },
  {
    "chunk_id": 94,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 54,
    "text": "left \"Sessions\" panel displays \"+ New Patient\" and \"Patient 1\" (with ID \"99862010\") as the active patient. The central \"Agentic Patient Simulator\" panel features the ongoing chat. The doctor previously asked, \"Doctor: Do you have stomach ache?\", and the patient responded, \"Patient: No, I don't think that's been an issue.\" The doctor has now posed another question: \"Doctor: Do bright lights bpother you?\". The system indicates that the \"Patient is typing\" a response. Below the chat, there's an input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button. A \"Diagnosis not allowed yet\" message is also visible. The right panel, \"Patient Memory,\" lists \"nausea\" as a checked item. The \"Diagnostic Confidence\" section shows updated probabilities for potential diagnoses: \"Flu - 0%\", \"Food Poisoning - 44%\", and \"Migraine - 56%\". A browser notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen.\" At 00:09:02, the \"Agentic Patient Simulator\" web application is displayed, running on `localhost:5500`. Ashmit Verma, from 00:09:02 to 00:09:06, states, \"So what we'll be doing next is asking,\" and then from 00:09:06 to 00:09:07, he indicates the next question will be, \"do you have headaches?\" He further mentions, from 00:09:11 to 00:09:15, \"So these three symptoms.\" The AI Evaluator, from 00:09:15 to 00:09:21, asks Ashmit to \"elaborate on what you mean by going specifically in the context of nausea and light sensitivity?\" The application's interface is structured into three columns. The left \"Sessions\" panel shows \"+ New Patient\" and the active \"Patient 1 - 99862010.\" The central \"Agentic Patient Simulator\" panel displays the ongoing diagnostic chat. The doctor has asked, \"Doctor: Do bright lights bother you?\", and the patient has responded, \"Patient: Yes, bright light makes it worse.\" This interaction follows the patient's previous statement, \"Patient: No, I don't think that's been an issue.\" An input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are present below the chat. The right-hand panel shows \"Patient Memory\" with \"nausea\" and \"light_sensitivity\" listed. The \"Diagnostic Confidence\" section now shows \"Flu - 0%,\" \"Food Poisoning - 17%,\" and \"Migraine - 83%,\"",
    "created_at": "2026-02-02T05:41:15.681864+00:00"
  },
  {
    "chunk_id": 95,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 55,
    "text": "issue.\" An input field labeled \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button are present below the chat. The right-hand panel shows \"Patient Memory\" with \"nausea\" and \"light_sensitivity\" listed. The \"Diagnostic Confidence\" section now shows \"Flu - 0%,\" \"Food Poisoning - 17%,\" and \"Migraine - 83%,\" reflecting the updated patient symptoms. At 00:09:20, the \"Agentic Patient Simulator\" web application is shown, running on `localhost:5500`. The AI Evaluator, from 00:09:15 to 00:09:21, asks for clarification, stating, \"Can you elaborate on what you mean by going specifically in the context of nausea and light sensitivity?\" The application's interface is divided into three main panels. The left \"Sessions\" panel lists \"+ New Patient\" and the currently active \"Patient 1 - 99862010\". The central \"Agentic Patient Simulator\" panel displays the ongoing diagnostic conversation. The doctor previously asked, \"Doctor: Do bright lights bpother you?\", to which the patient replied, \"Patient: Yes, bright light makes it worse.\" Now, the doctor has asked, \"Doctor: Do you have headaches?\", and the system indicates that the \"Patient is typing\" a response. An input field \"Ask a question (e.g. Do you have fever?)\" with a \"Send\" button is available below the chat. The right panel shows \"Patient Memory\" with confirmed symptoms: \"\u2713 nausea\" and \"\u2713 light_sensitivity\". The \"Diagnostic Confidence\" section shows \"Flu - 0%\", \"Food Poisoning - 17%\", and \"Migraine - 83%\", indicating a high probability for Migraine. The message \"Diagnosis not allowed yet\" is also visible. The bottom of the screen displays system information including weather (\"18\u00b0C Mostly sunny\"), a \"Search\" bar, and screen sharing controls (\"projects.icapp.co.in is sharing your screen.\", \"Stop sharing\", \"Hide\", \"5:05 PM\", \"12/27/2025\"). At 00:09:21, the \"Agentic Patient Simulator\" web application is shown, running on `localhost:5500`. The AI Evaluator, from 00:09:15 to 00:09:21, asked Ashmit Verma to elaborate on what he meant by going specifically in the context of nausea and light sensitivity. Ashmit Verma, from 00:09:22 to 00:09:29, explained that in the Flask backend, the symptoms array was initially empty. He continued, from 00:09:30 to 00:09:37, that the 'land graph' was asking for symptoms, and as symptoms were revealed, they",
    "created_at": "2026-02-02T05:41:15.681865+00:00"
  },
  {
    "chunk_id": 96,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 56,
    "text": "meant by going specifically in the context of nausea and light sensitivity. Ashmit Verma, from 00:09:22 to 00:09:29, explained that in the Flask backend, the symptoms array was initially empty. He continued, from 00:09:30 to 00:09:37, that the 'land graph' was asking for symptoms, and as symptoms were revealed, they were being sent to the Flask backend, as stated from 00:09:37 to 00:09:43. From 00:09:45 to 00:09:55, Ashmit clarified that this process transitions the state for the particular session ID of the patient, which is 998620F0. He further explained, from 00:09:55 to 00:10:04, that if a new session or patient were created, it would have a different session ID, and the 'land graph' would connect to that different session ID, as mentioned from 00:10:05 to 00:10:09. Finally, from 00:10:09 to 00:10:13, Ashmit stated that they are currently doing this and he will proceed with the diagnosis. The application's interface is divided into three panels: \"Sessions,\" the main chat area, and \"Patient Memory/Diagnostic Confidence.\" The left \"Sessions\" panel displays \"+ New Patient\" and \"Patient 1 - 99862010\" as the active patient. The central panel shows the ongoing chat where the patient has confirmed \"Yes, bright light makes it worse\" in response to a doctor's question about bright lights. The doctor then asked, \"Doctor: Do you have headaches?\", and the patient responded, \"Patient: Yes, I've been having headaches.\" Crucially, the system now indicates \"Diagnosis can now be attempted\" and provides an input prompt: \"Type diagnosis: flu / migraine / food_poisoning\" with a \"Send\" button. The right panel's \"Patient Memory\" section has updated to include \"nausea,\" \"light_sensitivity,\" and \"headache.\" The \"Diagnostic Confidence\" section shows updated probabilities: \"Flu - 13%,\" \"Food Poisoning - 10%,\" and \"Migraine - 77%.\" At 00:10:10, the \"Agentic Patient Simulator\" web application is shown, running on `localhost:5500`. The interface has transitioned to a new patient session, displaying \"Patient 2 - 33c93325\" in the central interaction panel. The left \"Sessions\" column lists \"Patient 1\" and \"Patient 2,\" with \"Patient 2\" now active, along with a \"+ New Patient\" option. The central panel explicitly states \"Diagnosis not allowed yet,\" indicating that further",
    "created_at": "2026-02-02T05:41:15.681866+00:00"
  },
  {
    "chunk_id": 97,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 57,
    "text": "transitioned to a new patient session, displaying \"Patient 2 - 33c93325\" in the central interaction panel. The left \"Sessions\" column lists \"Patient 1\" and \"Patient 2,\" with \"Patient 2\" now active, along with a \"+ New Patient\" option. The central panel explicitly states \"Diagnosis not allowed yet,\" indicating that further questions are needed before a diagnosis can be attempted. An input field is available to \"Ask a question (e.g. Do you have fever?)\" with a \"Send\" button. The right column contains \"Patient Memory\" and \"Diagnostic Confidence\" sections, which are currently empty of specific diagnostic percentages. A screen sharing notification from \"projects.icapp.co.in\" is visible at the bottom. Ashmit Verma, from 00:10:09 to 00:10:13, states, \"But now right now, we are doing this. So what I will be doing is diagnosis,\" indicating a shift in focus to the diagnostic process. He then explicitly mentions \"migraine\" at 00:10:17, and from 00:10:20 to 00:10:23, he confirms, \"Yeah. So diagnosis migraine is good.\" This suggests he is either demonstrating the outcome of a diagnosis or is about to initiate a diagnosis for migraine, despite the on-screen message for \"Patient 2\" currently saying \"Diagnosis not allowed yet.\" At 00:10:22, the \"Agentic Patient Simulator\" web application is displayed, running on `localhost:5500`. Ashmit Verma, from 00:10:20 to 00:10:23, comments that the diagnosis of migraine is good. He further explains, from 00:10:23 to 00:10:29, that he will proceed by showcasing successful test cases, followed by error test cases. He clarifies, from 00:10:30 to 00:10:33, that these error cases would demonstrate scenarios where the diagnosis performed is incorrect. The application's interface is divided into three panels. The left \"Sessions\" panel shows \"Patient 1\" and \"Patient 2,\" with \"Patient 1\" being the active session, identified as \"Patient 1 - 99862010\" in the central panel. The central \"Agentic Patient Simulator\" panel displays the ongoing patient-doctor chat. The patient has confirmed symptoms, stating, \"Patient: Yes, bright light makes it worse,\" and \"Patient: Yes, I've been having headaches,\" in response to the doctor's questions, including \"Doctor: Do you have headaches?\". The system now indicates \"Diagnosis can now be attempted\" and provides an input field",
    "created_at": "2026-02-02T05:41:15.681869+00:00"
  },
  {
    "chunk_id": 98,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 58,
    "text": "the ongoing patient-doctor chat. The patient has confirmed symptoms, stating, \"Patient: Yes, bright light makes it worse,\" and \"Patient: Yes, I've been having headaches,\" in response to the doctor's questions, including \"Doctor: Do you have headaches?\". The system now indicates \"Diagnosis can now be attempted\" and provides an input field to \"Type diagnosis: flu / migraine / food_poisoning\" with a \"Send\" button. The right panel shows \"Patient Memory\" containing \"nausea,\" \"light_sensitivity,\" and \"headache.\" The \"Diagnostic Confidence\" section presents the likelihood of various conditions: \"Flu - 13%,\" \"Food Poisoning - 10%,\" and \"Migraine - 77%,\" with Migraine being the highest confidence diagnosis. At 00:10:32, the \"Agentic Patient Simulator\" web application is visible, running on `localhost:5500`. Ashmit Verma, from 00:10:30 to 00:10:33, clarifies that the error test cases he plans to showcase are intended to demonstrate situations where the diagnosis he has made is incorrect or \"bad.\" The application's interface features a \"Sessions\" panel on the left, listing \"+ New Patient,\" \"Patient 1,\" and \"Patient 2.\" The central \"Agentic Patient Simulator\" panel shows the active session for \"Patient 1 - 99962010.\" The chat log indicates a progression in the diagnostic process, with the patient confirming symptoms like light sensitivity (\"Patient: Yes, bright light makes it worse.\") and headaches (\"Patient: Yes, I've been having headaches.\"). The doctor has now explicitly entered a diagnosis: \"Doctor: diagnosismigraine.\" An input field is present at the bottom, allowing the user to \"type diagnosis: flu / migraine / food_poisoning\" and a \"Send\" button. The right-hand \"Patient Memory\" panel confirms the presence of \"\u2713 nausea,\" \"\u2713 light_sensitivity,\" and \"\u2713 headache.\" The \"Diagnostic Confidence\" section provides a breakdown of probabilities: \"Migraine - 77%,\" \"Flu - 13%,\" and \"Food Poisoning - 10%.\" A screen sharing notification from \"projects.icapp.co.in\" is also visible. At 00:10:33, the \"Agentic Patient Simulator\" web application is displayed, running on `localhost:5500`. Ashmit Verma, from 00:10:30 to 00:10:33, explains that he intends to demonstrate scenarios where the diagnosis he has performed might be incorrect or \"bad.\" He then transitions to discussing \"Treatment\" from 00:10:35 to 00:10:36. From 00:10:42 to 00:10:50, Ashmit Verma elaborates on different treatments, specifically mentioning that",
    "created_at": "2026-02-02T05:41:15.681870+00:00"
  },
  {
    "chunk_id": 99,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 59,
    "text": "running on `localhost:5500`. Ashmit Verma, from 00:10:30 to 00:10:33, explains that he intends to demonstrate scenarios where the diagnosis he has performed might be incorrect or \"bad.\" He then transitions to discussing \"Treatment\" from 00:10:35 to 00:10:36. From 00:10:42 to 00:10:50, Ashmit Verma elaborates on different treatments, specifically mentioning that for migraine, there are three treatments: dark room, painkillers, and rest. He continues, from 00:10:50 to 00:10:52, to list ORS and hydrogen diseases as treatments for food poisoning. The application's interface shows the active session for \"Patient 1 - 99962010\" in the central \"Agentic Patient Simulator\" panel. The chat history includes the patient stating, \"Patient: Yes, I've been having headaches,\" and the doctor's diagnosis, \"Doctor: diagnosis:migraine.\" A significant update is the display of a \"System: Verdict: CORRECT \u2014 Diagnosis matches and sufficient evidence was gathered,\" indicating a successful diagnosis. Following this, the message \"Diagnosis completed \u2014 prescribe treatment\" appears, prompting the next step in patient care. The \"Patient Memory\" panel on the right lists the symptoms \"nausea,\" \"light_sensitivity,\" and \"headache.\" The \"Diagnostic Confidence\" section provides probability scores: \"Flu - 13%,\" \"Food Poisoning - 10%,\" and \"Migraine - 77%,\" with Migraine being the most probable. An input field at the bottom of the central panel allows the user to \"Type diagnosis: flu / migraine / food_poisoning\" and send it. At 00:10:54, the screen displays a Visual Studio Code interface, presenting the `diseases.py` file from the \"agentic-patient-simulator\" project. The AI Evaluator, from 00:11:00 to 00:11:02, confirms, \"I'm following your presentation,\" and then at 00:11:03, encourages the presenter to \"Please continue.\" The `diseases.py` file, located in the `data` directory, contains a JSON-like configuration defining various diseases and their associated symptoms. The visible code snippets include definitions for a general \"diseases\" entry (with \"common_symptoms\": [\"fever\", \"cough\", \"sore throat\"] and \"optional_symptoms\": [\"fatigue\", \"body pain\", \"headache\", \"rash\"], requiring \"min_symptoms\": 1), \"migraine\" (\"common_symptoms\": [\"headache\"], \"optional_symptoms\": [\"light sensitivity\", \"nausea\", \"unilateral headache\", \"throbbing pain\"], \"min_symptoms\": 1), and \"food_poisoning\" (\"common_symptoms\": [\"nausea\", \"vomiting\", \"diarrhea\"], \"optional_symptoms\": [\"stomach cramps\", \"fever\", \"chills\"], \"min_symptoms\": 2). The file explorer on the left panel shows the project directory structure, with `diseases.py` highlighted. Other on-screen elements include a",
    "created_at": "2026-02-02T05:41:15.681871+00:00"
  },
  {
    "chunk_id": 100,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 60,
    "text": "\"rash\"], requiring \"min_symptoms\": 1), \"migraine\" (\"common_symptoms\": [\"headache\"], \"optional_symptoms\": [\"light sensitivity\", \"nausea\", \"unilateral headache\", \"throbbing pain\"], \"min_symptoms\": 1), and \"food_poisoning\" (\"common_symptoms\": [\"nausea\", \"vomiting\", \"diarrhea\"], \"optional_symptoms\": [\"stomach cramps\", \"fever\", \"chills\"], \"min_symptoms\": 2). The file explorer on the left panel shows the project directory structure, with `diseases.py` highlighted. Other on-screen elements include a \"project.lucapp.com is sharing your screen\" notification, a prompt to install a Python extension, and system information like \"5:07 PM\", \"12/27/2025\", and \"18\u00b0C Mostly sunny\". At 00:11:22, the screen displays the \"Agentic Patient Simulator\" web application, which is running on `localhost:5500`. Ashmit Verma, from 00:11:10 to 00:11:22, explains that once the treatment for the current patient (Patient 1) is completed, that specific session is stopped. He then indicates the intention to proceed to \"patient two\" to ask further questions, suggesting a continuation of the demonstration with a new patient scenario. The application's user interface is clearly structured into three distinct panels. The left-hand \"Sessions\" panel provides navigation options including \"+ New Patient,\" \"Patient 1,\" and \"Patient 2.\" The central and most prominent panel, titled \"Agentic Patient Simulator,\" showcases the ongoing chat interaction for \"Patient 1 99862010.\" The chat log details the progression of the simulation, showing that the \"Diagnosis completed \u2014 prescribe treatment\" stage has been reached. The doctor's input, \"Doctor: diagnosismigraine,\" is followed by the system's confirmation: \"System: Verdict: CORRECT \u2014 Diagnosis matches and sufficient evidence was gathered.\" Subsequently, the doctor entered a treatment: \"Doctor: treatmentrest in dark.\" An interactive input field is present at the bottom of this central panel, prompting the user to \"Type diagnosis: flu / migraine / food_poisoning,\" alongside a \"Send\" button. The right-hand panel provides crucial diagnostic information, starting with \"Patient Memory,\" which lists observed symptoms such as \"nausea,\" \"light_sensitivity,\" and \"headache.\" Below this, the \"Diagnostic Confidence\" section quantifies the system's certainty for various diseases: \"Flu - 13%,\" \"Food Poisoning - 10%,\" and a high confidence of \"Migraine - 77%.\" Other visible elements on the screen include various project-related text like \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"ICAPP Internships,\" and \"A School,\" as well as a \"Patient is typing\" indicator and a notification that",
    "created_at": "2026-02-02T05:41:15.681872+00:00"
  },
  {
    "chunk_id": 101,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 61,
    "text": "\"Flu - 13%,\" \"Food Poisoning - 10%,\" and a high confidence of \"Migraine - 77%.\" Other visible elements on the screen include various project-related text like \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"ICAPP Internships,\" and \"A School,\" as well as a \"Patient is typing\" indicator and a notification that \"projects.icapp.co.in is sharing your screen.\" At 00:11:23, the \"Agentic Patient Simulator\" web application is visible, running on `localhost:5500`. Ashmit Verma, from 00:11:23 to 00:11:25, makes a comment about something At 00:11:33, the \"Agentic Patient Simulator\" web application is displayed, running on `localhost:5500`. Ashmit Verma explains from 00:11:30 to 00:11:33 that he has implemented a random assignment mechanism within his `script.js` file. He elaborates from 00:11:33 to 00:11:39 that this functionality ensures each patient will be assigned a unique disease. The AI Evaluator, from 00:11:47 to 00:11:53, confirms understanding by asking if it's a \"random disease\" and not just a \"different\" one, referencing Ashmit's mention of assigning a random disease via `script.js`. The application's interface shows the \"Sessions\" panel on the left, with \"Patient 2\" currently selected. The central panel displays \"Patient 2 - 93699320\" and the status \"Diagnosis not allowed yet,\" along with an input field \"Ask a question (eg. Do you have fever?)\" and a \"Send\" button, indicating the start of a new patient interaction. The right panel contains \"Patient Memory\" and \"Diagnostic Confidence\" sections. A browser notification \"projects.icapp.co.in is sharing your screen.\" is visible at the bottom. At 00:12:19, the \"Agentic Patient Simulator\" web application is actively displayed, showing an ongoing patient interaction for \"Patient 2 - 33c93325\". Ashmit Verma explains between 00:12:18 and 00:12:20 that the symptoms being discussed were indicative of flu, specifically mentioning fever and cough from 00:12:21 to 00:12:22. The application's central chat panel reflects this, showing a dialogue where the Doctor has inquired about \"fever\" and \"headaches,\" to which the Patient has confirmed experiencing both. The right-hand \"Patient Memory\" panel has updated to include \"\u2713 fever\" and \"\u2713 headache,\" while the \"Diagnostic Confidence\" section now displays probabilities: \"Flu - 60%\", \"Food Poisoning - 0%\", and \"Migraine - 40%\". This indicates the system is",
    "created_at": "2026-02-02T05:41:15.681873+00:00"
  },
  {
    "chunk_id": 102,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 62,
    "text": "about \"fever\" and \"headaches,\" to which the Patient has confirmed experiencing both. The right-hand \"Patient Memory\" panel has updated to include \"\u2713 fever\" and \"\u2713 headache,\" while the \"Diagnostic Confidence\" section now displays probabilities: \"Flu - 60%\", \"Food Poisoning - 0%\", and \"Migraine - 40%\". This indicates the system is processing the patient's responses to update its diagnostic assessment. The interface also includes an input field for the Doctor to \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button, facilitating further interaction. The status \"Diagnosis not allowed yet\" is still present, suggesting the diagnostic process is ongoing. At 00:12:25, the \"Agentic Patient Simulator\" web application is shown, operating on `localhost:5500`. No speakers were detected, and no utterances were assigned to this keyframe. The application's user interface is clearly divided into three main sections. On the left, the \"Sessions\" panel allows for navigation between patients, displaying \"+ New Patient\", \"Patient 1\", and \"Patient 2\". The central panel is dedicated to the ongoing interaction with \"Patient 2\", identified by the ID \"33c93325\". The status \"Diagnosis not allowed yet\" is displayed, indicating the diagnostic process is still underway. The chat conversation between the Doctor and Patient 2 has progressed, with the Patient confirming \"Yes, I've had a fever.\" and \"Yes, I've been having headaches.\" in response to the Doctor's questions. The Doctor has now posed a new question: \"Do you have food poisoning?\". The interface indicates \"Patient is typing\", suggesting Patient 2 is formulating a response. An input field labeled \"Ask a question (e.g. Do you have fever?)\" along with a \"Send\" button is available for the Doctor to continue the interaction. On the right side, the \"Patient Memory\" panel has recorded \"\u2713 fever\" and \"\u2713 headache\", reflecting the patient's confirmed symptoms. Below this, the \"Diagnostic Confidence\" panel provides a probabilistic assessment: \"Flu - 60%\", \"Food Poisoning - 0%\", and \"Migraine - 40%\", showing how the system's diagnostic probabilities are evolving with new information. A system notification at the bottom of the screen confirms that \"projects.icapp.co.in is sharing your screen,\" with options to \"Stop sharing\" or \"Hide\" the notification. Additional",
    "created_at": "2026-02-02T05:41:15.681874+00:00"
  },
  {
    "chunk_id": 103,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 63,
    "text": "probabilistic assessment: \"Flu - 60%\", \"Food Poisoning - 0%\", and \"Migraine - 40%\", showing how the system's diagnostic probabilities are evolving with new information. A system notification at the bottom of the screen confirms that \"projects.icapp.co.in is sharing your screen,\" with options to \"Stop sharing\" or \"Hide\" the notification. Additional minor UI elements like weather information (\"18\u00b0C\", \"Mostly sunny\"), a search bar, and the current time and date (\"5:08 PM\", \"12/27/2025\") are also present. At 00:12:26, the \"Agentic Patient Simulator\" web application is displayed, showing an ongoing patient interaction. Ashmit Verma states between 00:12:33 and 00:12:36 that based on the current information, he can now diagnose the patient with flu. The application's interface is divided into three main sections: \"Sessions\" on the left, the main chat interface in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" on the right. The central panel focuses on \"Patient 2\" (ID: 33c93325), with the status \"Diagnosis not allowed yet\" still present. The chat log shows the Patient confirming \"Yes, I've been having headaches.\" and expressing general discomfort with \"I'm just not feeling very well. Please help me !!\" The Doctor's last question visible is \"Do you have food poisoning?\". The \"Patient Memory\" panel on the right confirms \"\u2713 fever\" and \"\u2713 headache\" as recorded symptoms. The \"Diagnostic Confidence\" section provides updated probabilities: \"Flu - 60%,\" \"Food Poisoning - 0%,\" and \"Migraine - 40%.\" An input field at the bottom allows the user to \"Ask a question (e.g. Do you have fever?)\" and \"Send\" it. Browser-related elements like \"Projects - Canva,\" \"Minimalist Clean Gears Green,\" \"ICAPP Internships,\" and \"A School\" are also visible on the screen. At 00:12:34, the screen transitions to a code view, displaying a JSON-like structure that defines symptoms for a medical diagnosis. Ashmit Verma states between 00:12:33 and 00:12:36, \"So now I can diagnose it with flu.\" The code on screen details \"required_symptoms\" as \"cough\", \"fever\", and \"fatigue\", and \"optional_symptoms\" as \"headache\", \"light_sensitivity\", \"nausea\", \"vomiting\", and \"stiff_neck\". This code likely represents the diagnostic criteria or a symptom profile for a specific illness, possibly flu, which aligns with Ashmit's utterance about being",
    "created_at": "2026-02-02T05:41:15.681875+00:00"
  },
  {
    "chunk_id": 104,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 64,
    "text": "I can diagnose it with flu.\" The code on screen details \"required_symptoms\" as \"cough\", \"fever\", and \"fatigue\", and \"optional_symptoms\" as \"headache\", \"light_sensitivity\", \"nausea\", \"vomiting\", and \"stiff_neck\". This code likely represents the diagnostic criteria or a symptom profile for a specific illness, possibly flu, which aligns with Ashmit's utterance about being able to diagnose flu. The presence of these symptoms in the code suggests how the patient simulator determines a diagnosis. At 00:12:38, the screen displays a Visual Studio Code editor, showing the `symptoms.py` file, which is part of the `agentic-patient-simulator` project. No speakers were detected, and no utterances were assigned to this keyframe. The `symptoms.py` file contains JSON-like data structures that define various medical conditions and their associated symptoms. Specifically, three conditions are detailed: 1. \"symptoms\": This entry defines `required_symptoms` as \"fever\" and \"cough\", and `optional_symptoms` as \"fatigue\", \"body ache\", and \"headache\". It also specifies `min_questions` as 0. 2. \"migraine\": This condition requires \"headache\" and has `optional_symptoms` including \"light sensitivity\", \"nausea\", \"unilateral headache\", and \"visual disturbances\", with `min_questions` set to 0. 3. \"food_poisoning\": This definition includes \"nausea\" and \"vomiting\" as `required_symptoms`, and \"stomach pain\" and \"diarrhea\" as `optional_symptoms`, also with `min_questions` as 0. The file explorer on the left shows the project structure, with `symptoms.py` located under `backend/data`. The VS Code interface also displays standard menu options (\"File Edit Selection View Go Run Terminal Help\"), a terminal panel (\"Problems Output Debug Console Terminal\"), and system notifications, including a prompt to install the Python extension and a screen sharing notification (\"projects.icapp.co.in is sharing your screen.\"). The status bar indicates the current line and column (Ln 19, Col 18), encoding, and language. The system time is 5:08 PM on 12/27/2025. At 00:12:39, the screen transitions to a live demonstration of the \"Agentic Patient Simulator\" web application, running on `localhost:5500`. The AI Evaluator explains from 00:12:39 to 00:12:50, \"And now I will be showcasing what if my diagnosis is wrong. Like, if my diagnosis is your question. With my Please continue. And I'll do my best to help you understand the symptoms.\" This utterance sets the context for the demo, indicating a scenario",
    "created_at": "2026-02-02T05:41:15.681876+00:00"
  },
  {
    "chunk_id": 105,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 65,
    "text": "explains from 00:12:39 to 00:12:50, \"And now I will be showcasing what if my diagnosis is wrong. Like, if my diagnosis is your question. With my Please continue. And I'll do my best to help you understand the symptoms.\" This utterance sets the context for the demo, indicating a scenario where the system is guiding the doctor to gather more information when a diagnosis is uncertain or potentially incorrect. The application's UI is clearly visible, structured with \"Sessions\" on the left, the main \"Agentic Patient Simulator\" chat in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" on the right. The central chat panel shows an interaction with \"Patient 2\" (ID: 33c93325). The patient has stated, \"Yes, I've been having headaches.\" and expressed general distress with, \"I'm just not feeling very well.Please help me!!\" The Doctor has responded by asking, \"Do you have food_poisoning?\". The system currently displays \"Diagnosis not allowed yet,\" suggesting that more information is needed before a definitive diagnosis can be made. The \"Patient Memory\" panel confirms that \"fever\" and \"headache\" have been recorded as symptoms. The \"Diagnostic Confidence\" section provides a probabilistic assessment: \"Flu - 60%\", \"Food Poisoning - 0%\", and \"Migraine - 40%\". This visual information directly supports the AI Evaluator's point about a potentially 'wrong' diagnosis or the need for further questioning to refine the diagnostic confidence. At 00:12:44, the 'Agentic Patient Simulator' web application is actively demonstrated, running on `localhost:5500`. The AI Evaluator, speaking from 00:12:39 to 00:12:50, explains, 'And now I will be showcasing what if my diagnosis is wrong. Like, if my diagnosis is your question. With my Please continue. And I'll do my best to help you understand the symptoms.' This sets the context for the ongoing patient interaction shown on screen, where the system is guiding the diagnostic process when a diagnosis is uncertain. The UI is structured with 'Sessions' on the left, showing 'Patient 1' and 'Patient 2', with 'Patient 2' (ID: 33c93325) currently active. The central chat panel displays the conversation: the patient has reported 'headaches.' and expressed general discomfort, saying 'I'm just not feeling very well.Please help",
    "created_at": "2026-02-02T05:41:15.681877+00:00"
  },
  {
    "chunk_id": 106,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 66,
    "text": "a diagnosis is uncertain. The UI is structured with 'Sessions' on the left, showing 'Patient 1' and 'Patient 2', with 'Patient 2' (ID: 33c93325) currently active. The central chat panel displays the conversation: the patient has reported 'headaches.' and expressed general discomfort, saying 'I'm just not feeling very well.Please help me !!'. The Doctor has asked two questions: 'Do you have food_poisoning?' and 'Do you have cough?'. The prompt 'Ask a question (e.g. Do you have fever?)' and 'Send' button are visible, along with 'Patient is typing', indicating an awaiting response. The right-hand panel shows 'Patient Memory' with 'fever' and 'headache' recorded, and 'Diagnostic Confidence' indicating 'Flu - 60%', 'Food Poisoning - 0%', and 'Migraine - 40%'. The 'Diagnosis not allowed yet' status for Patient 2 remains visible, reinforcing the need for further questioning as highlighted by the AI Evaluator. At 00:12:45, the 'Agentic Patient Simulator' web application is actively being demonstrated, running on `localhost:5500`. The AI Evaluator, speaking from 00:12:39 to 00:12:50, sets the stage by explaining, \"And now I will be showcasing what if my diagnosis is wrong. Like, if my diagnosis is your question. With my Please continue. And I'll do my best to help you understand the symptoms.\" Ashmit Verma then continues from 00:12:51 to 00:12:58, asking, \"Like, if I do a diagnosis as, something else. So what will happen? Like, it will\" and then from 00:12:59 to 00:13:06, clarifies, \"show that verdict is verdict is incorrect. So I will now show the this is the error handling that I have done in my project.\" This indicates the current demo focuses on the system's response to an incorrect diagnosis or how it guides the user to a correct one. The UI shows 'Patient 2 33c93325' as the active patient. The central chat panel displays the ongoing interaction: the Doctor has asked, 'Do you have food_poisoning?' and 'Do you have cough?'. The Patient has responded with 'I'm just not feeling very well.Please help me!!' and confirmed, 'Yes, I've been coughing.'. Crucially, the system now displays 'Diagnosis can now be attempted' and prompts the user with 'Type diagnosis:",
    "created_at": "2026-02-02T05:41:15.681878+00:00"
  },
  {
    "chunk_id": 107,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 67,
    "text": "interaction: the Doctor has asked, 'Do you have food_poisoning?' and 'Do you have cough?'. The Patient has responded with 'I'm just not feeling very well.Please help me!!' and confirmed, 'Yes, I've been coughing.'. Crucially, the system now displays 'Diagnosis can now be attempted' and prompts the user with 'Type diagnosis: flu / migraine / food_poisoning' and a 'Send' button. The 'Patient Memory' panel on the right has updated to include '\u2713 fever', '\u2713 headache', and '\u2713 cough'. Correspondingly, the 'Diagnostic Confidence' percentages have shifted to 'Flu - 71%', 'Food Poisoning - 0%', and 'Migraine - 29%', reflecting the new symptom information. At 00:13:02, the \"Agentic Patient Simulator\" web application is being demonstrated, running on `localhost:5500`. Ashmit Verma, speaking from 00:12:59 to 00:13:06, explains that the system is designed to \"show that verdict is verdict is incorrect\" and that he is now demonstrating \"the error handling that I have done in my project.\" This aligns with the on-screen content, which shows a scenario where the 'Doctor' has entered \"diagnosis:migraine\" in the chat, but the 'Diagnostic Confidence' panel on the right indicates a Flu confidence of 71% versus Migraine at 29%. The patient's symptoms listed in 'Patient Memory' include '\u2713 fever', '\u2713 headache', and '\u2713 cough'. The chat history shows the 'Patient' initially stating, \"I'm just not feeling very well.Please help me !!\", followed by the 'Doctor' asking, \"Do you have cough?\", and the 'Patient' confirming, \"Yes, I've been coughing.\" The presence of \"Patient is typing\" suggests the patient is about to respond to the doctor's diagnosis or provide further information, potentially challenging the 'migraine' diagnosis given the higher confidence for 'Flu'. The UI also provides a prompt \"Type diagnosis: flu / migraine / food_poisoning\" for the user. At 00:13:03, the \"Agentic Patient Simulator\" web application is actively being demonstrated. Ashmit Verma, speaking from 00:12:59 to 00:13:06, explains that the system is designed to \"show that verdict is verdict is incorrect,\" highlighting this as a demonstration of the error handling he has implemented in his project. He elaborates, from 00:13:07 to 00:13:15, on the scenario where a doctor might wrongly diagnose a",
    "created_at": "2026-02-02T05:41:15.681879+00:00"
  },
  {
    "chunk_id": 108,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 68,
    "text": "speaking from 00:12:59 to 00:13:06, explains that the system is designed to \"show that verdict is verdict is incorrect,\" highlighting this as a demonstration of the error handling he has implemented in his project. He elaborates, from 00:13:07 to 00:13:15, on the scenario where a doctor might wrongly diagnose a patient. The on-screen content directly illustrates this, showing a chat interaction where the 'Patient' confirms, \"Yes, I've been coughing.\" The 'Doctor' then inputs \"diagnosismigraine.\" The system immediately responds with \"System: Verdict: INCORRECT \u2014 The symptoms do not match migraine,\" which aligns with Ashmit's explanation of error handling. The 'Patient Memory' panel on the right lists the patient's symptoms as \"fever,\" \"headache,\" and \"cough.\" The 'Diagnostic Confidence' panel further supports the system's verdict by showing \"Flu - 71%,\" \"Food Poisoning - 0%,\" and \"Migraine - 29%,\" indicating that migraine is a low-confidence diagnosis given the symptoms. Below the incorrect verdict, the system prompts the user to \"Type diagnosis: flu / migraine / food_poisoning\" with a 'Send' button, guiding them towards a more accurate diagnosis. Ashmit Verma then states, from 00:13:16 to 00:13:21, \"Diagnosis flu. Now it shows that,\" likely indicating the next step in the demonstration. The AI Evaluator, from 00:13:30 to 00:13:34, acknowledges Ashmit's point, stating, \"You mentioned that the system shows what verdict is incorrect.\" At 00:13:34, the 'Agentic Patient Simulator' web application is being demonstrated, showcasing its diagnostic and error handling capabilities. The AI Evaluator, speaking from 00:13:30 to 00:13:34, noted that the system indicates when a diagnosis verdict is incorrect. Subsequently, from 00:13:34 to 00:13:42, the AI Evaluator posed a detailed question, asking for elaboration on how the system determines an incorrect diagnosis and the specific steps taken in its error handling process. On-screen, the central chat panel of the simulator illustrates this process. A previous 'Doctor' input of 'diagnosismigraine' resulted in a 'System: Verdict: INCORRECT \u2014 The symptoms do not match migraine.' Following this, the 'Doctor' has now entered 'diagosis:flu'. The system is currently in a state where 'Patient is typing', suggesting a response or further interaction is pending. The user is prompted to 'Type diagnosis:",
    "created_at": "2026-02-02T05:41:15.681880+00:00"
  },
  {
    "chunk_id": 109,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 69,
    "text": "input of 'diagnosismigraine' resulted in a 'System: Verdict: INCORRECT \u2014 The symptoms do not match migraine.' Following this, the 'Doctor' has now entered 'diagosis:flu'. The system is currently in a state where 'Patient is typing', suggesting a response or further interaction is pending. The user is prompted to 'Type diagnosis: flu / migraine / food_poisoning' and 'Send'. The 'Patient Memory' panel clearly lists the patient's symptoms as '\u2713 fever', '\u2713 headache', and '\u2713 cough', which are critical for diagnosis. The 'Diagnostic Confidence' panel provides a probabilistic breakdown: 'Flu - 71%', 'Food Poisoning - 0%', and 'Migraine - 29%', reinforcing the system's internal assessment that aligns with the 'flu' diagnosis being more probable than 'migraine' given the symptoms. At 00:13:35, the \"Agentic Patient Simulator\" web application is being demonstrated, showcasing its diagnostic and error handling capabilities. The AI Evaluator, from 00:13:34 to 00:13:42, posed a detailed question, asking for elaboration on how the system determines an incorrect diagnosis and the specific steps taken in its error handling process. This aligns with Ashmit Verma's previous explanations (from 00:12:59 to 00:13:06) about demonstrating the error handling he implemented, where the system shows an \"incorrect\" verdict for a wrong diagnosis. On-screen, the application's interface is visible, structured with \"Sessions,\" \"Patient Interaction,\" and \"Patient Memory/Diagnostic Confidence\" panels. The central \"Patient Interaction\" chat panel shows a previous system verdict: \"INCORRECT \u2014 The symptoms do not match migraine,\" indicating a failed diagnosis attempt. Subsequently, the \"Doctor\" has entered \"diagnosis:flu.\" The patient's message, \"I'm just not feeling very well.Please help me !!\", is displayed, and an input field is available for entering a diagnosis. The right-hand panel, \"Patient Memory,\" lists the patient's symptoms as \"\u2713 fever,\" \"\u2713 headache,\" and \"\u2713 cough.\" Below this, \"Diagnostic Confidence\" shows \"Flu \u2014 71%,\" \"Food Poisoning \u2014 0%,\" and \"Migraine \u2014 29%.\" Ashmit Verma began to speak at 00:13:49, stating \"Of this is,\" likely in response to the AI Evaluator's question, before the utterance concluded. No utterances were made during this keyframe at 00:13:56. The \"Agentic Patient Simulator\" web application is being demonstrated, running on `localhost:5500`. The user interface is structured into three",
    "created_at": "2026-02-02T05:41:15.681882+00:00"
  },
  {
    "chunk_id": 110,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 70,
    "text": "Verma began to speak at 00:13:49, stating \"Of this is,\" likely in response to the AI Evaluator's question, before the utterance concluded. No utterances were made during this keyframe at 00:13:56. The \"Agentic Patient Simulator\" web application is being demonstrated, running on `localhost:5500`. The user interface is structured into three main panels: \"Sessions\" on the left, the central \"Agentic Patient Simulator\" chat area, and the \"Patient Memory\" and \"Diagnostic Confidence\" panels on the right. In the central chat, the conversation shows the \"Patient\" expressing, \"I'm just not feeling very well.Please help me !!\" The \"Doctor\" has subsequently entered a diagnosis: \"diagnosis:flu\". The system indicates \"Patient is typing\", suggesting an ongoing interaction. A user prompt \"Type diagnosis: flu / migraine / food_poisoning\" with a \"Send\" button is present, guiding the user on how to input a diagnosis. The \"Patient Memory\" panel confirms the patient's symptoms as \"\u2713 fever\", \"\u2713 headache\", and \"\u2713 cough\". Correspondingly, the \"Diagnostic Confidence\" panel displays the system's confidence levels: \"Flu - 71%\", \"Food Poisoning - 0%\", and \"Migraine - 29%\". This state reflects a progression from the previous keyframe where the doctor had also entered \"diagnosis:flu\" after an incorrect diagnosis attempt for migraine. The system is now showing updated confidence levels, with Flu being the most probable diagnosis based on the listed symptoms. A browser notification \"projects.icapp.co.in is sharing your screen.\" is visible at the bottom of the window, along with a local time of \"5:10 PM\" and date \"12/27/2025\". At 00:13:57, the \"Agentic Patient Simulator\" web application is actively being demonstrated. The AI Evaluator, from 00:13:58 to 00:14:02, inquired about the session and asked Ashmit Verma to elaborate on the aspects of treatment he was referring to. Ashmit Verma, from 00:14:07 to 00:14:12, then announced his intention to demonstrate a scenario where the treatment provided is incorrect. On-screen, the application's interface is visible, structured with \"Sessions\" on the left, the main \"Agentic Patient Simulator\" chat area in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" panels on the right. The central panel shows the interaction with \"Patient 2,\" where the patient previously stated, \"I'm just not",
    "created_at": "2026-02-02T05:41:15.681883+00:00"
  },
  {
    "chunk_id": 111,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 71,
    "text": "incorrect. On-screen, the application's interface is visible, structured with \"Sessions\" on the left, the main \"Agentic Patient Simulator\" chat area in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" panels on the right. The central panel shows the interaction with \"Patient 2,\" where the patient previously stated, \"I'm just not feeling very well.Please help me !!\" Following a doctor's diagnosis of \"flu,\" the system has now provided a verdict: \"System: Verdict: CORRECT \u2014 Diagnosis matches and sufficient evidence was gathered.\" This indicates a successful diagnosis. The prompt \"Diagnosis completed - prescribe treatment\" is displayed, along with an input field \"Type diagnosis: flu / migraine / food_poisoning\" and a \"Send\" button, guiding the user to the next step. The \"Patient Memory\" panel on the right confirms the patient's symptoms as \"\u2713 fever,\" \"\u2713 headache,\" and \"\u2713 cough.\" The \"Diagnostic Confidence\" section further supports the \"flu\" diagnosis with \"Flu - 71%,\" while \"Food Poisoning\" is at \"0%\" and \"Migraine\" at \"29%.\" The current state of the demo shows a successful diagnosis, setting the stage for Ashmit's upcoming demonstration of an incorrect treatment scenario. At 00:14:12, the \"Agentic Patient Simulator\" web application is being demonstrated, showcasing the progression from diagnosis to treatment. Ashmit Verma, from 00:14:07 to 00:14:12, announced his intention to demonstrate a scenario where the treatment provided is incorrect, stating, \"Now I will be showing what if the treatment is wrong. Like, if I\". The application's interface is visible, structured with \"Sessions\" on the left, the main \"Agentic Patient Simulator\" chat area in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" panels on the right. The central panel shows the interaction with \"Patient 2,\" where the system has confirmed the \"Doctor's\" diagnosis of \"flu\" as \"CORRECT\" because \"Diagnosis matches and sufficient evidence was gathered.\" Crucially, the \"Doctor\" has now prescribed \"treatment:rest.\" The system currently indicates that the \"Patient is typing,\" implying a response to the treatment. The user is still prompted to \"Type diagnosis: flu / migraine / food_poisoning\" with a \"Send\" button, although the current context is treatment. The \"Patient Memory\" panel confirms the patient's symptoms as \"\u2713 fever,\" \"\u2713",
    "created_at": "2026-02-02T05:41:15.681884+00:00"
  },
  {
    "chunk_id": 112,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 72,
    "text": "The system currently indicates that the \"Patient is typing,\" implying a response to the treatment. The user is still prompted to \"Type diagnosis: flu / migraine / food_poisoning\" with a \"Send\" button, although the current context is treatment. The \"Patient Memory\" panel confirms the patient's symptoms as \"\u2713 fever,\" \"\u2713 headache,\" and \"\u2713 cough,\" and the \"Diagnostic Confidence\" panel displays \"Flu \u2014 71%,\" \"Food Poisoning \u2014 0%,\" and \"Migraine \u2014 29%.\" At 00:14:13, the \"Agentic Patient Simulator\" web application is being demonstrated, showing the completion of a patient interaction. The AI Evaluator, from 00:14:13 to 00:14:16, made a meta-comment about the system's state, first stating \"Evaluation\" and then confirming that the \"Evaluation has been successfully saved.\" On-screen, the application's interface is visible, structured with \"Sessions\" on the left, the main \"Agentic Patient Simulator\" chat area in the center, and \"Patient Memory\" and \"Diagnostic Confidence\" panels on the right. The central panel, focused on \"Patient 2 \u2022 33c93325,\" shows the progression from the previous keyframe where the \"Doctor\" prescribed \"treatment:rest.\" Now, the \"Patient\" has responded with \"Okay doctor, I'll follow this treatment.\" Subsequently, the \"System\" has confirmed \"Treatment accepted.\" and declared \"Session completed.\" The overall status \"Treatment completed\" is prominently displayed. The \"Patient Memory\" panel continues to list \"\u2713 fever,\" \"\u2713 headache,\" and \"\u2713 cough\" as confirmed symptoms. The \"Diagnostic Confidence\" panel shows \"Flu - 71%,\" \"Food Poisoning - 0%,\" and \"Migraine - 29%,\" reflecting the diagnostic outcome. This keyframe marks the successful conclusion of the simulated patient session. At 00:14:15, the \"Agentic Patient Simulator\" web application is being demonstrated, showing a new patient session. The AI Evaluator, from 00:14:14 to 00:14:16, confirmed that an action \"has been successfully saved.\" Subsequently, from 00:14:16 to 00:14:18, the AI Evaluator instructed the user to \"You may now disconnect,\" and concluded the interaction at 00:14:19 by stating, \"We are done here.\" On-screen, the application's interface is visible, running on `localhost:5500`. The layout consists of three main panels: the left \"Sessions\" panel lists \"Patient 1,\" \"Patient 2,\" and \"Patient 3,\" with \"Patient 3\" currently selected. The central panel, labeled \"Agentic Patient Simulator,\" displays \"Patient 3",
    "created_at": "2026-02-02T05:41:15.681884+00:00"
  },
  {
    "chunk_id": 113,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 73,
    "text": "00:14:19 by stating, \"We are done here.\" On-screen, the application's interface is visible, running on `localhost:5500`. The layout consists of three main panels: the left \"Sessions\" panel lists \"Patient 1,\" \"Patient 2,\" and \"Patient 3,\" with \"Patient 3\" currently selected. The central panel, labeled \"Agentic Patient Simulator,\" displays \"Patient 3 ba5579af\" as the active patient. A key message, \"Diagnosis not allowed yet,\" is displayed, along with a prompt to \"Ask a question (e.g. Do you have fever?)\" and a \"Send\" button, indicating the initial stage of a patient interaction where information gathering is expected. The right panel features \"Patient Memory\" and \"Diagnostic Confidence\" sections, which are currently blank. A notification at the bottom confirms screen sharing is active from \"projects.icapp.co.in.\" At 00:14:19, the \"Agentic Patient Simulator\" web application is being demonstrated. The AI Evaluator, from 00:14:18 to 00:14:19, concluded the interaction by stating, \"We are done here.\" On-screen, the application's interface is visible, running on `localhost:5500`. The layout includes a left \"Sessions\" panel with \"Patient 1,\" \"Patient 2,\" and \"Patient 3\" listed, with \"Patient 3\" (ID `ba5579af`) currently highlighted. The central panel, identified as \"Agentic Patient Simulator,\" displays the message \"Diagnosis not allowed yet.\" An input field prompts the user to \"Ask a question (e.g. Do you have fever?)\" and is accompanied by a \"Send\" button, indicating the start of a new patient interaction or a reset state. A browser pop-up, labeled \"ICAPP Internships\" and associated with `projects.icapp.co.in`, confirms that \"This tab is sharing your screen\" and shows a small preview of the simulator. It also indicates \"Memory usage: 561 MB.\" A notification at the bottom of the screen reiterates that \"projects.icapp.co.in is sharing your screen,\" offering \"Stop sharing\" and \"Hide\" options. Additional on-screen elements include \"Projects - Canva\", \"Minimalist Clean Gears Green\", \"A School\", \"Agentic Patient Simul\", \"ent Memory\", \"gnostic Confidence\", \"18\u00b0C\", \"Mostly sunny\", \"Search\", \"5:10 PM\", \"12/27/2025\". At 00:14:20, the screen presents a web application interface for \"ICAPP Internships,\" hosted at `projects.icapp.co.in`. The browser tab is titled \"Projects - Canva\" and also mentions \"Minimalist Clean Gears Green.\" The user interface includes a left-hand navigation panel with options like",
    "created_at": "2026-02-02T05:41:15.681886+00:00"
  },
  {
    "chunk_id": 114,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 74,
    "text": "\"gnostic Confidence\", \"18\u00b0C\", \"Mostly sunny\", \"Search\", \"5:10 PM\", \"12/27/2025\". At 00:14:20, the screen presents a web application interface for \"ICAPP Internships,\" hosted at `projects.icapp.co.in`. The browser tab is titled \"Projects - Canva\" and also mentions \"Minimalist Clean Gears Green.\" The user interface includes a left-hand navigation panel with options like \"Home,\" \"My Application,\" \"My Project Tasks,\" \"My Course Tasks,\" and \"Logout,\" all associated with the user \"Ashmit Verma(2024134)\" and their email \"ashmit24134@iiitd.ac.in.\" The central display area highlights a project described as \"A Multi-User Agentic Medical Interview System,\" which is noted to be built using \"Flask and Vanilla Web,\" \"LangGraph,\" and \"LangChain.\" A pop-up window is visible, showing a \"Processing...\" status and a \"Transcript\" section. Inside the transcript, a chat message reads \"best the association is completed,\" accompanied by a \"Send\" button. A system notification at the bottom of the screen indicates that \"projects.icapp.co.in is sharing your screen.\" The page footer contains links to \"Companies and Educators\" and an \"About\" section, which partially describes the ICAPP Internships, Capstone Projects, and Apprenticeship Program as designed to provide students with hands-on experience and valuable insights. There were no utterances during this keyframe. At 00:14:21, the \"Agentic Patient Simulator\" web application is displayed, running on `localhost:5500`. The interface is clearly divided into three main columns. The left column, labeled \"Sessions,\" lists available patient profiles, specifically \"Patient 1,\" \"Patient 2,\" and \"Patient 3,\" with \"Patient 3\" currently highlighted, indicating it's the active patient. The central column, which is the core simulator interface, shows \"Patient 3 ba5579ef\" and a prominent message: \"Diagnosis not allowed yet.\" Below this message, there's an interactive text input field that prompts the user to \"Ask a question (eg. Do you have fever?)\" and a \"Send\" button, suggesting the initiation of a new diagnostic interaction. The right column is dedicated to \"Patient Memory\" and \"Diagnostic Confidence,\" both of which appear to be empty in this view. A browser tab at the top indicates the application's title as \"Agentic Patient Simulator,\" alongside \"Projects - Canva\" and \"Minimalist Clean Gears Green.\" A system notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen.\"",
    "created_at": "2026-02-02T05:41:15.681886+00:00"
  },
  {
    "chunk_id": 115,
    "user_id": "project",
    "meeting_name": "project",
    "meeting_type": "live_meeting",
    "meeting_index": 0,
    "project_type": "medical",
    "chunk_index": 75,
    "text": "Memory\" and \"Diagnostic Confidence,\" both of which appear to be empty in this view. A browser tab at the top indicates the application's title as \"Agentic Patient Simulator,\" alongside \"Projects - Canva\" and \"Minimalist Clean Gears Green.\" A system notification at the bottom confirms that \"projects.icapp.co.in is sharing your screen.\" During this keyframe, from 00:14:26 to 00:14:28, Ashmit Verma asks, \"Like, what if I do now,\" possibly inquiring about the next step in the simulator. Subsequently, from 00:14:32 to 00:14:36, the AI Evaluator states, \"We are done here. Please return to your task,\" signaling the conclusion of the current task or demonstration.",
    "created_at": "2026-02-02T05:41:15.681887+00:00"
  },
  {
    "chunk_id": 116,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 0,
    "text": "[Speaker 2] I am an evaluator. I'll be evaluating your presentation based on the project and task requirements. Please begin when you're ready. Note, this session is being recorded. [Speaker 1] Hello everyone, this is a project agent in AI. So it is an AI chatbot application that simulates the patient behavior, which interacts with a user who is acting as a doctor. The message sent by the user goes to the backend through FastAPI, which acts like a bridge between the frontend and the backend. Then the message goes to the land graph, where it is classified. This intent classification helps the land chain to create a response accordingly. We also maintain patient memory so that the bot does not forget the context of the conversation. The tasks performed by the land graph, land chain, and patient memory together are known as the agent, which is created for a particular session ID. Tasks are completed with the help of an LLM accessed via API key. The reply generated by the agent goes back to the frontend through FastAPI. Technology used. The frontend is deployed on Vercel. The backend is deployed on Render. The Identity AI is built with LangChain and LangGraph. The LLM is served via Grok for fast response generation. The backend is coded in Python and the frontend is built using HTML, CSS and JavaScript. Key design choices. The message which goes to the LangGraph can be classified as an intake message, a treatment message or a closing message. Once a message is classified as closing, a conversation-ended flag becomes true, and any further input receives a session-ended response. A new session ID can be created using the reset button. [Speaker 2] You mentioned that the message goes to the LangGraph where it is classified. Could you elaborate on how this LangGraph is implemented and what features of the message it uses for intent classification? [Speaker 1] I would like to show you the code for intent classification. We use an LLM here, which is Grok, accessed through the API key. We set the temperature to zero",
    "created_at": "2026-02-02T05:41:15.682047+00:00"
  },
  {
    "chunk_id": 117,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 1,
    "text": "elaborate on how this LangGraph is implemented and what features of the message it uses for intent classification? [Speaker 1] I would like to show you the code for intent classification. We use an LLM here, which is Grok, accessed through the API key. We set the temperature to zero so that we get deterministic output. In the prompt, we define rules based on key terms. For example, intake when the user says hi or hello, treatment when treatment is recommended, and closing when the user politely ends the conversation. The message is classified and a JSON value is returned by the LLM indicating whether the message is intake, treatment, or closing. [Speaker 2] You mentioned setting the temperature to zero. Could you elaborate on why you chose this specific value and what effect it has? [Speaker 1] Temperature zero ensures deterministic output. Since intent classification only determines user intent, randomness is undesirable. This helps LangChain generate clearer responses based on intent. [Speaker 2] I'm following your presentation. Please continue. [Speaker 1] Here is another demonstration of chatbot functionality. The session ID created on the frontend goes to the backend with the user message. If that user ID already has an agent, the same agent is reused; otherwise, a new agent is created. [Speaker 2] You mentioned that the same agent is reused. Can you elaborate on how you ensure the correct agent state is maintained? [Speaker 1] We maintain a dictionary that stores session IDs as keys and agent objects as values. When a request comes, the dictionary is checked. If the session ID exists, the same agent is used, ensuring isolation between sessions. [Speaker 2] You mentioned that the object stored is the agent created for each session ID. Could you elaborate on how this agent object is structured? [Speaker 1] The agent object is created using a create_agent function, which invokes the LangGraph. The agent is activated and replies are generated through it. [Speaker 2] You mentioned that the agent is basically a patient which contains a LangGraph. Could you elaborate? [Speaker 1] The LangGraph maintains the",
    "created_at": "2026-02-02T05:41:15.682050+00:00"
  },
  {
    "chunk_id": 118,
    "user_id": "sarthak",
    "meeting_name": "ppt_eval",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "medical",
    "chunk_index": 2,
    "text": "structured? [Speaker 1] The agent object is created using a create_agent function, which invokes the LangGraph. The agent is activated and replies are generated through it. [Speaker 2] You mentioned that the agent is basically a patient which contains a LangGraph. Could you elaborate? [Speaker 1] The LangGraph maintains the control flow. It classifies intent so that LangChain can generate responses based on intent. The graph structure ensures the flow between intake, treatment, and closing states. [Speaker 2] I'm following your presentation. Please continue. So here is a demonstration of our application. [Speaker 1] I can start the conversation by asking about symptoms. The bot responds accordingly. I can ask follow-up questions, and responses are generated based on context. The session ID is created after the first message and reused for subsequent messages. I can give treatment intent, accept treatment, and then end the conversation. Once the conversation ends, further messages return a consultation-ended response. [Speaker 2] Evaluation has been successfully saved. You may now disconnect.",
    "created_at": "2026-02-02T05:41:15.682052+00:00"
  },
  {
    "chunk_id": 119,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 1,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] So, this is my vision glass. This is the range parameter that how many centimeters or how many units it can do and this is for example, you cannot see 60 degrees. Like we can only see what is ahead of us. That in radius like that it is there. And this is how much the object is visible. So, this is a vision glass basically. [Speaker 2] So, this is what we observed. Try to keep it simple, keep everything simple, make sure the system is not forgotten. [Speaker 1] We are adding rooms with width and height. A person can only see up to 5 meters. If an object is at 6 meters, it should not be visible. This is implemented in demo.py. We created a world object and added walls. If a wall is in the field of view, objects behind it are not visible. Entities have types and locations. There are multiple agents, and currently we are printing their visible objects. [Speaker 1] In Jericho, each node represents a room. We need to define which objects exist in each room and specify coordinates for vision to work. [Speaker 2] When Jericho returns a message, it will list visible objects. Try to keep the first cut simple. First configure Jericho with your system without extra complexity. Show Jericho working first, then extend it later.",
    "created_at": "2026-02-02T05:41:15.682154+00:00"
  },
  {
    "chunk_id": 120,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] I've shared a couple of links with you just now. First, there's this paper, Generative Agents, right? You've seen that, right? So what they did is they put lots of relevant agents in a word and let them walk around and do things and make things happen, right? Read it carefully, how they've done that. You read it? The second one is this one, which you know about text-based games, right? Adventure games and Zork and stuff like that, right? So this put LLM inside those games and said, can you play them? So read this paper, right? And this paper has a link to their code, which is not, I've just tried to get it unlocked from the IIIT network, but it's right now some blockages there, so I'm telling you on my phone, right? I said this is not a game site, this is actually a research paper site, right? So just because it's called .ai or something, it's, but basically they have various games and they have the code for that, et cetera. This uses an environment called Jericho, right? Which allows basically, if you look at the quick start, you basically can, you have to do all this stuff and then you get the games, right? And then you basically create the environment, right? So you have to get this Z5 environment from whatever this Jericho game suite, right? You have to put this somewhere and you do, right? And you just see if you can use this. Basically, this lets you play the game in Python. So this TextQuest actually uses this environment to do play games, right? So what we want to do is first, all these games are single user games, right? So I'll start with that, that single user games are a problem. But first goal is let's get an LLM agent to play a game, right? The game environment will have a description of what's happening, et cetera, et cetera, right? And what we need to do is once the LLM agent goes and plays the game and maybe",
    "created_at": "2026-02-02T05:41:15.682320+00:00"
  },
  {
    "chunk_id": 121,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "are a problem. But first goal is let's get an LLM agent to play a game, right? The game environment will have a description of what's happening, et cetera, et cetera, right? And what we need to do is once the LLM agent goes and plays the game and maybe you should make it talks to itself, maybe it talks to somebody else, et cetera, et cetera. So let's say it plays the game, one. Second, let's see. There's this. I basically generated, I used the VO3 video generator of Google, right? So you basically, you have to create it. So we'll come to the keys issue later, right? So you have first person view of talking to a customer and trying to sell them a washing machine, right? Or other case, I had various different. So basically, we generated some videos like this, right? [Speaker 2] So this is a great deal. [Speaker 1] So it generates a video. It's pretty costly, right? So each of these things will probably end up eight seconds But you have, have you created an account in Germany? [Speaker 2] Are you exhausted? [Speaker 1] Are you exhausted 300 credits? When did you create? Last month. You still have some time. You have those 300 credits free, right? For example, I have over here my Germany account. If you associate a billing, look at the credits. So when you create an account and make it a paid account, right? You get 26,000 rupees. It lasts for three months, but that's okay. Enough for this course, right? So you have two months of your account left, right? And then you can use my account after that, right? So I hope you won't exhaust, but your time will exhaust. But then after that, you can use mine, right? So you can be pretty, 26,000 means pretty much like 2,000 videos, right? Isn't it? No. 200 videos, right? Those four videos are occurring, right? So 200 videos, I mean, each second may cost you 100 rupees to make a video or something, right? That's what, for",
    "created_at": "2026-02-02T05:41:15.682322+00:00"
  },
  {
    "chunk_id": 122,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "use mine, right? So you can be pretty, 26,000 means pretty much like 2,000 videos, right? Isn't it? No. 200 videos, right? Those four videos are occurring, right? So 200 videos, I mean, each second may cost you 100 rupees to make a video or something, right? That's what, for eight seconds, right? So it depends, right? You can see. But don't be too free about making videos, okay? Now there's a 1,000 rupee joining fee to pay. You can get, I'll write a note. You can get it back from there. And even if it goes above this, whatever it is, you can get back from there. But don't spend too much on the videos, okay? [Speaker 1] So the idea is this. First goal, use the Jericho environment, study those papers, how they made the LLMs, do various things, etc. Now the LLM is not just doing it blindly, right? So some of the ideas of the Simulca paper, you'll have to start thinking how to do, right? You'll keep the memory, what it's doing on, etc., etc., right? And so for the first goal, I think in January, we should get the situation where for one or two or three games, you can play a game. And then for any particular part of that game, we can generate a small video, right? So the game has all of the transcript. I did this, I did this, I saw this, I saw this. And then you generate a video of the player playing that game as if he's going there. So it's like you're creating that clip, right? And then that's step one, right? Hopefully this month we can do that, right? So just one clip for one eight-second period. Then there may be multiple clips. This concept of having continuity, having the same look and feel throughout the game, all that can be done, but we won't do that now. So that requires taking the first image and last image and making sure you give those two images and the video will interpolate between them. So it",
    "created_at": "2026-02-02T05:41:15.682324+00:00"
  },
  {
    "chunk_id": 123,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "of having continuity, having the same look and feel throughout the game, all that can be done, but we won't do that now. So that requires taking the first image and last image and making sure you give those two images and the video will interpolate between them. So it looks like the same environment you're playing through, right? That is the part two, right? But after first month, part two ke pehle, what we want to do is we want to, that you can start, once you start understanding this environment, you start understanding how to make a non-playing character in this environment, right? Or something in this environment, which is, can speak or do something like using an agent, right? Rather than the agent only taking actions and just creating a video with hardly any articulation, maybe music or whatever. There is somebody who's speaking back, right? Some animal says something, some, whatever, these are various types, right? So how do you bring a non-player character into this environment that you do some R&D on, right? You do that while you're doing this basic work, right? And then the step two will be that now you have non-player characters who are LLM agents, right? They are speaking, you are going, you are speaking to them as an agent, right? So you are articulating something to them, which is going into the, some is going into the game. Some is going into your transcript of the game. Means what I speak normally doesn't go to the game. Game doesn't care what you speak. Game only cares did you go right, left, did you pick up the sword, did you do this, did you do that, right? It doesn't care what you said. But you keep that speech here. What that guy said, once you have made changes to this, so that there's a non-player character who can speak, not just take action. Then their speech will come through the game and come back to us or come sideways somehow, some channel you have to make, right? And that becomes",
    "created_at": "2026-02-02T05:41:15.682325+00:00"
  },
  {
    "chunk_id": 124,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "that guy said, once you have made changes to this, so that there's a non-player character who can speak, not just take action. Then their speech will come through the game and come back to us or come sideways somehow, some channel you have to make, right? And that becomes part of the transcript. Next time you have video in part two, you will have not only just the environment, but you will also have other players talking to you in that game. And that is what gets animated. So it becomes like a mini story. So that's the basic thing. And if you can get that far, it would be very nice. Then we can see if there's time we can try to stitch the videos together to make them continual. And then, of course, if we get even that far, I don't think we will in one semester. But suppose now you want to put your avatar into the game. That means you will register your face and your personality and say, now you will speak with my personality. That's just prompts, right? Prompts and that's all bells and whistles. This can be done later. Right. OK. A core engine has to be built. No UI, no UI. [Speaker 1] Let's just if I just add the code level, let's build this. Then we can make we can think about making that later. Right. If you want to do it in some other time or whatever else you want, somebody else will do it. Is it clear? Tell me now what you expect to do in the project and what you expect to do. [Speaker 2] So one thing I want to say is that I read the generated engine paper and I tried to build something like that. So from Python, what I will do is, let's say we build a 2D world. We will be adding rooms of cuboid shape or multiple cuboid shapes to make one room. And in those rooms we can add multiple agents that will be interacting with each other and talking",
    "created_at": "2026-02-02T05:41:15.682326+00:00"
  },
  {
    "chunk_id": 125,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 2,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "that. So from Python, what I will do is, let's say we build a 2D world. We will be adding rooms of cuboid shape or multiple cuboid shapes to make one room. And in those rooms we can add multiple agents that will be interacting with each other and talking to each other. So for that in Python, I coded a message queue, like message bus, that if one agent wants to send a message, it will send to this message bus. It will be thing based, like every agent should take some action or process what message they have received, read the inbox. [Speaker 1] So you already have the core elements of a multi-agent thing, but you have to stitch it into this. Why? Because these games already have some descriptions of castles or... Oh, who wrote the descriptions? Already have, because all the games are built in. Let's just use that, because somebody has written it.",
    "created_at": "2026-02-02T05:41:15.682327+00:00"
  },
  {
    "chunk_id": 126,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 0,
    "text": "[Speaker 1] speaker, you just connect Okay, so the first thing that I tried, it was that I tried first in Jericho, like how can we, I made a simple script first that, you can just start it. So just a basic script that will be, that will ask us for actions and this was one of the games that was invented in Jericho. Hello? I think it will... So, basically, we can choose our options and... So, I was exploring, like, how it works in the games or things, I don't know. So, this was, like, the user is giving an input to the agent and it is taking action and doing all those things. So, basically, this was the first thing and then I tried... This is just plain Jericho. So this was like I wanted to explore how it works. So I saw that. So how the environment works. So that's the first thing that I see. And then you might try to inform. So first, I want to also say that I am. So it will provide me an ID, it's private. So then the next thing that I said, I want to inform seven. So there are lots of rules within it. So what happens is that we can create like a room. So we have to create a room and then we have to create items and every item. What is this? This is inform ID. So this is the language that is used for creating Jio games. we can create rooms, we can create objects and entities. So, every object will have its own properties and we can interact. So, basically, this what happens is that all environment are reset. First of all, environment is set. And this is environment. [Speaker 2] I know that. Now, tell me, in the framework you had created. [Speaker 1] Yes. [Speaker 2] There was a concept of an agent, right? Doing something. [Speaker 1] Yes. [Speaker 2] Have you done anything together with that or this would be an independent game? The thing that I showed",
    "created_at": "2026-02-02T05:41:15.682600+00:00"
  },
  {
    "chunk_id": 127,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 1,
    "text": "know that. Now, tell me, in the framework you had created. [Speaker 1] Yes. [Speaker 2] There was a concept of an agent, right? Doing something. [Speaker 1] Yes. [Speaker 2] Have you done anything together with that or this would be an independent game? The thing that I showed you last. [Speaker 1] Yes. No, this is independent. The thing that I was struck about is that, okay, I also tried making my own game and all. I tried to make an NPC and all. [Speaker 2] What was the goal? What was the goal you set for last? [Speaker 1] I had to create an agent in the Jericho that interacts and then I had to also check out the Jetman API, how we can do. [Speaker 2] The idea was that you have an agent. What did you do last? What did you show me last time? [Speaker 1] Last time I showed my script that creates walls and room and, it recognizes that in the field of view these objects are there and agent can send messages to each other. That's what you told me to explore with Jericho. [Speaker 2] The idea was that your agent could be a rapper around Jericho. That was what we discussed first. Your agent, there is going to be some position of that guy. For you, for Jericho, you are moving around in various rooms. When you move to a particular room, then your agent sort of come into your last time's agent is sitting in that room. It has, what is this, there are some items, no? [Speaker 1] Yes. This is the observation. All this, this is a particular thing and then the item, what is the event, what is there in that room? We do inventory. This is what the description shows. Every room has like text description. [Speaker 2] I agree, but what else is there? There must be some item in that room, right? There are some things that you have etc. etc. Now the idea was that once you're doing all this, you're",
    "created_at": "2026-02-02T05:41:15.682602+00:00"
  },
  {
    "chunk_id": 128,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 2,
    "text": "This is what the description shows. Every room has like text description. [Speaker 2] I agree, but what else is there? There must be some item in that room, right? There are some things that you have etc. etc. Now the idea was that once you're doing all this, you're moving around in this room, What was that we discussed last time? Tell me. You're supposed to summarize that also. We didn't do that. Right? So, the agent will be the basic. So, the agent will be able to interact. [Speaker 1] And with what? [Speaker 2] With, we will have our script on top of Jericho. Then the agent will be able to interact. So, agent can interact with Jericho is fine. It's just a matter of writing an LLM which will do things. Right. Which is, anyway, You just have to call them and you do something to take the output you give. What was the, we discussed something that you could write a wrapper on [Speaker 1] One thing we had discussed was that, okay, my code was about how the room looks. Like, in the room that at this position, this object is there. So, Jerry code just say that agent is in that room, and within the room, at what position, what things are there, that will be your Python code. That will not... That's not possible in Form 7 and Jerry. [Speaker 2] That's fine, okay, right. So, what have you... So, you put that aside. [Speaker 1] Yes, there can be a workaround, so I am trying those things also. But that you are not doing. But directly it's not possible. [Speaker 2] Alright, so now what are you, what is your goal, what are you trying to do? [Speaker 1] So, this is the one thing I did and the second thing, using inform rules I tried to make an NPC inside inform. So, basically, [Speaker 2] You made a game? [Speaker 1] Yes, using Inform7, this ID. And that game is being run in this script. So, what happens in this",
    "created_at": "2026-02-02T05:41:15.682603+00:00"
  },
  {
    "chunk_id": 129,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 3,
    "text": "is the one thing I did and the second thing, using inform rules I tried to make an NPC inside inform. So, basically, [Speaker 2] You made a game? [Speaker 1] Yes, using Inform7, this ID. And that game is being run in this script. So, what happens in this game is that there is a server here, that's our NPC basically. So, I am acting over all commands, the house is to the node. You can see a heavy chest and a saboteur there. I will do things, so here the saboteur arrives from the north. So what happens is that I try to base the NPC. It will move around north and south and take the chest. It will do things on its own. So I did not command the saboteur to move. [Speaker 2] Can you insert this saboteur in another existing game? By modifying the code of that? [Speaker 1] If I get the infant code. [Speaker 2] So, this is my game. [Speaker 1] No, but inside the Jericho, the master, all that could be there, right? [Speaker 2] Yes, all of that would have that, is it not, the Z5s we could have that? You can open it up. So, can you actually put a character inside that game? [Speaker 1] I can. [Speaker 2] You can, right? So, now what we should do is, so you have explored Jericho, you know how to put NPC. Now, what we need to do is, you need to, have a mechanism to put a NPC in one of their games. Because I don't want to write the entire description of words and all that stuff. Well, that creative stuff, I don't want to waste that. And somebody already done it. We'll work with that. But we should be able to put some NPC into that game. That NPC will, suppose you as a player go into that game, right, and you see an NPC there. You should be able to talk to that NPC. How to do that? Now there is a saboteur is there. Saboteur",
    "created_at": "2026-02-02T05:41:15.682605+00:00"
  },
  {
    "chunk_id": 130,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 4,
    "text": "should be able to put some NPC into that game. That NPC will, suppose you as a player go into that game, right, and you see an NPC there. You should be able to talk to that NPC. How to do that? Now there is a saboteur is there. Saboteur is now sitting, it's there, that guy only knows the saboteur is there. The fact that that saboteur is there, there should be something about that saboteur in some database which is outside Jericho, which is your database, there is this saboteur, it has some phase, whatever, etcetera, some metadata is there, and now if I want to speak to saboteur, I should be able to put an action here, speak to saboteur. That action should be picked up, that will not go to Jericho, that will go to your system, that will go and say I am speaking to saboteur, And saboteur will be, will say, okay, this is what I said. Saboteur will respond. There will be a conversation between you and saboteur. Jericho is not aware that we are talking. Jericho has no clue. Jericho only deals with movements of things. And what, who picks up, what is where, etc. And then they can move around and separate. Right. So, the idea is this, that right now you are talking to saboteur. The saboteur may actually give you an avatar which is actually playing this game with them, right? So then you are basically making this whole thing into a multi-user system. Many people can be playing this game. So they are all NPCs, as far as I am concerned, they are NPCs, right? And for that guy, you are the NPC. So we will create a system where The next two players are playing on both the systems, they are the agent and the other is the NPC. But before that, before you come to that situation right now, suppose there is an NPC, the fact that the NPC is being controlled by somebody else, that is part two. Think about it. But the idea is",
    "created_at": "2026-02-02T05:41:15.682606+00:00"
  },
  {
    "chunk_id": 131,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 5,
    "text": "the systems, they are the agent and the other is the NPC. But before that, before you come to that situation right now, suppose there is an NPC, the fact that the NPC is being controlled by somebody else, that is part two. Think about it. But the idea is that can you talk to them? So then when you can, talk, then it's like, I go there, there is this guy over there, like, there's a context of that guy and my conversation, then that guy moves around, I move around, that guy comes back, then that conversation is retrieved, then I can continue the conversation with that guy. So it's like, it's like that simulacra, simulacra, but instead of moving around in those rooms, you're moving around in this game. And these, these things are all moving around and you're doing this. But now, so the control of this saboteur, Right now, you are able to move that way around, right? [Speaker 1] Yes. [Speaker 2] How? [Speaker 1] By, actually, my agent or the cemetery. Cemetery, how does it move? [Speaker 2] Cemetery, it moves on its own. So, what happens is that there are, it informs that there are every turn rules. So, once I did an action, it's like a tick. So, after every tick, every turn rules are activated. So, cemetery will have, there's a random function and inform also. So, it will do, like, cause a point, And it will say I should wait or I should go somewhere. That's your code. [Speaker 1] You've written that code. For saboteur. That saboteur code could also be an agent playing the game. That's saboteur. But then, Jericho, do we have like two agents? [Speaker 2] I think only one agent. [Speaker 1] No, no. Only one. But that, what that saboteur needs to, he's in a particular room. You know, what is all in that room, you know. It can decide to move there, there, etc. It can decide, right? That's a decision that you can take, right? Just like you are taking right",
    "created_at": "2026-02-02T05:41:15.682607+00:00"
  },
  {
    "chunk_id": 132,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 6,
    "text": "Only one. But that, what that saboteur needs to, he's in a particular room. You know, what is all in that room, you know. It can decide to move there, there, etc. It can decide, right? That's a decision that you can take, right? Just like you are taking right now, right? One of the guys can be the main agent. Other guys are NPCs are playing the game, right? For the moment. That NPC, what it does, it's up to you to decide how it does, right? Instead of having something random, that guy can also be seeing and doing various things. In some sense, that can be an NPC agent. It can call anything to decide what to do, right? Now the, now that's the tricky part, that inside the inform7 code, there are after rules. So, in the inform only we have to specify rules. [Speaker 1] But can it not, that rule cannot call code. So, yes, that's, that's the thing, that the rule works within itself. [Speaker 2] But that rule can, it's just some Python code or what is it? [Speaker 1] No, those are inform7 code. InformSum. So it's a different language, like this. I guess you can see that the lab is a room. So InformSum, there are objects. There is a room or like this only. It can create room objects and every room has, we can add a west, north, east, south, four directions. So into four directions, we can add another room. So it's a very constricted language. So where is that 7W? How is that added? Let me see. It may be in another file. This one was... I think I have overwritten that code. Okay, yes, this is the same code. The garden is a room and this is a description of that. So the house is north of the garden. So like this is the code of that. So if we write the house is north of the garden, there will be room here. The player is in the garden. So that means us. The heavy",
    "created_at": "2026-02-02T05:41:15.682608+00:00"
  },
  {
    "chunk_id": 133,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 7,
    "text": "this is a description of that. So the house is north of the garden. So like this is the code of that. So if we write the house is north of the garden, there will be room here. The player is in the garden. So that means us. The heavy chest is a container in the house. It is closed and locked. So, closed and locked are its properties. They are Boolean properties. So, this language is there. The saboteur, where is it defined, doesn't it? The saboteur is a man in the house. So, it will start in the house. Understand, wait is waiting. So, carry on waiting. So, we added extra action of wait to the saboteur because it can choose or not to choose to do an action. So, there is no option in inform that we cannot do an action. So I created an action that's paid. It's called paid, so it does nothing. So every time when the saboteur is in the house. So a turn is done and we give it a command. That's one turn. So every time when the saboteur is in the house, it carries a silver cane, heavy chest is locked, try the saboteur and lock it in the hand. So like that, after every turn, these rules will be applied to saboteur. And it will decide whether to do it or not. So this is the random one. So that may be a random direction. then destination will move away from location of servitor. If destination is a room, try the servitor going there. [Speaker 1] Now this goes into a file, is it? [Speaker 2] Yes, this will be, we can release it and like that. Release where? No, it will release as a Z8 or Z55. The entire code, game? Yes, this code. So, for now it's Z code. But the Z code is for the entire game? Yes, it's for the entire game. And this is a tool? Yes, this is IDE. [Speaker 1] So, then some other mechanisms have to be used, right? So, then",
    "created_at": "2026-02-02T05:41:15.682609+00:00"
  },
  {
    "chunk_id": 134,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 8,
    "text": "entire code, game? Yes, this code. So, for now it's Z code. But the Z code is for the entire game? Yes, it's for the entire game. And this is a tool? Yes, this is IDE. [Speaker 1] So, then some other mechanisms have to be used, right? So, then in some sense the fact that this particular, so in some sense you are going to have to have the game is there which will only have the descriptions of various things, right? And there will be, which room the saboteur is in we do not know, right? If it starts in the house, it starts in the house in 7 years and then it every turn it goes randomly to one room and if it is in the house then it will and it sees a chest and it has a key it will unlock it. These are the rules of 7 years. Okay, so let's assume that there is a for the moment there is a there is an NPC in various places and they can move randomly. Now the question is if you enter a room, and that NVC called saboteur is there, right? It's moving around, it's a nice, it's not a reason, right? But the talking to that guy, and maintaining the context of your conversation with that guy, that is up to you. That is in your own. If you come across that guy again, then the context is retrieved and the conversation continues. You understand what I'm saying? So this, there could be ten to five different agents sitting in various places all moving around randomly. If they come together, then this is, the conversation starts from where you have talked to them, you have talked to them, now the conversation of both of them together, you are talking to each other, right? And they are also talking back to you. So conversation between you as an agent and these NPCs, you are taking care of in your system. I see. You are playing this game, there are some NPCs which are sitting",
    "created_at": "2026-02-02T05:41:15.682610+00:00"
  },
  {
    "chunk_id": 135,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 9,
    "text": "of them together, you are talking to each other, right? And they are also talking back to you. So conversation between you as an agent and these NPCs, you are taking care of in your system. I see. You are playing this game, there are some NPCs which are sitting over there and they are talking to you, right? You are the one who is moving around in the game, right? They are moving randomly, right? At the moment, you can also be moving randomly. How does it matter? You are moving randomly, they are moving randomly. So in some sense, both of them are equal. We can make it more stateful. I can add some properties to separator that will change with every interaction so we can make it more stateful also. So it will not be completely random. That change is going to be hard coded inside this code. [Speaker 1] Yes. So let's say the properties of separator is that like happy equal to true or false. So, if he interacts with us, he is happy. If he is happy, he is more likely to go to North. There is no way to get just this particular side of this code can... On run tag, we cannot do. On run tag, once the game is running, then we cannot install this element on the code. So, the decision making will be hard coded in this code. [Speaker 2] No, but it can read things which are in the room, right? [Speaker 1] Yes, yes, yes. It can say what things are there in the room and we also have a guard view feature that we can, for the user also, we can retract everything that is there in the whole room. So, we can make such rules like for a Saturday. [Speaker 2] So, it can read everything which is there, right? Right. And that, everything which is there can be written by anybody else? Can we return? Can you... Say that I want to place something in the room, that something has properties which you decide.",
    "created_at": "2026-02-02T05:41:15.682611+00:00"
  },
  {
    "chunk_id": 136,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 10,
    "text": "like for a Saturday. [Speaker 2] So, it can read everything which is there, right? Right. And that, everything which is there can be written by anybody else? Can we return? Can you... Say that I want to place something in the room, that something has properties which you decide. From the Python's group. Or from your, when you talk to zero you can say I can, I want to put a note in the room which says this. [Speaker 1] We can do that. Text which is your own text. [Speaker 2] But like the objects must exist at the starting. [Speaker 1] But can you change the properties of the object? [Speaker 2] Yes, we can change the properties. So what properties? [Speaker 1] Yeah, like the chest was locked. No, but can it have text in the properties? [Speaker 2] Yeah, it can say. So, like, we are saying, say you wait. [Speaker 1] Yeah, but can that you wait? Suppose I say, I write, there's a piece of paper there. Okay. And as an agent playing the game, can I say, write on this piece of paper something? I have not seen this. We can probably do that. Because descriptions exist for every object. But that description is changed here, or changed by the agent. [Speaker 2] No, it can be changed. Probably changed. In Enform score or by the agent? In Enform score. [Speaker 1] But by the agent acting, you can't change it? OK. So then maybe this stuff about what is there in which room, et cetera. So that's what I said in the beginning. You need to take this game and map out the entire thing and create a copy of that map, the graph. Right, and then the, what is happening, what is the context of each agent with each other agent is kept with you. You have a sort of parallel copy of what's going on over here, right? And you can keep running that and figuring out what's going on over there. You have a God view, right? You",
    "created_at": "2026-02-02T05:41:15.682612+00:00"
  },
  {
    "chunk_id": 137,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 11,
    "text": "what is the context of each agent with each other agent is kept with you. You have a sort of parallel copy of what's going on over here, right? And you can keep running that and figuring out what's going on over there. You have a God view, right? You know what is where. You always know what is where. You can keep that updated. And then, then the context of, what that agent is thinking, doing, etc., etc., right, is there in your context, right? So, you understand what I am trying to say, right? You want to be able to talk to these players in this game, just like in simulacra, they are talking to each other. And that context is maintained. The next time that you talk to that agent, he remembers what you said last time, etc. That conversation can be played, right? [Speaker 2] Yes, but we want that conversation inside the uniform. [Speaker 1] In your name. That's what you should do. And from that, we can make the movies, right? And then the next step is those agents which are keeping track, one of them is the guy who is running the ticks. The others can be people who are connected. They are avatars which are just playing. They don't have to be actually playing because they are avatars of people. Can you summarize what you are doing in Slack? You summarize what you decided to do. You want to create a layer on top where you can talk to these NPCs and the NPCs have to be inside the game which exists. These two things let's see we can do. You can talk to them, maintain their context. So you know what is the context of my chat with this guy, chat with that guy, where we met, which room we met, all that stuff we know. It's just a little layer on top where I can talk to the NPC. And then we use these games to create those sort of narratives. This is my story, your story. You can have",
    "created_at": "2026-02-02T05:41:15.682613+00:00"
  },
  {
    "chunk_id": 138,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 12,
    "text": "with that guy, where we met, which room we met, all that stuff we know. It's just a little layer on top where I can talk to the NPC. And then we use these games to create those sort of narratives. This is my story, your story. You can have a story from his point of view also, Saboteur's point of view. So all that can be done later. Once you have this basic framework. OK. Can you repeat? [Speaker 1] What we will do is, we will create a layer of the Enfon7 game. All the other players, apart from our player, of the user, they will be as NPCs, they will be taking their own decision in the cherry-picking. But, the conversation that they were having, we will show in the states of them in our Python code. So now, when I encounter you, right, as an NPC, you are the NPC, I will have a conversation with you. That conversation will happen in the Python. That context is stored in your database, right? Okay. That is stage one. Next, we can have, they can also talk to each other. All that can happen, right? Because I think what is happening is up to you. So, you are effectively, your own symbol cry is happening, right? is a copy of your simulgra. It's just that the descriptions of what is happening in terms of the environment and what is this area or some castle or this, all that is coming from the game. So it's a little bit more interesting. The simulgra was one fixed area, right? Here any game, it can be, right? So it is arbitrary. So all the users will be as an NPC first and then moving and some of the rules will be hard coded in the genico. All their movements. All movements and actions in the game are hard coded. But what they are talking and what they are interacting with each other. Okay, maybe we can have some actions that are on the Python end that are not happening on Jericho,",
    "created_at": "2026-02-02T05:41:15.682614+00:00"
  },
  {
    "chunk_id": 139,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 13,
    "text": "hard coded in the genico. All their movements. All movements and actions in the game are hard coded. But what they are talking and what they are interacting with each other. Okay, maybe we can have some actions that are on the Python end that are not happening on Jericho, but they are happening on the Python end. Like I slapped another thing. Exactly. It may not be happening on the Python end. Jericho may be used to just track the movements. Movements and any items which are lying around, etc. Because if the item is there, it comes back, the item is still there, somebody picked it up, all that stuff can be inside Jericho. So some action, user will not, the over end user will not know what is happening inside. He can observe, right? In the end, there will be no loser. There will only be an enemy. So ultimately from this, I want to create a, any particular, particular strand of thing happening, room plus time. Time plus locations. I'll create that narrative from this, whatever history is there or what is going on and from there it will appear on you. So we can create many stories of what is happening. That is part two. Once we can create things happening, interaction happening in the world, then we can look at snapshots of it and then back the story, that story, that story. So what I will try to do is that I will look upon that God using and what I can, let's say my This is room 1, this is room 2 and this is the agent here and this is the NPC here. What they are looking at, I will apply loop command on both of them and what I say is that when we put the print statement here, So, I will print all of the observations together from the genic room and the observation will be like agent observe this at this room at this time and NPC observe this. Both of this maybe will be feed into an LLM",
    "created_at": "2026-02-02T05:41:15.682615+00:00"
  },
  {
    "chunk_id": 140,
    "user_id": "utsav",
    "meeting_name": "live_meeting",
    "meeting_type": "live_meeting",
    "meeting_index": 3,
    "project_type": "system_design",
    "chunk_index": 14,
    "text": "that when we put the print statement here, So, I will print all of the observations together from the genic room and the observation will be like agent observe this at this room at this time and NPC observe this. Both of this maybe will be feed into an LLM and LLM will be also be fed along with these observations their personalities or whatever. Interaction. Let us see, okay. Good, anything else? No, I think that's as much and let me see. This was Gemini. Okay, now 12, it's going to be 11.30 on Monday. Clear? Okay with you, right? All right.",
    "created_at": "2026-02-02T05:41:15.682617+00:00"
  }
]