[Speaker 1] So, you have to wait, because you are supposed to be at 11.38. No, I had received it was 12. No, I changed it. Anyway, just wait, it is not 11.30. We will meet today, but first let me finish this. So, this stores the information for every thing. What is that on the frame? What is the topic on the frame? You have to join the meeting and share. That is like Google meeting.

[Speaker 2] Google meeting.

[Speaker 1] There is a Google meeting up today. It is on calendar. Open your calendar. Now you can share the screen. Done. Shut the mic. So, there is evidence for each summary and every thing is sourced. Like the name, the serving, this is the decision item. These are different decisions taken by people. There are different actions taken by people. Then there is a chapter. Chapters are like a general summary and time frame. This was the title summary. People, decisions, action items. Let's say you have a speaker called Rahul. This will try to find the name of the speaker. This is the meeting stage in which you have speaker notes. You have all the speakers. All they are.

[Speaker 1] You are telling me the how. You haven't told me what. What is it going to do? What have you done? What is all this about? You are going to the internals of it.

[Speaker 2] First it takes out the aspect using Deepgram. And then it takes out. So, you take the audio, take the video. We take key frames from the video using another library called Cpredict. ICpredict. But this doesn't work. There is a fallback also. Let's see how the images are. And then we use Gemini to get the details from the key frames. Screenshots of the video and summary from the audio. And then the audio you are passing to the Gemini through the transcription. And then we make small time frames for movements. They can be slightly changing key frames changing or the person stops talking. There is a break between the speech. We create a small time frame. And then we compare the post accessibility of all these time frames. Whichever time frame was similar, we will open it in the templates. Then we use these chapters and pass them to the editor to create summary. Then we also store different things like the name of the speaker. What else is happening. So, let's see what works so far. This was the output. These are the chapters that got generated. I have stored them into five active objects. This is the summary. The title we got. This is for this video.

[Speaker 1] We have the key points. Some of the decisions that were made. And then these are the movement IDs. These are the movements to make up the chapter. We got this movement from 90 seconds to the hundred and third second. And we have the text also. These are the other chapters. This is the frame analysis for all the different frames that have been detected. We get a view like what are they. Is it slides. Is it mixed. Is it camera. What the topic is. The visual summary. What is on the screen. The text. The entities. And then if there is a name of the speaker present on the screen. These are the different key frames. These are the movements. The key frames. The audio. This library actually gives the different speakers. You can see the AI evaluator was there. It was given the name of the speaker 0. And then the student was speaking. It was given a label of 1. For example, let's say the student purchases something, or his name is visible on the screen, this speaker will get replaced by that.

[Speaker 1] As regards the goals of the project, what progress has been made so far? What are the goals and where have we reached?

[Speaker 2] The goals was to create data which includes the demonstration which are done on the screen. There is something happening in terms of the meeting or in the presentation which is based on the conversation and any other context which is pre-provided, which is the PDF file that is already there. So that data is already there. The transcript is there. Everything is there. But what is happening on the screen at a particular point in time is not there. You have the transcript. You have the context. And now you have done something which has created all this structure. Idea was to firstly in batch mode augment that meeting transcript with some information about what the presenter is presenting on the screen in terms of a demonstration. Have you been able to say this is a demo going on versus something else? This is part of the presentation or slide one, slide two, slide three.

[Speaker 2] I think it is. You could say for example this is a night demonstration and showcase the time code.

[Speaker 1] Where is that demo happening? What key frame is it happening? You have generated a whole bunch of stuff. But is it can we actually get out in this presentation at this key frame from this frame to this frame somebody is giving a demo?

[Speaker 2] In these tablets. This is what that evidence object was about. For this one you get this summary. The presentation introduces an AI chatbot in this file.

[Speaker 1] Where do you get the references? From what moments? We talked about it will be shown on the screen. But you are not answering my question. You have done a lot of stuff. But I still do not see that in this entire transcript when is the presenter actually either showing a demo or showing some code? Do we have that information? It should be somewhere in the chapter but it is not given clearly.

[Speaker 1] So what we wanted to do was, okay you have done something. It is fine. Interesting. Certainly interesting. But does it solve the problem? It may solve some other problem. It may say I have dumped all this, now I do not know what the hell this is. Some information has come out. But is it telling me that this transcript is there and then from this period to this period there is a demo going on? And number two, within that demo, what is going on is the description of what that person presented in that demo. Exactly what that narration. If you were to describe this demo to me, that this is what the person presented. This is what actually presented. Or this is slide one. A lot of stuff can come from the context. What you produced is actually more data but it's not informative in the sense that we wanted it to be.

[Speaker 1] Yes, you could probably use this data. Information that I think is definitely there but I don't think that is happening. So what we wanted were two things. One is in batch mode, that this entire meeting should be enhanced. The transcript of that meeting needs to be enhanced with whatever is on the screen. Could be a presentation slide on the screen and a narration what that slide is. All that in some sense already there because the PDF of that somewhere is there. The file is also there on the screen.

[Speaker 1] The system is asking questions only based on what he's speaking, but nothing to do with the screen. But suppose the guy is speaking, but the screen is showing something totally wrong? Or the system has no idea what's on the screen? So a lot of stuff has been generated, but from these two goals perspective, to what extent have we got there? That's what I think we need to think. Think like from this data generated algorithm, we can go. So what I think we need to do now is, we need to take the result of what has happened so far and say this is the input, and this is the desired output.

[Speaker 1] And now I think we should stay on Slack. After this meeting, can you summarize on Slack what are the two inputs and outputs that we need? One is batch mode, one is data type. At least then Pankaj can take a look. He's also on Slack. Second thing is, this particular meeting that we are having today, get a transcript of that meeting. Summarize that transcript and post it on Slack. At least we know that we are keeping them up to date in what is happening.

[Speaker 2] They just need to get a batch summary.

[Speaker 1] Batch summary will be done fast. I think this deliverable will be done very fast. And the next thing will be to, by the way, they will have to supply their tool and then we have to do it with their tool. Meanwhile, before they supply that tool, let's see if we can then, after the batch summary is done, maybe a week after next, we can work on can we make it real-time, which I can integrate. Batch summary should be done based on meetings, but now let's generate that batch summary for not just that meeting, but let's generate for the other meetings which are shared. And so now we have results for three, four meetings. Let that be the target for next week. A batch summary process, this is what the person is telling me about on the screen at this particular time, where the demos are, et cetera. So then now that can be the basis for what Akshit can put into his chat box.

[Speaker 2] I have a request. So they write out the meetings that are not shared with anyone speaker. If it's possible, could I get some?

[Speaker 1] You will. So this will be there. Then our previous meeting is also there. Usman, two speakers, and then the other guy's meeting will also be there. Soon there'll be two speakers, and earlier there was no presentation. Now they'll be starting making presentations. I'd like you to make a GitHub repo for this. Add me also. Don't make it public. Make it private. Add me, and then we'll add Pankaj. And then we'll see. Same events. Action items, write down on Slack. Goals, write down on Slack. Just do a summary of today's meeting and put it on Slack. Today's meeting, when it's recorded and ready, I'll upload it, and then you can get it.

[Speaker 1] Ultimately, my goal is, I want this integrated real-time. Their goal, they have to specify after seeing this exactly what they want. The core engine should be generic, but there'll be a real-time version, and there'll be a batch version. Batch version will be based on what you're doing now, but batch version will be more toward what they want. And the real-time version is what is required by them. How long does it take, the whole pipeline?

[Speaker 2] This video is 10 minutes and around two minutes. I think if it's real-time, you can do, like, one frame takes very long. It's very quick, probably one frame. It's like 80 frames or something, it's a video for two minutes. But chain detection, in terms of the keyframe, whether it's a keyframe or not a keyframe, based on what is there. I mean, that architecture will have to be well done. To see is this something new happening, and is it a demo? If it's a demo, I start augmenting. If it's not a demo, I don't want to augment.

[Speaker 1] Overall, we're able to think through this. But first, let's get past this phase. From this to a system is a journey. This is easy. Getting into a system which is usable in real-time, or a system which is usable for their purpose, is a journey. Transcription is happening, it's batched. Every 10 seconds, it's being batched. So, it's not a simple architecture. It's a complex architecture. Anyway, those steps have been taken. All right, we'll end this one. I think we'll provide the batch and release on the tomorrow after.